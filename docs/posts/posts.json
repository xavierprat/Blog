[
  {
    "path": "posts/encounters_with_alien_life/",
    "title": "Encounters with alien life",
    "description": "A vast distance in space, time, and intellect",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2024-07-03",
    "categories": [
      "Essay"
    ],
    "contents": "\n\nContents\nIntroduction: The Fermi paradox is not a paradox\nThe vast distance\nTime: The time window of civilization seems unlikely and short\nIntellect: the intellectual distance between species\nAll together\n\nIntroduction: The Fermi paradox is not a paradox\nChatGPT says that “The Fermi Paradox is the apparent contradiction between the high probability of extraterrestrial civilizations existing in the universe and the lack of evidence or contact with such civilizations.”\nEnrico Fermi was known for many things, among them he was known to guesstimate a number, a back of the envelope\ncalculation to know the ballpark or magnitude of a quantity. I don’t agree with the Fermi paradox, mostly because we only know of one single planet with life and we don’t have any serious theory that explains\nthe formation of life (abiogenesis), so we are trying to do statistics with a sample of n=1.\nHere I want to lay out also that even if the formation of life is guaranteed (given a set of\nconditions of composition and temperature of a planet), there are robust statistical estimations\nthat explain why we haven’t seen anyone yet.\nThe vast distance\nThe dimensions of the universe are too large and information (light or matter) travels way too slow.\nSo the first explanation is that we have not seen anyone yet because we are too far away and cannot hear each other.\nSome numbers to show such vastness:\nClosest star: proxima centauri 4.2 light years\nVia lactea: 105,700 light years wide\nClosest galaxy: 25,000 light years\n2nd closest galaxy: 75,000 light years\n\nDistance\nSo this is the first barrier for contact, unless we find a\nwormhole in spacetime or a parallel universe,\nthe very vastness of space and the time that it takes to travel becomes too large.\nTime: The time window of civilization seems unlikely and short\n\nWindow of civilization\nAny form of life capable of civilization may go through 3 stages:\nPre-technology period\nTechnology period\nDecline or self-destruction\nTwo civilizations will hear from each other only during the technology period.\nNow we are getting outside of physics and more perhaps into History or Anthropology. In other words,\nwe have too look into how civilizations are created, evolve, perhaps reach a technological state, and eventually fade.\nThe hypothesis here is that any living society, as it progresses,\nit will reach a third stage when technology becomes so powerful or the expo that it evolves or plateaus to something else.\nMy guess is that such change or plateau can happen in 4 different ways.\nIt never achieves the technology for space exploration and communication and remains in a pre-technology stage. This seems to me very likely. I don’t see in Darwinian biology any evidence\nthat humans had to have such supremacy and allowed to reach a technology state. The fact that after\n4.5E9 years of life on earth only the last 2E4 have had human technology seems a rare event. It took too\nlong to be necessary or spontaneous,\nso my vote is that this option is the most frequent.\nAn external factor ends its civilization: An astronomical (meteor), biological (pandemic) or environmental (drought, flood…) just prevents them to continue or severely halts its progress\nSelf-destruction: the very genes making us a superior species are a threat to ourselves.\nThe survival and selfish genes which made us survive and thrive are now threatening our self-destruction.\nA note on self-destruction: A quote attributed to Albert Einstein says “I know not with what weapons World War III will be fought, but World War IV will be fought with sticks and stones.”\nOne should expect technology to evolve faster than evolution, so our rudimentary genes are not\naltruistic enough to deal with nuclear bombs and a worldwide common good. This is why humans are a threat to\nthemselves. The only possible escape from destruction is if our primitive genes are Replaced by AI.\nHedonic decline: We have to remember that our intrinsic need to explore space comes from our genes that\nmakes us improve our lives in an environment of scarcity.\nWill we still have the need to explore space when all our needs are satisfied?\nIf technology provides all the primitive genetic needs maybe a civilization will stop exploring and will settle.\nWill we be connected to VR worlds with VR\nexperiences and injected drugs to make us feel all what we desire\nwhile our bodies linger? This is the decline of the Roman Empire but without a barbaric invasion.\nThere’s a scene in the Wall-e movie that shows what I mean (https://www.google.com/search?q=wall-e+scene+with+sedentary+people)\nIn conclusion, we should accept that there may be a window of technology (rather than a lasting stage of technology).\nThe universe is 15 billions years old (15E9), there’s been life on earth for 4 billions (4E9). The formation of life may not be rare, but the window of technology may be rare and short considering that it’s been only 150 years since we’ve had radio technology and 50 since space travel and we’re already talking about potential civilization collapse.\nIntellect: the intellectual distance between species\n\nIntellectual distance\nAnd now we are in the philosophical realm. Different species do not perceieve reality the same\nway.\nThis goes in the direction of\nThere are things we know\nThere are things we know we don’t know\nThere are things we don’t know we don’t know\nBeing this third category by definition the largest one.\nThere may be advanced species who perceive reality in a way we cannot comprehend.\nThe same way that a dog cannot comprehend the patterns of the planets, maybe we cannot comprehend other patterns.\nThe scary part is that they may already have contacted/influenced us without us realizing.\nDo bees realize that we use them for honey? Do they know we exist? Is a bee capable of “knowing”?\nEven if they have not contacted us, expecting this alien species to be in our intellectual\nvicinity is quite pretentious. I do not know what species on earth we are cognitively closest, chimpanzees perhaps? And yet, chimpanzees cannot comprehend the very idea of our technology.\nSo, if our intellectually closest species, with whom we share most of our genetics and biochemistry,\nthey cannot see “eye to eye” with us, how naive is that we can see eye to eye with a civilization\nthat has evolved independently from us?\nA counterargument to this “intellectual distance” argument is that may be that IQ plateaus.\nAs soon as a species can develop language and social structures, it will protect the weak and\ncognitive evolution stops.\nThe other option to this “IQ plateau” hypothesis is an AI-motivated IQ inflation. That is,\nif they developed some AI that surpasses the biological IQ and it keeps improving at a pace\nmuch quicker than what evolution would go.\nAll together\nFinally if we put all these distances together,\nit does not seem to me that it is a paradox that we have not seen (or recognized) any alien life.\n\nFermi’s paradox is not a paradox. Even if we assume that life is frequent in the universe there are 3 groups of reasons why we have not heard from each other.\n\n\n\n",
    "preview": "posts/encounters_with_alien_life/pic4_all.png",
    "last_modified": "2024-07-08T09:37:10-05:00",
    "input_file": "encounters_with_alien_life.knit.md",
    "preview_width": 1704,
    "preview_height": 1192
  },
  {
    "path": "posts/the_joyful_struggle/",
    "title": "The joyful struggle",
    "description": "A personal view on wellness",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2024-07-03",
    "categories": [
      "Essay",
      "Wellness"
    ],
    "contents": "\n\nContents\n1. Introduction: the cactus in the rainforest\n2. The goal: keep yourself sharp\n2.1 Goal 1: Physically fit\n2.2 Goal 2: mentally sharp\n2.3 Goal 3: spiritually or socially generous\n\n3. The obstacles and how to overcome them\n3.1 Maslow Pyramid of Cravings and St Thomas Aquinas’ idols.\n3.2 The dynamic cycle.\n3.3 Quitting the rest.\n3.4 Distinguishing between achievement treadmills and true eudaimonia\n\nBibliography\n\n1. Introduction: the cactus in the rainforest\nThis is not an academic post, I don’t speak from a scholar point of view, but rather from someone who is a teacher, mentor, and father to a new generation that is exposed to new challenges. The new challenge is the dealing with the very abundance we are surrounded in the first world. There is really not a cultural framework that we all agree and can rely upon to navigate this inflation of information, choices, and temptations.\nWhile this can be easily rejected as silly first world problems, I have witnessed in myself and others how the current abundance of distractions and pleasure is self-destructing. The big question is one of the oldest, what is the life that is worth living in the current times?\nTrying to answer such question is very pretentious. This question has been tackled by many great minds before and I am not going to pretend to answer it in a systematic and thorough manner. This is not even a practical handbook, rather, it is a collection of personal impressions that by experience or reading others I have been accumulating.\nI have been trying to articulate some attitudes and habits that helped me and perhaps I have already adopted, because they feel true and I want to make them last and solidify them in my routine. But also it is when I articulate it that I understand and therefore I can share it and perhaps help other people.\nI will start with a fairly graphical metaphor. We are a cactus in the rainforest\n\nGenerated by Dall-E with the prompt “Draw an overfed cactus in the rainforest”\nOur bodies and minds are misplaced from the conditions they evolved from. We are a cactus in the rainforest. An overfed cactus. We evolved in the scarcity of food, sex, comfort, material goods… but now the abundance and easy access to all these things is making us addicted to pleasure, and this excess of comfort is making us miserable. The goal therefore is to sharpen ourselves and avoid blindly following our pleasure instincts.\n2. The goal: keep yourself sharp\nI’ll say it from the start and I will justify it later. In order to live a fulfilling and meaningful life, we must keep ourselves sharp, in constant improvement. The premise is that by exposing ourselves to constant challenges we will get used to it, I will refer to this getting used to this discomfort as “feeling in the zone”. This approach will also allow us to take responsibility, own our lives, and not leave it up to external variables.\nWe are seeking growth, not just pleasure. Pleasure is necessary and I will explain where it fits. But I am associating growth with fulfillment and meaning. One could spend too much time on definitions and not move the needle. Hopefully it will make sense later.\nWe exist in three different areas and those different areas need to be kept sharp\nthe body: keeping yourself physically fit\nthe mind: keep learning and creating new things\nthe spirit or you in relationship with others (religious and philosophical overtones here, change the language if it throws you off): keep giving to others, give them your compassion, your time, even if they don’t deserve it, especially when they don’t deserve it.\nThis constant improvement is made of a cycle of pain and rest that I will talk about, but it is important to understand that one must get used to discomfort and constant challenge in these three areas. The goal is to build a habit of discomfort.\nWe can use science to justify and say that there’s enough scientific evidence about what the goal is for these three areas: physically fit, mentally sharp, socially connected. These are not objectives in themselves, the process, the barrier is the way, buiding a growth mindset will give us meaning and purpose.\nIn all cases notice that I’m inviting you to forget about yourself, your selfish attachments, and your limbic needs and whatever your body, mind, and soul are craving. This will be painful, it requires being vulnerable, constantly exposed to your weaknesses, but you’ll get used to it. Because we evolved in scarcity, evolution is making us lazy and selfish and we self-destruct. Back then, the very scarcity kept us sharp. Not anymore, in our times of abundance we must self-infringe scarcity to keep us sharp. By exposing ourselves to struggle we improve.\n2.1 Goal 1: Physically fit\nLet’s start with perhaps the least important. Albeit if your motivation is longevity or healthspan, then this would be the most important one. Keeping yourself physically fit is fundamental, but when in conflict with the other two areas of the self, this one should take lowest priority. Physically fit has nothing to do with your looks, not even your BMI. Again, I’m not talking about where you are today, but what direction you’re heading.\nPeter Attia talks about the four main areas that one must be working on. Aerobic health, Anaerobic, Strength, and Balance.\nTo be very specific, one can run a quick check on the specifics. For aerobic health you need low heart rate, you should be able to hike or even slow run for long periods of time. For anaerobic the VO2max describes your maximum capacity.\nNotice that I’m not talking about diet and sleep. We are just listing the challenges, the grind, the discomfort. These are the goals. Attia’s book goes into great detail. To me the most compelling argument is not necessarily the lifespan but rather threefold\nHow you feel after a workout and how that affects sleep, your hunger, your motivation, and even your mental abilities.\nIt is a great venue to transfer resilience and growth mindset. Getting used to discomfort here is a good starting point even though as I said it may not be the most important goal.\nThe more physically capable you are, specifically as you get old, the better you will be able to achieve other goals. The quality of life at the end of our lives is severely limited by our health.\nWe are not going to spend a lot of time so that we don’t lose our focus on our ultimate goal, remember, it is not necessarily our health span, it is living a life that is worth living. However, one should look at their physical fitness the same way one looks at their savings for retirement. The same way that we have financial advisors, Attia suggests that our physicians should also have a plan for retirement.\nNotice in this graph that if at 45 you cannot do certain things, by 75 you will be incapable of engaging in the cycle of challenge that I’m describing below\n\nLike a retirement account, the savings of today are the benefits of tomorrow\n2.2 Goal 2: mentally sharp\nProblem solving keeps us mentally sharp.\nI’m using the term mentally sharp as a very wide net of human activities. For example, playing an instrument, singing or dancing, they all require a mental skill that must be learned, learned through discomfort and pain.\nOne can start with daily low-level activities such as writing in a diary, reading a classic, doing the daily sudoku or calculating your own taxes. But to move the needle one may need a big project.\nSome “big-project” examples can be\nWrite a book\nLearn a language or a new technology\nEngage in uncomfortable conversations\nCreate art, express yourself\nWhy big projects? Big projects help us with changing our routines. The same way that for keeping physically fit it is not enough to decide to take the stairs, because a tiny change may improve our health, but remember we are trying to improve our growth mindset, so a harder challenge will help us do that. In that sense, one can decide to run a marathon or do 100 push ups a day as a way to break bad habits.\nEngage in problem solving: both in complicated problems and in complex problems.\nNote: Some people distinguish between complicated problems (such as learning math or a strategy to win at chess), and complex problems that require other mental skills besides analytical skills. For example, world hunger, the justice system or the Israel-Palestine 100 year conflict is a complex problem.\n2.3 Goal 3: spiritually or socially generous\nThis is probably the most important and the most fulfilling one, but probably the one that I am going to say the least to remain neutral or at least ecumenical. Meaning, capital M, will come from this goal. Purpose capital P. Who have you helped today that maybe didn’t deserve it? Maybe you wanted to talk to someone, and yet that someone needed you. Giving love, care, and attention is the highest form of living. Especially when it was not deserved, not calculated and therefore not for our own benefit.\nSo these are our goals, now comes the hard part, how to walk towards them. Again, the goals are not an end point, to keep walking towards those goals is the actual goal.\n3. The obstacles and how to overcome them\nOnce we have established our goal, the important component is how to achieve it, how to make it sustainable. The answer is as simple to describe as hard it is to achieve. Choose carefully, make it a routine, discard the rest.\nBut first we have to identify the obstacles that we will encounter and be able to put the pleasures of our comfortable lives in the right place.\n3.1 Maslow Pyramid of Cravings and St Thomas Aquinas’ idols.\n\nMaslow’s pyramid of needs\n\nSt. Thomas Aquinas\nIn the popular Maslow pyramid of needs not all levels are created the same. The top of the pyramid is not self-destructive, while the lower ones are. Here’s where I want to introduce St Thomas Aquinas’ four idols, four false gods. They are:\nRecognition and Fame\nPower\nMoney\nPleasure\nIt is easy to fall for these four idols, and while necessary, they cannot be a goal in themselves as they are bottomless, self-consuming and ultimately empty. We can almost build a one to one relationship between the lower levels of Maslow’s pyramid and the St Thomas’ four idols. Notice then that it is not the same at all to give love than to be loved. The need to feel loved is a craving that while it is fundamental to have, it can be self-destructive when made a goal in itself. To give love which can also be interpreted as to forget about yourself on the other hand is one of our goals, in fact, the most important one.\nDon’t get me wrong. Our bodies and minds need all these “low levels”. That’s why we crave them, but since we evolved in its scarcity we must have some systems in place to keep them in scarcity and not fall into them. This “falling into them” is often reffered as the dopamine or hedonic treadmill and why some people to escape it call it a “dopamine detox”. The mechanism of neurotransmitters responsible for pleasure and reward are more complicated than just one molecule, so we are just using the word dopamine as a label.\nIt is the controlled and metacognitive pleasure what turns pleasures into enjoyments. If you build metacognition and awareness to these pleasures they won’t be as self-destructive. One can enjoy a meal with family friends, and that enjoyment will be more fulfilling and lasting than eating a bag of chips while on the couch.\nThis system in place is what I call the cycle of pleasure and pain, the cycle of abundance and scarcity, the cycle of recovery and challenge.\n3.2 The dynamic cycle.\nThe key of the proposed cycle is homeostasis, it combines the sharpening with the necessary recovery. It can be understood as the perfect combination of hedonism and eudemonia, the combination may be different for different people, but both must be present. They are both equally important and one cannot exist without the other. It incorporates all our hedonic needs but it places them under strict restrictions of time, magnitude and metacognition. In other words, you must manage your cravings and your emotions instead of letting them manage you. Metacognition or at least your social boundaries will turn those cravings into experiences. Arthur Brooks says that pleasure becomes enjoyment once you add people and memories, but that seems to me a personal choice. The important part is that even these more “civilized experiences” cannot be constantly present, we must go back to a baseline, to a homeostasis through the cycle of pain and pleasure, sharpening and healing or using endurance training terms, adaptation and recovery.\n\nThe joyful struggle is a routine, a growth mindset cycle that combines healing and sharpening\nNote: Distinguish good stress from bad stress.\nWhen we don’t rest enough or our cycle is unbalanced, we will experience the bad stress, the one that overwhelms you. Here’s where I bring the picture of a lion chasing a gazelle, they are both under stress, but one has a lot to gain while the other has a lot to lose. Work on the cycle starting slow, the sharpening dose must be proportional to the resting you have.\n3.3 Quitting the rest.\nI am not advocating for an extreme spartan stoicism. Embrace and leave room for the enjoyment of those more hedonistic pleasures. But you must discard any elements that are not either enjoyable nor do not contribute to your sharpening and constant improvement. Especially the habits you cannot control. Some people may have one beer a month, others cannot control it. This is a personal choice. For example, to me social networks are not specially tempting, they don’t make me feel miserable, but if they do it for you, just quit. Maybe someone has a very sweet tooth, so then just quit carbohydrates, because they do not contribute to your healing side nor my sharpening side.\nDo you have a phone addiction? Do you need people’s approval? Whatever it is that wakes up your “monkey brain” identify your “dopamine treadmill” and quit, be intentional about the sources of your enjoyment, make sure they provide healing, and get used to the pain that comes from its scarcity and focus on the challenge.\n3.4 Distinguishing between achievement treadmills and true eudaimonia\nOne should be careful at quickly classifying activities as sharpening or healing, the roles may exchange. They are not absolute.\nBeing creative with art, problem solving, writing a poem can be an excellent way to heal. For myself, going out for a run can give me the mental rest I need. What matters is what role that specific activity has in your cycle at that moment.\nAlso, careful with the wrong sharpening activities. Even the sharpening can become addicting. A workaholic may not live a meaningful life because they are ruled by an addiction to the pleasure that satisfaction provides. I know of some academic colleagues who seem addicted to publishing research papers. This may sound a virtuous cycle, but in reality it can become just another addicting dopamine treadmill.\nIf the three areas are in contradiction (you can’t workout, or can’t create, due to social engagements), the most important one is the social one, the one that you give up yourself to others. It is the one that abandons your pride, which in the Christian doctrine is the worst of sins. David Brooks distinguishes between resume virtues and eulogy virtues. The eulogy virtues are the ones that allow you to sharpen your soul by giving your time and effort to others. Careful, this is not spending time with your friends as this would belong to the healing part of the cycle. This is going out of your comfort zone to dedicate your time and effort to other peoples’ needs. So one must be careful with the hidden dopamine treadmills like workaholics or people’s pleasers. Ultimately it depends on the motivation of those tasks.\nIn this sense a job can be a good hobby, and its rewards will give you satisfaction in the cycle used above, but it will be the connecting/transcendent/beauty part of that job (the one that allows you to be in the zone) that will give you meaning and purpose. This is the true eudaimonic life.\nAlso, as we said above because both physical and mental activities can become addictive themselves. People who just focus on their creative work or their physical performance will fall short. The artists or scientists that do their work because they are seeking power or recognition, or because they are addicted to the thrill, they are just on the productivity treadmill and are serving one of the Aquinas’ “false gods”. That is, the moment that this task detaches us from our spiritual generosity it becomes addicting and to be avoided. One cannot be addicted to being generous, because when you are connected to nature, the past, and to others you stop having control, this is why saying that it’s like “dancing to a music” is a good metaphor of letting yourself go and be in the zone…\nBibliography\nThe following books have triggered my thinking but by no means I have used them to write this little essay.\n“Outlive: The science and art of longevity” by Bill Gifford and Peter Attia\n“Road to character” by David Brooks\n“Dopamine nation: Finding balance in the age of indulgence” by Anna Lembke\n“From strength to strength” by Arthur C Brooks\nInterview with Peter Attia and Arthur Brooks: https://www.youtube.com/watch?v=X1GNc70-584\nInterview with David Brooks on how to live a meaningul life https://www.youtube.com/watch?v=-JRSLhClyvw\n\n\n\n",
    "preview": "posts/the_joyful_struggle/thecycle.png",
    "last_modified": "2024-07-05T12:59:03-05:00",
    "input_file": {},
    "preview_width": 1440,
    "preview_height": 432
  },
  {
    "path": "posts/Querying_GoogleScholar_with_Python/",
    "title": "Querying Google Scholar with Python",
    "description": "And build an always current list of publications in your department",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2024-07-01",
    "categories": [
      "Python",
      "Javascript"
    ],
    "contents": "\nBesides the faculty research sites, should a department be listing their faculty’s productivity on their site?\nA department may want to highlight their identity to future employees or visitors. For example,\nan academic unit with specific research agenda will benefit from showing in one place what their\nfaculty are currently working on.\nSome departments have enough budget that they can dedicate resources (and people) to manually selecting the publications of their faculty. If you leave it up to faculty to reach out to this administrative person you will probably end up with some gaps (as some members may not communicate their publications as often as others). On the other hand, if you leave it up to the department to select what publications\nmust be highlighted, unless this person is an expert on the field there may be some gaps as well.\nSo I found a compromise between the two options. One can build a Google Scholar’s profile (https://scholar.google.com/intl/en/scholar/citations.html). GoogleScholar will automatically add\nnew publications and the faculty member can decide what to include and what to remove. The good\nnews is that even if the faculty member doesn’t do anything, as long as they create the profile once,\nthe list of publications will keep updating.\nThis approach can work because there is a python library called scholarly (https://pypi.org/project/scholarly/) that can query Google Scholar.\nI created a python script that queries all the GoogleScholar profiles of our faculty. The python script creates a JSON file for each author and I wrote a HTML/JS “very bare bones” interface that allows the user\nto navigate the large list of publications and filter them by year, person, topic…etc\nBut hey, there is even an interactive chartJS graph that can count how many publications related to “learning research” there are per year.\n\nInteractive chart on the website http://chemdata.r.umn.edu/scholar/\nAll the code is on github (https://github.com/xavierprat/scholarly-at-UMR?tab=readme-ov-file) and a live example of the interface is available on my site http://chemdata.r.umn.edu/scholar/ . Check the Github repository for instructions about customization and installation.\nThe python script needs be run everytime you want the list to be updated so you may want to setup a crontab on your server to periodically query GoogleScholar.\nIf you try it and have questions, just let me know!\n\n\n\n",
    "preview": "posts/Querying_GoogleScholar_with_Python/chart.png",
    "last_modified": "2024-07-05T09:47:37-05:00",
    "input_file": {},
    "preview_width": 1162,
    "preview_height": 1082
  },
  {
    "path": "posts/calculating_pmf_with_umbrella/",
    "title": "Calculate the Potential of Mean Force from MD of an enzymatic reaction",
    "description": "Parsing files, plotting coordinates, calculating free energy",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2021-08-17",
    "categories": [
      "Modeling",
      "R"
    ],
    "contents": "\n\nContents\nIntroduction\nThe reaction and the reaction coordinates\nLoad trajectory files and measure time-dependent magnitudes\nAnalysis of reaction coordinate: nucleophilic addition/elimination O - C - O\nDynamic analysis\nUmbrella Sampling\n\nAnalysis of additional reaction coordinates: proton transfers\nDynamic analysis\n\nConclusion\n\nIntroduction\nOne of the daunting tasks when running molecular simulations with thousands of atoms is to understand the results of your calculations. You have many files, long simulations, and it may be hard to reduce all that information into the chemistry of arrow pushing mechanism that you are used to. Granted that if you have the time you want to spend it looking at the actual structures on VMD/PyMol, but your supervisor/collaborator doesn’t have time for that and wants to see graphs, tables and evidence of what you are talking about.\nHere I will try to show some basic analysis of an umbrella sampling simulation to calculate the free energy profile along a reaction coordinate. Mostly the purpose of this post is to show some basic R scripting to manipulate and plot data.\nThe reaction and the reaction coordinates\nThe reaction to study is a trans-acetalization. In terms of organic chemistry is a nucleophilic addition and nucleophilic elimination. The goal here is to understand if that happens through a stable intermediate and what level of synchronocity exists between the distances labeled as “A3” and “A4”.\n\nThe active site of the reaction we want to study\nOur molecular dynamics simulations were based on the scanning of the asymmetric stretch coordinate (A3-A4) under an umbrella potential.\nLoad trajectory files and measure time-dependent magnitudes\nConsidering that we used CHARMM to run the simulations we will use CHARMM’s scripting analysis tools. Alternative tools such as MDAnalysis are very powerful, but for the sake of pedagogy I will use a DIY approach, even if it takes us longer. The following is a CHARMM script that loads the trajectory file dcd and measures the distances involved in the reaction coordinate.\n!load the dcd trajectories\nopen unit 51 read name @inputdir/@window.dcd\n\n!Open data output file\nopen write form UNIT 28 name @d/@window.a3.data\nopen write form UNIT 29 name @d/@window.a4.data\nopen write form UNIT 30 name @d/@window.515o2_h.data\nopen write form UNIT 31 name @d/@window.h_o3.data\nopen write form UNIT 32 name @d/@window.477o1_h.data\nopen write form UNIT 33 name @d/@window.515o1_h.data\nopen write form UNIT 34 name @d/@window.477o2_h.data\n\nCORREL MAXSERIES 34 MAXTIMESTEPS 60000 MAXATOMS 8500\n\nENTER A3 DIST sele ((resid 1 .and. type O3 .and. segid S).or.(resid 3 .and. type C1 .and. segid G)) end\nENTER A4 DIST sele ((resid 477 .and. type OD1 .and. segid C).or.(resid 3 .and. type C1 .and. segid G)) end\nENTER D1 DIST sele ((resid 515 .and. type OE2 .and. segid C).or.(resid 1 .and. type HO3 .and. segid S)) end\nENTER D2 DIST sele ((resid 1 .and. type O3 .and. segid S).or.(resid 1 .and. type HO3 .and. segid S)) end\nENTER D3 DIST sele ((resid 477 .and. type OD1 .and. segid C).or.(resid 1 .and. type HO3 .and. segid S)) end\nENTER D4 DIST sele ((resid 515 .and. type OE1 .and. segid C).or.(resid 1 .and. type HO3 .and. segid S)) end\nENTER D5 DIST sele ((resid 477 .and. type OD2 .and. segid C).or.(resid 1 .and. type HO3 .and. segid S)) end\n\nTRAJ  FIRSTU 51 NUNIT 1 begin 1 \n!If your run didn't finish, you need to tell traj when to stop:\n!TRAJ  FIRSTU 51 NUNIT 1 begin 1 stop 8580\n\nwrite A3 unit 28 dumb time\nwrite A4 unit 29 dumb time\nwrite D1 unit 30 dumb time\nwrite D2 unit 31 dumb time\nwrite D3 unit 32 dumb time\nwrite D4 unit 33 dumb time\nwrite D5 unit 34 dumb time\n\nstop\nThe code above needs to be applied to each window of simulation which will give reaction coordinate files for each window. We will use R to load all these files and analyze them.\n\n\nShow code\n\ngetFiles <- function(myPath,pattern){\n  setwd(myPath)\n  listOfFiles = Sys.glob(\"eqq*.data\")\n  listOfFiles = listOfFiles[grepl(pattern,listOfFiles)]\n  n = listOfFiles[grepl(\"n\",listOfFiles)]\n  p = listOfFiles[grepl(\"p\",listOfFiles)]\n  listOfFiles = c(sort(n,decreasing = TRUE),p)\n  return(listOfFiles)\n}\nbuildDataFrame <- function(maxlen,myPath, removepat, filenames){\n  setwd(myPath)\n  thisDataFrame <- data.frame(matrix(NA,nrow = maxlen, ncol = length(filenames)))\n  for (i in 1:length(filenames)){\n    thisCol = read.table(filenames[i],header = FALSE)\n    thisColName = gsub(removepat,\"\",filenames[i])\n    names(thisDataFrame)[i] = thisColName\n    #R must have a function, but thisCol may be shorter than maxlen, so better to do it one by one\n    for (j in 1:length(thisCol$V2)){\n      thisDataFrame[[thisColName]][j] = thisCol$V2[j]\n    }\n  }\n  return(thisDataFrame)\n}\n\nmyPath = \"/Users/xavier/Gd/Research/Chile/Camilo/analysis_dist/\"\na3_files = getFiles(myPath,\"a3\")\na4_files = getFiles(myPath,\"a4\")\n\n#this assumes that the first window of the list has the maximum length (a finished job)\n#some windows were not run to the finish so they will have NA at the end\nsample = read.table(a3_files[1],header = FALSE)\nmaxlen = length(sample$V2)\na3 = buildDataFrame(maxlen,myPath,\".a3.data\",a3_files)\na4 = buildDataFrame(maxlen,myPath,\".a4.data\",a4_files)\na3a4 = a3-a4\n\nfe_a3a4 = read.csv(\"/Users/xavier/Gd/Research/Chile/Camilo/10ns/free_energy\",header = FALSE, sep = \"\\t\")\n\n\n\nAnalysis of reaction coordinate: nucleophilic addition/elimination O - C - O\nDynamic analysis\nA brief statistical summary (using R’s function summery) can give information about each window:\n\n\nShow code\n\nfor (i in 1:ncol(a3a4)){\n   print(paste(\"Showing results for the Rc=A3-A4 for window:\",names(a3a4)[i],sep = \"\"))\n   print(summary(a3a4[,i]))\n}\n\n\n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n17\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.870  -1.749  -1.716  -1.716  -1.683  -1.575 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n15\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.676  -1.554  -1.522  -1.522  -1.491  -1.370 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n13\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.503  -1.371  -1.338  -1.338  -1.304  -1.183 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n11\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n -1.291  -1.176  -1.143  -1.144  -1.114  -1.010 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n09\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-1.1195 -1.0145 -0.9868 -0.9857 -0.9554 -0.8627     142 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n07\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.9724 -0.8650 -0.8388 -0.8377 -0.8073 -0.6646 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n05\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.8267 -0.6944 -0.6540 -0.6543 -0.6174 -0.4623 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n04\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n-0.7823 -0.5912 -0.5475 -0.5481 -0.5018 -0.3225     107 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n03\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n-0.6324 -0.4406 -0.3930 -0.3934 -0.3449 -0.1337 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n02\"\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.40947 -0.27016 -0.22751 -0.22572 -0.18576 -0.00251 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_n01\"\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n-0.30212 -0.12723 -0.08176 -0.08421 -0.03752  0.12434 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p00\"\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.151132  0.003918  0.042896  0.043282  0.084240  0.225363 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p01\"\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n-0.008844  0.129696  0.170706  0.169833  0.208377  0.365419 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p02\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.1217  0.3071  0.3493  0.3496  0.3943  0.5379 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p04\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.3570  0.4893  0.5232  0.5237  0.5586  0.6624 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p06\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.5663  0.6972  0.7309  0.7298  0.7635  0.8702 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p07\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.6487  0.7753  0.8095  0.8089  0.8429  0.9580 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p09\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.7698  0.9134  0.9479  0.9487  0.9829  1.1488     339 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p11\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9461  1.0755  1.1146  1.1131  1.1487  1.2664 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p13\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.145   1.271   1.308   1.306   1.343   1.498 \n[1] \"Showing results for the Rc=A3-A4 for window:eqq_p15\"\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.350   1.455   1.488   1.491   1.527   1.657 \n\nIn addition to the above average data and in order to get a more visual and dynamic overview of what the reaction coordinates look like in each of our windows we can plot the two distances that make up our reaction coordinate. A3 in red and A4 in blue.\n\n\nShow code\n\n#this builds a 3x3 lattice. oma and mar take care of margins\npar(mfrow=c(3,3),\n    oma = c(5,4,0,0) + 0.1,\n    mar = c(0,0,1,1) + 0.1)\n\n\nfor (i in 1:length(a3_files)){\n  #I'm using ohfiles just for naming and finding windows\n  colname = gsub(\".a3.data\",\"\",a3_files[i])\n  #only show y-axis numbers on the first graph of the row\n  #print(colname)\n  if ( (i+2)%%3 == 0){\n    plot( a3[[colname]],ylim=c(0.5,3.5),type=\"l\",col=\"red\" ,ann = FALSE,xaxt='n')\n    lines(a4[[colname]],ylim=c(0.5,3.5),type=\"l\",col=\"blue\",ann = FALSE,xaxt='n')\n  }else{\n    plot( a3[[colname]],ylim=c(0.5,3.5),type=\"l\",col=\"red\" ,ann = FALSE,xaxt='n',yaxt='n')\n    lines(a4[[colname]],ylim=c(0.5,3.5),type=\"l\",col=\"blue\",ann = FALSE,xaxt='n')\n  }\n  rcValue = gsub(\"eqq_\",\"\",colname)\n  graphTitle = paste(rcValue,\"Avg:A3=\",round(mean(a3[[colname]], na.rm = TRUE),digits = 2),\n                                \" A4=\",round(mean(a4[[colname]], na.rm = TRUE),digits = 2),sep = \"\")\n  title(graphTitle,line=-7)\n}\n\n\n\n\nThe chemistry question is wether the nucleophilic attack (A4: blue) happens at the same time as the elimination (A3: red) or not. The elimination (A3) only starts happening once the attacking nucleophile (A4 blue) is close enough at window n04. It is also true that while there is a tetra-coordinate intermediate at window p00, the distances do not belong to a regular O-C distance (1.5) and they are rather long around 2.0. This fact allows to predict without looking a the free energy that it cannot possibly be a stable intermediate. Therefore the nucleophilic/elimination reaction happens rather synchronously and in one step.\nUmbrella Sampling\nWhen plotting a histogram one can build a histogram of all windows at the same time:\n\n\nShow code\n\ndens = hist(data.matrix(a3a4),breaks = 300,plot = FALSE)\nplot(dens$mids,dens$density,type=\"l\",main=\"A3-A4\")\n\n\n\n\nNotice that the above histograms does not distinguish between windows. In order to see the overlap between windows we will build a histogram for each window.\n\n\nShow code\n\nplot(0,0,xlim = c(-2,1.70),ylim = c(0,200),type = \"n\")\ncl = rainbow(ncol(a3a4))\n## here you set the font size \nop <- par(cex = 0.5)\nfor (i in 1:ncol(a3a4)){\n  dens = hist(a3a4[,i],breaks = 25,plot = FALSE)\n  lines(dens$mids,dens$counts,col=cl[i],type='l')\n}\nlegend(\"topleft\",legend = names(a3a4),text.col = cl)\n\n\n\n\nThis representation allows us to see that there’s a gap in some values at R=0.6. This is bad because each window must significantly overlap with the neighboring windows. The reason is the integration constants: \\[ \\Delta G = -RT ln(\\rho) + Constant\\] In the straight umbrella sampling (not WHAM) we make it so that the G of the two neighboring windows are the same at the overlapping point. If there’s no overlapping the free energy is not a continuous function. We are going to continue with the analysis but in a real context we would have to run more windows to cover those gaps shown in the histograms above.\nWe can still plot the free energy along this coordinate using the WHAM method. We will use Alan Grossfield’s WHAM code.\n\n\nShow code\n\nplot(fe_a3a4$V1,fe_a3a4$V2)\n\n\n\n\nAnalysis of additional reaction coordinates: proton transfers\nThe fact that we were able to obtain an apparently continous free energy profile does not necessarily mean that the chosen reaction coordinate (A3-A4) is the right one. We know that proton transfers play a significant role and it could well be that while are scanning A3-A4 the protons are switching back and forth to accommodate the scanning rather than following the true reaction path.\nDynamic analysis\nWe know that there’s a proton than can jump from three different sites: Asp477, Glu515, and O3 (the substrate leaving group)\nLet’s start by loading up the files and calculate basic statistical for each window.\n\n\nShow code\n\n#Get file names\no1_477h_files = getFiles(myPath,\"477o1_h\")\no2_477h_files = getFiles(myPath,\"477o2_h\")\no1_515h_files = getFiles(myPath,\"515o1_h\")\no2_515h_files = getFiles(myPath,\"515o2_h\")\no3h_files = getFiles(myPath,\"h_o3\")\n\no1_477h = buildDataFrame(maxlen,myPath,\".477o1_h.data\",o1_477h_files)\no2_477h = buildDataFrame(maxlen,myPath,\".477o2_h.data\",o2_477h_files)\no1_515h = buildDataFrame(maxlen,myPath,\".515o1_h.data\",o1_515h_files)\no2_515h = buildDataFrame(maxlen,myPath,\".515o2_h.data\",o2_515h_files)\nh_o3 =    buildDataFrame(maxlen,myPath,\".h_o3.data\",o3h_files)\n\n\n\nA carboxylic acid has two potential oxygens that can hold a proton. We must first make sure that we are monitoring the right O-H distance. In other words, identify the oxygen that is holding or interacting with the proton.\nFor that we can have a quick look at averages and minimum distances during each trajectory.\n\n\nShow code\n\nfor (i in 1:length(o1_477h_files)){\n  colname = gsub(\".477o1_h.data\",\"\",o1_477h_files[i])\n  myLine = paste(colname,\"AspO1-H Ave\",sprintf('%.2f',mean(o1_477h[[colname]],na.rm = TRUE)),\n                                 \"Min\",sprintf('%.2f',min(o1_477h[[colname]],na.rm = TRUE)),\n                         \"AspO2-H Ave\",sprintf('%.2f',mean(o2_477h[[colname]],na.rm = TRUE)),\n                                 \"Min\",sprintf('%.2f',min(o2_477h[[colname]],na.rm = TRUE)),\n                 sep = \" : \")\n  print(myLine)\n}\n\n\n[1] \"eqq_n17 : AspO1-H Ave : 1.75 : Min : 1.20 : AspO2-H Ave : 3.52 : Min : 2.85\"\n[1] \"eqq_n15 : AspO1-H Ave : 2.00 : Min : 1.10 : AspO2-H Ave : 3.86 : Min : 2.89\"\n[1] \"eqq_n13 : AspO1-H Ave : 1.84 : Min : 1.12 : AspO2-H Ave : 3.94 : Min : 3.23\"\n[1] \"eqq_n11 : AspO1-H Ave : 1.18 : Min : 1.00 : AspO2-H Ave : 3.25 : Min : 2.91\"\n[1] \"eqq_n09 : AspO1-H Ave : 1.18 : Min : 1.00 : AspO2-H Ave : 3.26 : Min : 2.96\"\n[1] \"eqq_n07 : AspO1-H Ave : 2.28 : Min : 1.01 : AspO2-H Ave : 4.31 : Min : 3.05\"\n[1] \"eqq_n05 : AspO1-H Ave : 3.48 : Min : 2.62 : AspO2-H Ave : 5.47 : Min : 4.66\"\n[1] \"eqq_n04 : AspO1-H Ave : 3.51 : Min : 2.53 : AspO2-H Ave : 5.52 : Min : 4.51\"\n[1] \"eqq_n03 : AspO1-H Ave : 3.56 : Min : 2.77 : AspO2-H Ave : 5.49 : Min : 4.68\"\n[1] \"eqq_n02 : AspO1-H Ave : 3.74 : Min : 3.19 : AspO2-H Ave : 5.58 : Min : 4.76\"\n[1] \"eqq_n01 : AspO1-H Ave : 3.84 : Min : 3.32 : AspO2-H Ave : 5.63 : Min : 4.88\"\n[1] \"eqq_p00 : AspO1-H Ave : 3.91 : Min : 3.35 : AspO2-H Ave : 5.56 : Min : 4.79\"\n[1] \"eqq_p01 : AspO1-H Ave : 3.89 : Min : 3.38 : AspO2-H Ave : 5.53 : Min : 4.68\"\n[1] \"eqq_p02 : AspO1-H Ave : 4.03 : Min : 3.46 : AspO2-H Ave : 5.71 : Min : 5.06\"\n[1] \"eqq_p04 : AspO1-H Ave : 3.74 : Min : 3.29 : AspO2-H Ave : 5.34 : Min : 4.32\"\n[1] \"eqq_p06 : AspO1-H Ave : 3.61 : Min : 3.19 : AspO2-H Ave : 5.43 : Min : 4.96\"\n[1] \"eqq_p07 : AspO1-H Ave : 3.63 : Min : 3.20 : AspO2-H Ave : 5.44 : Min : 4.99\"\n[1] \"eqq_p09 : AspO1-H Ave : 3.67 : Min : 3.20 : AspO2-H Ave : 5.49 : Min : 5.03\"\n[1] \"eqq_p11 : AspO1-H Ave : 3.74 : Min : 3.23 : AspO2-H Ave : 5.57 : Min : 4.94\"\n[1] \"eqq_p13 : AspO1-H Ave : 3.75 : Min : 3.22 : AspO2-H Ave : 5.66 : Min : 4.99\"\n[1] \"eqq_p15 : AspO1-H Ave : 3.94 : Min : 3.35 : AspO2-H Ave : 5.79 : Min : 5.17\"\n\nSo we can conclude that Asp477 has “O1” as the active oxygen because the O2-H distance is always longer and therfore irrelevant.\n\n\nShow code\n\nfor (i in 1:length(o1_515h_files)){\n  colname = gsub(\".515o1_h.data\",\"\",o1_515h_files[i])\n  myLine = paste(colname,\"GluO1-H Ave\",sprintf('%.2f',mean(o1_515h[[colname]],na.rm = TRUE)),\n                                 \"Min\",sprintf('%.2f',min(o1_515h[[colname]],na.rm = TRUE)),\n                         \"GluO2-H Ave\",sprintf('%.2f',mean(o2_515h[[colname]],na.rm = TRUE)),\n                                 \"Min\",sprintf('%.2f',min(o2_515h[[colname]],na.rm = TRUE)),\n                 sep = \" : \")\n  print(myLine)\n}\n\n\n[1] \"eqq_n17 : GluO1-H Ave : 2.47 : Min : 2.24 : GluO2-H Ave : 1.10 : Min : 0.95\"\n[1] \"eqq_n15 : GluO1-H Ave : 2.47 : Min : 2.21 : GluO2-H Ave : 1.08 : Min : 0.94\"\n[1] \"eqq_n13 : GluO1-H Ave : 2.43 : Min : 2.17 : GluO2-H Ave : 1.09 : Min : 0.95\"\n[1] \"eqq_n11 : GluO1-H Ave : 2.63 : Min : 2.25 : GluO2-H Ave : 1.37 : Min : 1.07\"\n[1] \"eqq_n09 : GluO1-H Ave : 2.66 : Min : 2.24 : GluO2-H Ave : 1.40 : Min : 1.07\"\n[1] \"eqq_n07 : GluO1-H Ave : 2.56 : Min : 2.23 : GluO2-H Ave : 1.20 : Min : 0.94\"\n[1] \"eqq_n05 : GluO1-H Ave : 2.45 : Min : 2.14 : GluO2-H Ave : 1.07 : Min : 0.94\"\n[1] \"eqq_n04 : GluO1-H Ave : 2.45 : Min : 2.13 : GluO2-H Ave : 1.08 : Min : 0.94\"\n[1] \"eqq_n03 : GluO1-H Ave : 2.48 : Min : 2.19 : GluO2-H Ave : 1.18 : Min : 0.95\"\n[1] \"eqq_n02 : GluO1-H Ave : 2.53 : Min : 2.16 : GluO2-H Ave : 1.31 : Min : 1.04\"\n[1] \"eqq_n01 : GluO1-H Ave : 2.59 : Min : 2.19 : GluO2-H Ave : 1.43 : Min : 1.10\"\n[1] \"eqq_p00 : GluO1-H Ave : 2.56 : Min : 2.07 : GluO2-H Ave : 1.48 : Min : 1.15\"\n[1] \"eqq_p01 : GluO1-H Ave : 2.25 : Min : 1.29 : GluO2-H Ave : 1.89 : Min : 1.23\"\n[1] \"eqq_p02 : GluO1-H Ave : 1.85 : Min : 1.26 : GluO2-H Ave : 2.35 : Min : 1.45\"\n[1] \"eqq_p04 : GluO1-H Ave : 2.03 : Min : 1.36 : GluO2-H Ave : 2.22 : Min : 1.41\"\n[1] \"eqq_p06 : GluO1-H Ave : 2.76 : Min : 2.29 : GluO2-H Ave : 1.61 : Min : 1.17\"\n[1] \"eqq_p07 : GluO1-H Ave : 2.71 : Min : 2.28 : GluO2-H Ave : 1.59 : Min : 1.19\"\n[1] \"eqq_p09 : GluO1-H Ave : 2.68 : Min : 1.73 : GluO2-H Ave : 1.56 : Min : 1.20\"\n[1] \"eqq_p11 : GluO1-H Ave : 2.65 : Min : 2.03 : GluO2-H Ave : 1.56 : Min : 1.22\"\n[1] \"eqq_p13 : GluO1-H Ave : 2.42 : Min : 1.30 : GluO2-H Ave : 2.06 : Min : 1.35\"\n[1] \"eqq_p15 : GluO1-H Ave : 1.69 : Min : 1.35 : GluO2-H Ave : 2.72 : Min : 2.30\"\n\nFrom the above we see that both oxygens of Glu515 are active at a different times of the reaction.\nFinally we can look at when the proton makes it to its final destination, the leaving group O3.\n\n\nShow code\n\nfor (i in 1:length(o3h_files)){\n  colname = gsub(\".h_o3.data\",\"\",o3h_files[i])\n  myLine = paste(colname,\"O3-H Ave\",sprintf('%.2f',mean(h_o3[[colname]],na.rm = TRUE)),\n                                 \"Min\",sprintf('%.2f',min(h_o3[[colname]],na.rm = TRUE)),\n                 sep = \" : \")\n  print(myLine)\n}\n\n\n[1] \"eqq_n17 : O3-H Ave : 2.62 : Min : 1.92\"\n[1] \"eqq_n15 : O3-H Ave : 2.30 : Min : 1.46\"\n[1] \"eqq_n13 : O3-H Ave : 2.92 : Min : 1.80\"\n[1] \"eqq_n11 : O3-H Ave : 3.47 : Min : 2.88\"\n[1] \"eqq_n09 : O3-H Ave : 2.90 : Min : 2.39\"\n[1] \"eqq_n07 : O3-H Ave : 2.23 : Min : 1.35\"\n[1] \"eqq_n05 : O3-H Ave : 1.61 : Min : 1.21\"\n[1] \"eqq_n04 : O3-H Ave : 1.61 : Min : 1.21\"\n[1] \"eqq_n03 : O3-H Ave : 1.40 : Min : 1.03\"\n[1] \"eqq_n02 : O3-H Ave : 1.23 : Min : 0.98\"\n[1] \"eqq_n01 : O3-H Ave : 1.16 : Min : 0.98\"\n[1] \"eqq_p00 : O3-H Ave : 1.13 : Min : 0.96\"\n[1] \"eqq_p01 : O3-H Ave : 1.09 : Min : 0.92\"\n[1] \"eqq_p02 : O3-H Ave : 1.06 : Min : 0.93\"\n[1] \"eqq_p04 : O3-H Ave : 1.06 : Min : 0.94\"\n[1] \"eqq_p06 : O3-H Ave : 1.10 : Min : 0.92\"\n[1] \"eqq_p07 : O3-H Ave : 1.10 : Min : 0.94\"\n[1] \"eqq_p09 : O3-H Ave : 1.09 : Min : 0.95\"\n[1] \"eqq_p11 : O3-H Ave : 1.08 : Min : 0.95\"\n[1] \"eqq_p13 : O3-H Ave : 1.06 : Min : 0.93\"\n[1] \"eqq_p15 : O3-H Ave : 1.06 : Min : 0.95\"\n\nWe can see that it arrives as soon as the window Rc=-0.4 and it stays there for the rest of the reaction.\nSo it looks like in terms of proton transfer lots of things are happening among the four oxygens being mentioned.\nExcept for Asp477O2-H distance identified as not relevant, the other four will be taken into account in the below representation: The Asp477-O1 (red), the two Glu515-O (1:blue and 2:green) and the leaving group O3(black).\n\n\nShow code\n\n#this builds a 3x3 lattice. oma and mar take care of margins\npar(mfrow=c(2,3),\n    oma = c(5,4,0,0) + 0.1,\n    mar = c(0,0,1,1) + 0.1)\n\nfor (i in 1:length(h_o3)){\n  #I'm using ohfiles just for naming and finding windows\n  colname = gsub(\".h_o3.data\",\"\",o3h_files[i])\n  #only show y-axis numbers on the first graph of the row\n  #print(colname)\n  if ( (i+2)%%3 == 0){\n    plot(o1_477h[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"red\",ann = FALSE,xaxt='n')\n    lines(o1_515h[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"blue\",ann = FALSE,xaxt='n')\n    lines(o2_515h[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"green\",ann = FALSE,xaxt='n')\n    lines(h_o3[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"black\",ann = FALSE,xaxt='n')\n  }else{\n    plot(o1_477h[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"red\",ann = FALSE,xaxt='n',yaxt='n')\n    lines(o1_515h[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"blue\",ann = FALSE,xaxt='n',yaxt='n')\n    lines(o2_515h[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"green\",ann = FALSE,xaxt='n',yaxt='n')\n    lines(h_o3[[colname]],ylim=c(0.5,4.5),type=\"l\",col=\"black\",ann = FALSE,xaxt='n',yaxt='n')\n  }\n  rcValue = gsub(\"eqq_\",\"\",colname)\n  graphTitle = rcValue\n                            \n  title(graphTitle)\n}\n\n\n\n\nLet’s read carefully the above representations, from products p15 to reactants n17\np15: Hydrogen is already in O3 and Glu515-O1 is hydrogen bonded to it\np01 to p13: The roles of the two Glu515 oxygens to be hydrogen bonded to the H-O3 is switched.The window p02 is specially remarkable as it switches twice. This is not ideal. You want to see\nn05 to p00: This is the transition state area. There seems to be only Glu515-O2 participating with the transfer.\nn07: This window is particularly problematic because it shows a different H-bonding pattern at the beginning and at the end. This window starts with the bonding similar to n05 but it ends with the bonding of n09.\nn09 to n11: Asp477 is holding the proton during these two windows and it releases it back to Glu515O2 in n13,n15, and n17. This step doesn’t make much chemical sense because Asp477 is supposed to be the nucleophile by accepting a proton its nucleophilic character is diminished.\nConclusion\nIt is true that we were able to obtain a smooth free energy profile that took us from reactants to products. But at this point the work is not done. Some of the proton transfers in the negative side of the reaction coordinate, specially n11 to n07 will require longer simulations and discard any non-equilibrium arrangements. The goal here is to make sure that we can justify each proton transfer as a genuine and chemically justifiable rather than an artifact of scanning an incomplete reaction coordinate pulled across different energy valleys.\n\n\n\n",
    "preview": "posts/calculating_pmf_with_umbrella/distill-preview.png",
    "last_modified": "2022-08-15T14:56:10-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/calculating_pmf/",
    "title": "The Simplest Potential of Mean Force using Molecular Dynamics ",
    "description": "Using the rotation of ethane to understand some tools from statistical mechanics.",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2021-07-15",
    "categories": [
      "Modeling",
      "R"
    ],
    "contents": "\n\nContents\n1 What is this?\n2 Some theory: reaction paths, free energy, and molecular dynamics\n3 Studying the rotation of ethane: Free energy\n3.1 Analysis of a 1ns trajectory\n3.2 Option 1: Effect of increasing the temperature\n3.3 Option 2: Longer dynamics\n\n4 Studying the different options for the PMF\n\n\n\nShow code\n\ncalcBarrier <- function(dens,init,endit){\n  #this function calculates the barrier in a dens$log array\n  #the dens array comes from the hist function, the log is calculated separately\n  #init is the first angle and endit is the last angle\n  firstindex = which.min( abs(dens$mids - init ) )\n  lastindex = which.min( abs(dens$mids - endit ) )\n  DE= max(dens$log[firstindex:lastindex]) - min(dens$log[firstindex:lastindex])\n  return(DE)\n}\nop = function(x, d=2) sprintf(paste0(\"%1.\",d,\"f\"), x)\n\n\n\n1 What is this?\nThe goal of this post is exclusively pedagogical: learning to calculate PMFs in the simplest system for chemists. Some context here, you have a background in physical chemistry and computational chemistry but never really quite understood the meaning of free energy of reaction, aka Potential of Mean Force (PMF) and how to use molecular simulations to calculate it.\nI want to give a very quick overview of tools from statistical thermodynamics to study reactions. There are lots of textbooks covering this material and there’s online tutorials with complex biological systems. But I wonder if I can cover the simplest of the systems (conformational rotation of ethane) with the simplest of the approaches (unbiased molecular dynamics).\nFor higher barriers and higher number of atoms you will have to use more sophisticated approaches to calculate the free energy paths (check this review). And that may be the main roadblock for students who want to understand it. You are told to use tools such as metadynamics or string-based replica exchange dynamics without really knowing why. You may feel like you are not grasping the fundamentals… or at least that’s how I felt back when I was a grad student.\n2 Some theory: reaction paths, free energy, and molecular dynamics\nIn computational chemistry we are interested in identifying the molecular structures that participate in a chemical reaction, what we call a reaction mechanism or reaction path that connects reactants and products. Invoking the second law of thermodynamics (principle of minimum energy) we identify the lowest energy structures as the most stable and therefore the most likely path. Sometimes a good estimation is to calculate the potential energy of the system. Or at most you may just need to approximate your surface with a parabola (build a Hessian, diagonalize it, get the frequencies and calculate the harmonic vibrational partition function). You probably have run a gaussian freq calculation before?\nBut that won’t be the case if your system is wiggly and floppy (read here that your reactants is a collection of conformers or a rugged surface, not a single deep minimum) or your reaction requires a large rearrangement (high entropy change between reactants and transition state). That’s when you need to sample the phase space. The phase space is what we call all the positions and velocities available at a given temperature.\nI’m going to throw academic rigor out of the window and I will simplify the equations saying that the free energy along a coordinate “r” can be obtained by calculating the density (\\(\\rho(r)\\) : think of a histogram of probability) along that coordinate “r”\n\\[ G(r) = -RT ln(\\rho(r)) \\] In other words values of “r” that are more probable will show higher density and therefore lower free energy. In other words, this density is just a histogram of the values of your reaction coordinate during a molecular dynamics simulation.\nI remember being confused about this result. How could it be that just a histogram of a coordinate could contain the precious free energy information. To answer that question we need to unpack this density function.\nThe density \\(\\rho(r)\\) has in it all the thermal and stability information about the path:\n\\[\n\\rho(r) = \\frac{\\int{ \\delta(q-r) e^{(-V(q)/RT)} dq} }{\\int{ e^{(-V(q)/RT)} dq}}\n\\] “V(q)” is the potential energy of the system, “q” is all the coordinates of our system and the symbol \\(\\delta(q-r)\\) just Dirac’s delta function, which is just a way to say that we’re just counting states of a given value “r”.\nLet me translate that into English. Because we are going to be running a molecular dynamics simulation at a given temperature, the states that the simulation visits are already following the Boltzmann factor \\(e^{(-V(q)/RT)}\\). This means that high energy states are less visited than low energy states, which is exactly what the integral is doing.\nNotice that the integral is overall all positions. We hope we’ll visit all positions after some longish simulation. This is, simplified, the ergodic hypothesis, we will converge the integral (summation of states) that goes over all cases if we run the simulation over a long enough time. It is not exactly true that all of the states that we visit in a molecular dynamics run they all belong to the statistical ensemble. Two consecutive steps are too correlated and they probably overrepresenting that area of the phase space. This is why we will usually take measurements every “x” steps of a dynamics run. The longer “x” is the less correlated two measurements will be, but the longer the dynamics will need to be run. The objective of this strategy is to collect a true Markov chain.\nFor a useful discussion on ergodic hypothesis using molecular dynamics check this article.\n\n3 Studying the rotation of ethane: Free energy\nDisclaimer: The very concept of Temperature and some of the assumptions from Statistical Mechanics are not valid for a single molecule simulation. That being said, it is still conceptually useful to study a single molecule of ethane for how fast calculations are and how conceptually simple the system is.\nWe are a running a simple molecular dynamics simulations with the NAMD code. You may download the input and outputs used in this post here We will use VMD to load the trajectory file (dcd) and save the dihedral data. Assuming we’ve done all that, let the analysis begin!\n3.1 Analysis of a 1ns trajectory\n\n\nShow code\n\nsetwd(\"~/Teaching/Pchem/S18/Pchem_SharedFolder/10_Modeling/ethane\")\ndihed600 = read.table(\"./dihedral.txt\")\ndihed300 = read.table(\"./dihedral_300.txt\")\n\n\n\nLet’s plot how the dihedral evolves during a 1 ns simulation (1fs/step = 1E6 steps). We have saved the structure every 5 steps, which means we have 1E6/5 = 2E5 structures.\n\n\nShow code\n\nplot( dihed300$V2,type=\"l\",main=\"Ethane dihedral @300K/1ns\",ylab=\"Dihedral\",xlab=\"Structure/5steps\")\n\n\n\nShow code\n\n#legend(\"bottomleft\",legend=c(\"Ethane Dihedral 300K\"))\n\n\n\nSo, not much is happening, during most of the 1ns simulation, the dihedral remains at -60 and by structure 150000 it switches to +60 with some swing back and forth.\nThis same information can be obtained by calculating a histogram\n\n\nShow code\n\nhist(dihed300$V2,breaks=120,main=\"Ethane dihedral @300K/1ns\",xlab=\"dihedral\")\n\n\n\nShow code\n\n#legend(\"topright\",legend=c(\"Ethane Dihedral 300K\"))\n\n\n\nThis is a somewhat unexpected result. We would liked to see three peaks corresponding to three minima where the staggered conformation of ethane is most stable. The first conclusion that one may extract is that the dynamics has not explored all the available phase space given the 1ns simulation and therefore given that we have not converged the phase space integral we cannot calculate the free energy. So, we have two options if we want to explore it further:\nOption 1 is to increase the temperature\nOption 2 is to run the simulation longer than 1ns\nOption 3 is to add some additional potential or bias that makes the simulations explore other regions.\nIn this post we will explore option 1 and 2.\n3.2 Option 1: Effect of increasing the temperature\nIf we are interested in measuring the free energy barrier of the dihedral rotation at 300K. Increasing the temperature will improve our exploration of the phase space, but it won’t be the same ensemble as in 300K and therefore the free energy won’t be the same. But for the sake of argument, let’s just compare how much more the dihedral changes when increasing the temperature Remember, temperature for 8 atoms is ill-defined and it does not make much sense to exactly specifiy it. But the atoms will have higher velocity and therefore higher chance to overcome the energy barrier.\n\n\nShow code\n\nplot( dihed600$V2,type=\"l\",col=c(\"red\"),ylim=c(-200,200),main=\"Ethane dihedral MD@300K and @600K/1ns\",ylab=\"Dihedral\",xlab=\"Structure/5steps\")\nlines( dihed300$V2,col=c(\"black\"))\nlegend(\"bottomleft\",legend=c(\"Dihedral@300K\",\"Dihedral@600K\"),text.col=c(\"black\",\"red\"))\n\n\n\n\nSo there is clearly more exploration at a higher temperature. Let’s compare the histograms. I randomly decide to make a bin every three degrees giving a total of 360/3 = 120 bins.\n\nLet’s display the histogram\n\n\nShow code\n\ndenshist600 =hist(dihed600$V2,breaks=120,main=\"Ethane dihedral MD@600K/1ns\",xlab=\"dihedral\",xlim = c(-180,180))\n\n\n\nShow code\n\n#plot(denshist600$mids, denshist600$density,type=\"l\",col=c(\"red\"),xlim=c(-180,180))\n#legend(\"topright\",legend=c(\"Dihedral@600K\"),text.col=c(\"red\"))\n\n\n\nNow let’s calculate the free energy \\(\\Delta G = -RT ln(\\rho)\\) in kcal/mol (using R = 1.985E-3). As we said in the first section, the density \\(\\rho(r)\\) is just the histogram of the dihedral shown above. We’ll calculate the logarithm of the density and multiply it by (-RT).\n\n\nShow code\n\nR = 1.985E-3\nTemp = 600.0\ndenshist600$log = -R*Temp*log(denshist600$density)\nplot(denshist600$mids,denshist600$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(5,10),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\nlegend(\"topright\",legend=c(\"Free Energy@600K 1ns \"),text.col=c(\"red\"))\n\n\n\nShow code\n\n#record barriers\nbarr_all_f5_1ns_1 = toString(op(calcBarrier(denshist600,-179.9,-110)))\nbarr_all_f5_1ns_2 = toString(op(calcBarrier(denshist600,-70,10)))\nbarr_all_f5_1ns_3 = toString(op(calcBarrier(denshist600,50,130)))\n\n\n\nQualitatively, the barrier is between 1 and 1.5 kcal/mol at 600K, remember that free energy barriers will depend on the temperature. Also, notice that the dihedral barriers should be the same for all the staggered-eclipsed transitions. It looks like that the dihedral at -60 degrees is more stable than at 60 degrees, which makes no physical sense. We’ll have to discard some data, read below for more info about this.\nLet’s calculate the barrier between the -60 and 0 degrees as precisely as we can: 3.02 which is different than the one between +60 and +120 which is 2.33. They should not be different as all hydrogens are identical. This discrepancy means that we explored all phase space but not equally or at least we are oversampling the minimum at -60 which makes it too stable. Let’s remember this result for later.\n3.3 Option 2: Longer dynamics\nIf we run a longer molecular dynamics, say five times longer, 5 ns (5E6 steps) at 300 K we can see that we cover all of the dihedral’s phase space. The details are the same, 300K, saving a structure every 5 steps.\n\n\nShow code\n\nsetwd(\"~/Teaching/Pchem/S18/Pchem_SharedFolder/10_Modeling/ethane\")\ndihed300_5ns = read.table(\"./dihed_300_5ns.txt\")\ndenshist300_5ns =hist(dihed300_5ns$V2,breaks=120,main=\"Ethane diehdral MD@300K/5ns\",xlab = \"dihedral\",xlim = c(-180,180),plot=TRUE)\n\n\n\nShow code\n\n#plot(denshist300_5ns$mids, denshist300_5ns$density,type=\"l\",col=c(\"red\"),xlim=c(-180,180))\n#legend(\"topright\",legend=c(\"Dihedral@300K - 5ns\"),text.col=c(\"red\"))\n\n\n\nCompared to the 300K@1ns the histograms above are now showing that at 5ns at 300K we are exploring all the dihedral range. However, the 180 degree seems to be more visited than the -60 and +60, which still does not make physical sense. Let’s calculate the free energy using the above histogram to see its impact on the value of free energy of rotation.\n\n\nShow code\n\ninit = 10020 # discard the first 50000 steps (50 ps)\nfinal = 1000020\nfreq = 1 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300.0\nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\nplot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\nlegend(\"topright\",legend=c(\"Free Energy@300K-5ns \"),text.col=c(\"red\"))\n\n\n\nShow code\n\n#record barriers\nbarr_1s_f5_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_1s_f5_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_1s_f5_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nIf you look at the R code above you’ll see that I discarded the first 50ps of simulations. This is a common procedure to eliminate the first equilibration steps. The barrier is now 2.58 kcal/mol between -60 and 0 degrees and 2.65 kcal/mol between +60 and +120 degrees.\n4 Studying the different options for the PMF\nFor the above calculation of rotation barrier we have made some random decisions. We have collected a dihedral measurement every 5 steps of the simulation, we have discarded the first 50 ps but used the rest. So we have some valid questions ahead:\nShould we measure the dihedral less frequently? How often?\nShould we discard the first part of the simulation? How much?\nWhat tells us when we have converged results?\nBelow is the code for: 5ns - Every 10 steps - Discard the first 50ps - Use the rest\n\n\nShow code\n\ninit = 10020 # discard the first 50000 steps (50 ps)\nfinal = 1000020\nfreq = 2 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300.0\nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\nplot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\nlegend(\"topright\",legend=c(\"300K - Every 10 steps - from 50ps to 5ns \"),text.col=c(\"red\"))\n\n\n\nShow code\n\n#record barriers\nbarr_1s_f10_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_1s_f10_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_1s_f10_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 50 steps - Discard the first 50ps - Use the rest. We will not plot the rest of free energy profiles, but you may do it by uncommenting the plot lines in the R code.\n\n\nShow code\n\ninit = 10020 # discard the first 50000 steps (50 ps)\nfinal = 1000020\nfreq = 10 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 50 steps - from 50ps to 5ns \"),text.col=c(\"red\"))\n\n#record barriers\nbarr_1s_f50_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_1s_f50_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_1s_f50_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 5 steps - Discard the first 3ns - Use the rest\n\n\nShow code\n\ninit = 600020 # discard the first 3ns (5x6E5)\nfinal = 1000020\nfreq = 1 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 50 steps - from 50ps to 5ns \"),text.col=c(\"red\"))\n\n#record barriers\nbarr_2s_f5_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_2s_f5_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_2s_f5_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 10 steps - Discard the first 3ns - Use the rest\n\n\nShow code\n\ninit = 600020 # discard the first 3ns (5x6E5)\nfinal = 1000020\nfreq = 2 # every 10 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 10 steps - from 3ns to 5ns \"),text.col=c(\"red\"))\n\n#record barriers\nbarr_2s_f10_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_2s_f10_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_2s_f10_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 50 steps - Discard the first 3ns - Use the rest\n\n\nShow code\n\ninit = 600020 # discard the first 3ns (5x6E5)\nfinal = 1000020\nfreq = 10 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 50 steps - 50-5ns \"),text.col=c(\"red\"))\n#record barriers\nbarr_2s_f50_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_2s_f50_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_2s_f50_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nThe table below compares the barrier of rotation for all these different options. At this point we will avoid using statistical tools to assess how converged or how different these results are. For a longer discussion and a more systematic approach on assessing the convergence of simulations, check the paper by Grossfield and Zuckerman entitled Quantifying uncertainty and sampling quality in biomolecular simulations\nSimulation  Simulation / Freq / sample\n-180°→ -120°\n-60°→0°\n+60°→+120°\n600K 1ns/5 steps/0 - 1ns\n2.68\n3.02\n2.33\n300K 5ns/5 steps/50ps- 5ns\n2.82\n2.58\n2.65\n300K 5ns/10 steps/50ps- 5ns\n2.85\n2.60\n2.66\n300K 5ns/50 steps/50ps- 5ns\n2.79\n2.63\n2.65\n300K 5ns/5 steps/3ns- 5ns\n2.66\n2.91\n2.86\n300K 5ns/10 steps/3ns- 5ns\n2.71\n2.93\n2.84\n300K 5ns/50 steps/3ns- 5ns\n2.70\n3.04\n2.85\nNotice that for a simple system such as ethane and for a relatively long simulation such as 5ns one can observe differences of the order of 0.5 kcal/mol. In the paper cited above by Grossfield and Zuckerman, the authors list in section 4 a series of recommendations. The take home message is that there is still a fair amount of heuristic analysis to settle on the calculation of thermodynamic properties using molecular simulation.\n\n\n\n",
    "preview": "posts/calculating_pmf/distill-preview.png",
    "last_modified": "2022-08-15T14:56:10-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/the_four_horsemen/",
    "title": "The four horsemen of curriculum innovation apocalypse",
    "description": "What everyone should take into account before investing in curriculum change",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2021-06-01",
    "categories": [
      "Curriculum"
    ],
    "contents": "\nFour Horsemen of the Apocalypse, an 1887 painting by Viktor Vasnetsov\nSo you want to change your curriculum, you may even want to be innovative and change everything, assessment, course sequence, type of delivery, calendar, degree requirements. Everything.\nWell, before you start dreaming, brainstorming with post-its, and drawing on a whiteboard with multiple colors, there are four gates, four checkpoints, four horsemen that will send your little innovative ideas to the apocalypse unless you keep them in mind. You may decide to fight each of them or you may just let them limit your possibilities, but you cannot ignore them.\nThey work as control checkpoints or valves. They slow down the process, sometimes for a good reason, because any significant change in the curriculum has a lasting impact to the institution and to the future of our students.\nSo here they are, Death, Famine, War, and Conquest\nHorseman 1. Death: Admission requirements for graduate and professional schools.\nWill your courses be accepted elsewhere? Will they transfer-in and transfer-out? The moment your courses do not have the name or the exact content or the prescribed number of credits, they may not be accepted elsewhere. It is not uncommon to not be transfer friendly among undergraduate degrees, specially for courses in the major. But it is a completely different set of problems if professional and graduate schools do not recognize our courses. This is specially problematic when many of our students pursue a post-graduate career. Not paying attention to this problem may undermine students career forever.\nAdvice\nMake a decision early and be aware of it. Unless your institution has a prestigious and recognized brand (Stanford courses will be accepted everywhere) there will be schools who will not recognize your courses.\nUsually not being transfer friendly lowers the number of applying students. Bring the “Admissions department” to the conversation, they may not be too happy. Give them information and materials to help them “sell” your innovative curriculum.\nConsult with student counselors and academic coaches. What do they think? Are we really undermining our students long-term careers?\nTalk to professional and graduate schools ahead of time. Possibly making minor changes to the curriculum (changing course names, or adding a pre-req) may make your courses more transfer friendly.\nHorseman 2. Famine: Calendar and scheduling challenges\nYou may have thought about “block scheduling”, or limiting sections to 10 people, or perhaps starting your semester earlier or perhaps having all students take the same chemistry lab during the same week. While there may be a pedagogically justified reason for your innovation it may be hindered by the following realities 1. Financial Aid and calendar stiffness. 2. Number of staff skilled, available and willing to teach it. 3. Number of special rooms or labs available at a given time.\nAdvice\nThese problems revolve around money. Just be aware of how much money you have for your innovations.\nHorseman 3. War: Faculty buy in. Whip your votes.\nThe same way that it is not bad that there are systems in place to slow down any curriculum change, it is not bad either that faculty may be skeptical or may require convincing. That being said, it is also important to recognize that not everyone will be on board. Be always careful with members who will keep vocalizing opposition, if not by votes, by volume and repetition. The tyranny of the loudest usually undermines the morale of the many.\nAdvice\nCount your votes before putting a motion on the floor. Designate a whip. Listen and be ready to compromise\nBe transparent: Perhaps create a site or a FAQ answering the questions to all the worse case scenarios. Always control your message and do not let unanswered questions linger.\nIt doesn’t matter how right you think you are, unless you are going to create a parallel department, do not think you can carry on without a significant buy-in of the faculty.\nHorseman 4. Conquest: Know your students. How many are you willing to leave behind?\nOften, curriculum innovations are motivated by pedagogy. You want your students to learn better, learn more, transfer, apply, do research, hands-on experience. In other words, you want your students to practice what is called “high-order cognitive skills” as early as possible. Problem solving is a high-order cognitive skill. Having taught first-years I learned that there’s a significant portion of our cohort who are not ready to engage in high-order activities, the cause may be many, as many as they may be described in the books about student success: mental health, socioeconomic pressures, studying skills, work ethics, academic preparation…\nAdvice\nUnless you “play safe” and become a fairly exclusive institution by not admitting students under certain academic performance there are two possibilities ahead:\nMaximizing the number of students on board which will minimize true problem-solving or any other high-order cognitive skill\nReal experience, hands-on, true problem-solving which will leave some students behind.\nThis last option is specially delicate with internships in the community and private sector. Will they be carefully planned? Any placement process? Otherwise, putting an unprepared student in a “real” environment, will not only be detrimental to the student education, but to the relationship with the community partner.\n\n\n\n",
    "preview": "posts/the_four_horsemen/Apocalypse_vasnetsov.jpg",
    "last_modified": "2022-08-15T14:56:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/history4kids/",
    "title": "World History for kids in times of plague",
    "description": "A drag and drop silly interface to teach kids fundamental events in human civilization",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2020-05-22",
    "categories": [
      "Javascript",
      "Kids"
    ],
    "contents": "\nDuring the first weeks of the pandemic I decided to teach some basic historical events in human (western) civilization.\nI know nothing about how to teach History, but it seems to me that while the analysis of historical events will always be more important than remembering facts, there has to be a minimum of memorization. Every functioning citizen should be able to place the important events in a timeline.\nSo I wrote a silly drag and drop interactive page using the jQuery library to give my kids a sense of accomplishment while they memorize this stuff.\nHere’s the link to the silly html page with Javascript\n\n\n\n\n\n",
    "preview": "posts/history4kids/histokids.png",
    "last_modified": "2022-08-15T14:56:24-05:00",
    "input_file": {},
    "preview_width": 2290,
    "preview_height": 1582
  },
  {
    "path": "posts/giving_students_feedback_with_simple_learning_analytics/",
    "title": "Giving students feedback with simple learning analytics",
    "description": "Using learning data to inform students of their course performance",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2020-03-01",
    "categories": [
      "Curriculum",
      "R"
    ],
    "contents": "\n\nContents\nPreamble\nWhat was sent to students\nPreparing for Milestones\nScore in the milestone practice\nNumber of attempts.\nHow early should you start practicing?\nDo you need to watch the videos?\nHow many minutes of video watching do you need?\n\nAs a class, are the students watching the videos?\n\n\n\nShow code\n\n#this script looks at how students prepared for the milestones\n# library(psych)\n# library(knitr)\n# library(chron)\n# setwd(\"~/Gd/Teaching/Chemistry_Curriculum/CHEM4/S20/Analysis\")\n# #name, id, submitted, attempt, score, n.correct, n.incorrect\n# m1_practice=read.csv(\"m1_practice.csv\", header = TRUE)\n# m1_practice = subset(m1_practice, select = \n#                        c(\"name\",\"id\",\"submitted\",\"attempt\", \"score\", \"n.correct\", \"n.incorrect\"))\n# #Formatting dates and calcualting difference in dates\n# m1date = as.POSIXct(\"2020-02-12 17:15:00 UTC\", format = \"%Y-%m-%d %H:%M:%S\")\n# m1_practice$submitted<- as.POSIXct(m1_practice$submitted, format = \"%Y-%m-%d %H:%M:%S\")\n# #the difference is a number in minutes\n# m1_practice$timeDiff = m1date - m1_practice$submitted \n# \n# m2_practice=read.csv(\"m2_practice.csv\", header = TRUE)\n# m2_practice = subset(m2_practice, select = \n#                        c(\"name\",\"id\",\"submitted\",\"attempt\", \"score\", \"n.correct\", \"n.incorrect\"))\n# #Formatting dates and calcualting difference in dates\n# m2date = as.POSIXct(\"2020-02-28 17:15:00 UTC\", format = \"%Y-%m-%d %H:%M:%S\")\n# m2_practice$submitted<- as.POSIXct(m2_practice$submitted, format = \"%Y-%m-%d %H:%M:%S\")\n# #the difference is a number in minutes\n# m2_practice$timeDiff = m2date - m2_practice$submitted \n# \n# \n# m1_quiz=read.csv(\"m1_quiz.csv\", header = TRUE)\n# m1_quiz = subset(m1_quiz, select = c(\"name\",\"id\",\"submitted\",\"attempt\", \"score\", \"n.correct\", \"n.incorrect\"))\n# m1_quiz$submitted<- as.POSIXct(m1_quiz$submitted, format = \"%Y-%m-%d %H:%M:%S\")\n# \n# m2_quiz=read.csv(\"m2_quiz.csv\", header = TRUE)\n# m2_quiz = subset(m2_quiz, select = c(\"name\",\"id\",\"submitted\",\"attempt\", \"score\", \"n.correct\", \"n.incorrect\"))\n# ## Video data\n# videos = read.csv(\"~/GenChem2/S20/Analysis/Videos/results.csv\", header = TRUE)\n# \n# #Participation\n# #Previous grades\n# chem123 = read.csv(\"~/Teaching/Grades_and_SRT/chem123_f18f19.csv\")\n# \n# \n# #lets take the highest score in your practice and compare it with your score in milestones\n# stud1 = unique(m1_quiz$name)\n# \n# compileM1 = data.frame(matrix(ncol = 11, nrow = 0))\n# colnames(compileM1) = c(\"name\",\"id\",\"highestScorePract\",\"numbAttempt\",\"quizScore\",\"timeDiff\",\"TotalWatched\",\"TotalMissed\",\"chem1\",\"chem2\",\"chem3\")\n# for (st in stud1){\n#   thisSt = m1_practice[which(m1_practice$name == st),]\n#   hiSc = max(thisSt$score,na.rm = TRUE)\n#   nAtt = max(thisSt$attempt, na.rm = TRUE)\n#   timeDiff = max(thisSt$timeDiff, na.rm = TRUE)\n#   quizScore = m1_quiz[which(m1_quiz$name == st),]$score\n#   id = m1_quiz[which(m1_quiz$name == st),]$id\n#   \n#   c1 = chem123[which(chem123$X == id),]$Final.1\n#   c2 = chem123[which(chem123$X == id),]$Final.2\n#   c3 = chem123[which(chem123$X == id),]$Final.3\n#   if(length(c1)==0) {c1=NA}\n#   if(length(c2)==0) {c2=NA}\n#   if(length(c3)==0) {c3=NA}\n#   \n#   TotalWatched = videos[which(videos$X == st),]$TotalWatched\n#   #convert into minutes watched\n#   TotalWatched = as.numeric(times(TotalWatched))*60*24\n#   TotalMissed  = videos[which(videos$X == st),]$TotalMissed\n#   \n#   compileM1[nrow(compileM1) + 1,] = c(st,id,hiSc,nAtt,quizScore,timeDiff,TotalWatched,TotalMissed,c1,c2,c3)\n# }\n# compileM1$highestScorePract = as.numeric(as.character(compileM1$highestScorePract))\n# compileM1$highestScorePract = compileM1$highestScorePract/12*100\n# compileM1$numbAttempt = as.numeric(as.character(compileM1$numbAttempt))\n# compileM1$quizScore = as.numeric(as.character(compileM1$quizScore))\n# \n# stud2 = unique(m2_quiz$name)\n# compileM2 = data.frame(matrix(ncol = 11, nrow = 0))\n# colnames(compileM2) = c(\"name\",\"id\",\"highestScorePract\",\"numbAttempt\",\"quizScore\",\"timeDiff\",\"TotalWatched\",\"TotalMissed\",\"chem1\",\"chem2\",\"chem3\")\n# \n# for (st in stud2){\n#   thisSt = m2_practice[which(m2_practice$name == st),]\n#   hiSc = max(thisSt$score,na.rm = TRUE)\n#   nAtt = max(thisSt$attempt, na.rm = TRUE)\n#   timeDiff = max(thisSt$timeDiff, na.rm = TRUE)\n#   quizScore = m2_quiz[which(m2_quiz$name == st),]$score\n#   id = m2_quiz[which(m2_quiz$name == st),]$id\n#   \n#   c1 = chem123[which(chem123$X == id),]$Final.1\n#   c2 = chem123[which(chem123$X == id),]$Final.2\n#   c3 = chem123[which(chem123$X == id),]$Final.3\n#   if(length(c1)==0) {c1=NA}\n#   if(length(c2)==0) {c2=NA}\n#   if(length(c3)==0) {c3=NA}\n#   \n#   TotalWatched = videos[which(videos$X == st),]$TotalWatched\n#   #convert into minutes watched\n#   TotalWatched = as.numeric(times(TotalWatched))*60*24\n#   TotalMissed  = videos[which(videos$X == st),]$TotalMissed\n#   \n#   compileM2[nrow(compileM2) + 1,] = c(st,id,hiSc,nAtt,quizScore,timeDiff,TotalWatched,TotalMissed,c1,c2,c3)\n# }\n# compileM2$highestScorePract = as.numeric(as.character(compileM2$highestScorePract))\n# compileM2$highestScorePract = compileM2$highestScorePract/12*100\n# compileM2$numbAttempt = as.numeric(as.character(compileM2$numbAttempt))\n# compileM2$quizScore = as.numeric(as.character(compileM2$quizScore))\n\nsetwd(\"~/Gd/Teaching/Chemistry_Curriculum/CHEM4/S20/Analysis\")\ncompileM1 = read.csv(\"compileM1.csv\",header = TRUE)\ncompileM2 = read.csv(\"compileM2.csv\",header = TRUE)\n\n\n\nPreamble\nIt is recognized among educational scholars that quick and clear feedback to students on their performance improves their self awareness and ultimately their performance.\nIt is also true that if the instructor does not provide context to a grade or performance the students will fill up that context, and that one is usually not accurate and detrimental for their learning.\nA clear and efficient way to provide context to students is by plotting data readily available on your course management system. In this post I share what I compile and show my students mid-semester when there is still time for them to change their course outcome. Data such as how much and how often they watch the course videos, how many times they attempt their practice and graded quizzes (here called milestones), and how their behavior compares with other students and previous years. Here below is what I shared with them.\nWhat was sent to students\nWe are more than one month into the semester and it is now a good time to look into what students can do to improve their grade, or at least, understand the main factors that affect student performance. The conclusions here are based on general trends and averages. Individually, however, each student may be in a specific situation and not all conclusions may apply to them.\nWe are going to look at Milestone performance because we already had gone through two milestones. Also, considering that students are given the questions ahead of time, a low performance in milestone means that there is something that is not working.\nThe milestone grades are fairly low as we can see in the following histograms\n\n\nShow code\n\nhist(compileM1$quizScore,breaks=14, main = \"Milestone 1. Grade\", xlab = \"Score\", ylab = \"Number of students\")\n\n\n\nShow code\n\nhist(compileM2$quizScore,breaks=14, main = \"Milestone 2. Grade\", xlab = \"Score\", ylab = \"Number of students\")\n\n\n\n\nPreparing for Milestones\nCanvas records how many times and what score you obtained each time you attempt the practice milestone quiz as well as how early you attempted.\nScore in the milestone practice\nHere below we are correlating the score in the practice quiz and the score in the actual milestone quiz. The first plot is Milestone 1\n\n\nShow code\n\nplot(compileM1$highestScorePract,compileM1$quizScore,xlim = c(0,100),ylim = c(0,100),main = \"Milestone 1. Practicing score\",xlab = \"Highest score in practice quiz\",ylab = \"Score in Milestone 1\")\nlines(x = c(0,100), y = c(0,100))\n\n\n\n\nAnd here below is the same with Milestone 2.\n\n\nShow code\n\nplot(compileM2$highestScorePract,compileM2$quizScore,xlim = c(0,100),ylim = c(0,100),main = \"Milestone 2. Practicing score\",xlab = \"Highest score in practice quiz\",ylab = \"Score in Milestone 2\")\nlines(x = c(0,100), y = c(0,100))\n\n\n\n\nConclusion The high correlation between score in practice and score in the actual quiz indicates that if you are struggling to get a good score in the milestone, you need to make sure that you score high enough while preparing for it.\nNumber of attempts.\n\n\nShow code\n\nplot(compileM1$numbAttempt,compileM1$quizScore,ylim = c(0,100),main = \"Milestone 1. #Attempts\",\n     xlab = \"Number of attempts in practice quiz\",ylab = \"Score in Milestone 1\")\n\n\n\n\n\n\nShow code\n\nplot(compileM2$numbAttempt,compileM2$quizScore,ylim = c(0,100),main = \"Milestone 2. #Attempts\",\n     xlab = \"Number of attempts in practice quiz\",ylab = \"Score in Milestone 2\")\nabline(a=20,b=6.15)\n\n\n\n\nConclusion Number of attempts DOES NOT correlate with performance. However, one must attempt several times to obtain a good score, attempting one or two times won’t work. The rest, it is up to you.\nHow early should you start practicing?\nIf we plot how early you started practicing, we can see that there is no realy any correlation in Milestone 1. The x-axis tells you the time you started your first practice quiz, in minutes, which is an indication of how early you started studying.\n\n\nShow code\n\nplot(compileM1$timeDiff,compileM1$quizScore,ylim = c(0,100),main = \"Milestone 1. Early practice\",\n     xlab = \"Minutes before quiz time you started practicing\",ylab = \"Score in Milestone 1\")\n\n\n\n\n\n\nShow code\n\nplot(compileM2$timeDiff,compileM2$quizScore,ylim = c(0,100),main = \"Milestone 2. Early practice\",\n     xlab = \"Minutes before quiz time you started practicing\",ylab = \"Score in Milestone 2\")\n\n\n\n\nConclusion There is no correlation. Starting early does not necessarily imply doing better in the milestone exams.\nDo you need to watch the videos?\nCanvas also records if students watch a video or not. As you can see, watching all the videos (having missed zero, x = 0) guarantees a minimum of 70% in milestone 1.\n\n\nShow code\n\nplot(compileM1$TotalMissed,compileM1$quizScore,ylim = c(0,100),main = \"Milestone 1. Missed videos\",\n     xlab = \"Number of videos NOT watched\",ylab = \"Score in Milestone 1\")\nlines(x = c(40,0), y = c(0,100))\n\n\n\n\n\n\nShow code\n\nplot(compileM2$TotalMissed,compileM2$quizScore,ylim = c(0,100),main = \"Milestone 2. Missed videos\",\n     xlab = \"Number of videos NOT watched\",ylab = \"Score in Milestone 2\")\nlines(x = c(40,0), y = c(0,100))\n\n\n\n\nConclusion While there are outliers, the general trend is that the fewer videos you miss the better your score is.\nHow many minutes of video watching do you need?\nSometimes students will start watch the video, but they end up only watching a fraction of it. Let us check if you should watch the whole thing or even rewatch the videos.\n\n\nShow code\n\nplot(compileM1$TotalWatched,compileM1$quizScore,ylim = c(0,100),main = \"Milestone 1. Minutes of videos watched\",\n     xlab = \"Total number of minutes of videos watched\",ylab = \"Score in Milestone 1\")\nabline(a=0,b=0.25)\n\n\n\n\n\n\nShow code\n\nplot(compileM2$TotalWatched,compileM2$quizScore,ylim = c(0,100),main = \"Milestone 2. Minutes of videos watched\",\n     xlab = \"Total number of minutes of videos watched\",ylab = \"Score in Milestone 2\")\nabline(a=0,b=0.25)\n\n\n\n\nConclusion If students watch more videos and more minutes of it, they will do better in exams.\nAs a class, are the students watching the videos?\nIf we plot the number of students who skipped videos, we can see there are days that about one third of the class did not watch the videos.\n“Number of student who did not watch videos” vs “video number”\n\n\n",
    "preview": "posts/giving_students_feedback_with_simple_learning_analytics/distill-preview.png",
    "last_modified": "2022-08-15T14:58:16-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/openended_5_items/",
    "title": "Open ended questions: Name 5 relevant items",
    "description": "Analysis of asking students to name 5 relevant statement pertinent to a topic",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2020-01-15",
    "categories": [
      "R",
      "Assessment"
    ],
    "contents": "\n\nContents\nIntroduction\nAnalysis\nOverall O.E. grade distribution\nCorrelating performance in O.E. and course grades\nLetter grades groups and performance in OE\nTwo different types of mistakes in OE\n\nConclusions\n\nIntroduction\nThe following data was collected during the fall semester of 2019 from a cohort of about 200 first-year students taking a first-year chemistry semester. We are analyzing here the results of implementing the open ended assessments as they had been described in “Establishing open-ended assessments: investigating the validity of creative exercises” SE Lewis, JL Shaw, KA Freeman - Chem. Educ. Res. Pract., 2011, 12, 158-166 (https://doi.org/10.1039/C1RP90020J)\nDuring these “open Ended questions” (OE) students are asked to name 5 relevant statements about a topic. Students had practiced similar questions at the beginning of each class session. During the three semester exams they were asked to name five relevant things about certain topics. For example:\nExam 1: Given a table of “Mass atom” and “Natural abundance” for two unknown elements they were asked “Below are shown the isotopic breakdown of 2 different elements. Make 5 relevant statements about these two samples”\n\nExam 2: Make 5 relevant and true statements about the organic compound shown below. CH3CH(CH3)CH2CHO\nExam 3: Given three skeletal structures of biomolecules “Make 5 relevant and true statements about the three molecules below”\n\nAnalysis\nLet’s load the data and clean it up a little bit. All the information is in the “all” dataframe\n\n\nShow code\n\nsetwd(\"~/Gd/Research/OpenEnded5relevantThings/\")\n \n#Load grades\ngrades <- read.csv(\"./chem1331grades_f19_new.csv\",header=TRUE)\n#load gradescope results for Exam1, 2, and 3\nopend1 <- read.csv(\"./Ex1_f19_1_5_things.csv\",header=TRUE)\nopend2 <- read.csv(\"./Ex2_f19_1_5_Things.csv\",header=TRUE)\nopend3 <- read.csv(\"./Ex3_f19_1_5_Things.csv\",header=TRUE)\n#Merge the whole thing into on DF\nall = merge(opend1,opend2, by=\"Email\",all=TRUE)\nall = merge(all,opend3, by=\"Email\",all=TRUE)\nall = merge(all,grades, by=\"Email\",all=TRUE)\nrownames(all) <- all$Email\nall$Email <- NULL\n#calcualate mean score in OE and create a new column\nall$meanOE = rowMeans(subset(all,select = c(\"Score.x\",\"Score.y\",\"Score\")),na.rm = TRUE)\n\n\n\nOverall O.E. grade distribution\nOn a total grade of 5 points, if we average the score of each student over exam 1, 2, and 3 we can see that most students obtain an average between 2.5 and 4.\n\n\nShow code\n\n#plot the average vs final grade\nhist(all$meanOE, main=\"Average grade of Op End in Exam 1,2,3\")\n\n\n\n\nCorrelating performance in O.E. and course grades\nIf we plot performance in O.E. vs final grades we obtain a significant positive correlation\n\n\nShow code\n\n#remove students who dont have a final grade\nallNAfinal = all[!is.na(all$MAX),]\nplot(allNAfinal$meanOE,allNAfinal$MAX, main = \"Final course grade vs OE avg score\")\nr2<-cor(allNAfinal$meanOE, allNAfinal$MAX)\nabline(lm(allNAfinal$MAX~ allNAfinal$meanOE))\nlegend(x='bottomright',legend=paste(\"Cor=\",round(r2,4)))\n\n\n\n\nWe can also find for other grade categories that correlate better with O.E. than final grades. For obvious reasons, the highest correlation is with the open ended written exams as the O.E. questions are part of it.\n\n\nShow code\n\nr2finalexam<-cor(allNAfinal$meanOE, allNAfinal$Final.Exam )\nr2written<-cor(allNAfinal$meanOE, allNAfinal$OpenEnded )\nr2finalLab<-cor(allNAfinal$meanOE, allNAfinal$LabFinal )\nr2HW<-cor(allNAfinal$meanOE, allNAfinal$HW )\nr2Report<-cor(allNAfinal$meanOE, allNAfinal$Report.. )\nr2Prelab <-cor(allNAfinal$meanOE, allNAfinal$Prelab.. )\nr2table <- data.frame(\"Final grade\" = c(r2), \"Final Exam\" = c(r2finalexam), \"Writen Exams\"= c(r2written), \"Lab final\" = c(r2finalLab), \"Homework\" = c(r2HW), \"Lab reports\" = c(r2Report), \"Prelab\" = c(r2Prelab)  )\nkable(r2table)\n\n\nFinal.grade\nFinal.Exam\nWriten.Exams\nLab.final\nHomework\nLab.reports\nPrelab\n0.618099\n0.5305522\n0.747087\n0.4978805\n0.2553626\n0.1902491\n0.0019672\n\nLetter grades groups and performance in OE\nWe can analyze the performance in OE for different letter grades. The number of students obtaining C and D are too low and the noise is too high.\n\n\nShow code\n\ndescribeBy( allNAfinal$meanOE , allNAfinal$LETTER )\n\n\n\n Descriptive statistics by group \ngroup: A\n   vars  n mean   sd median trimmed  mad  min max range skew kurtosis\nX1    1 17 4.18 0.49      4    4.18 0.49 3.33   5  1.67    0    -1.41\n     se\nX1 0.12\n---------------------------------------------------- \ngroup: A-\n   vars  n mean   sd median trimmed  mad  min  max range skew\nX1    1 26 3.76 0.49   3.67    3.76 0.49 2.83 4.67  1.83 0.05\n   kurtosis  se\nX1    -1.02 0.1\n---------------------------------------------------- \ngroup: B\n   vars  n mean   sd median trimmed  mad  min  max range skew\nX1    1 27 2.86 0.68   2.67    2.83 0.49 1.67 4.33  2.67 0.54\n   kurtosis   se\nX1    -0.28 0.13\n---------------------------------------------------- \ngroup: B-\n   vars  n mean  sd median trimmed  mad min max range  skew kurtosis\nX1    1 30 2.78 0.7   2.75    2.79 0.86 1.5   4   2.5 -0.19    -1.05\n     se\nX1 0.13\n---------------------------------------------------- \ngroup: B+\n   vars  n mean   sd median trimmed  mad min  max range  skew\nX1    1 38 3.36 0.71   3.33    3.39 0.74 1.5 4.67  3.17 -0.52\n   kurtosis   se\nX1    -0.21 0.11\n---------------------------------------------------- \ngroup: C\n   vars  n mean   sd median trimmed  mad min  max range  skew\nX1    1 11 2.44 0.69   2.67     2.5 0.49   1 3.33  2.33 -0.63\n   kurtosis   se\nX1    -0.77 0.21\n---------------------------------------------------- \ngroup: C-\n   vars n mean   sd median trimmed  mad  min  max range skew kurtosis\nX1    1 6 2.64 0.36    2.5    2.64 0.12 2.33 3.33     1  1.1    -0.49\n     se\nX1 0.15\n---------------------------------------------------- \ngroup: C+\n   vars  n mean   sd median trimmed  mad  min  max range  skew\nX1    1 17 2.22 0.67   2.33    2.24 0.49 0.67 3.33  2.67 -0.85\n   kurtosis   se\nX1     0.44 0.16\n---------------------------------------------------- \ngroup: D\n   vars n mean   sd median trimmed  mad min max range skew kurtosis\nX1    1 4 1.33 0.45   1.17    1.33 0.12   1   2     1 0.68    -1.73\n     se\nX1 0.23\n---------------------------------------------------- \ngroup: D-\n   vars n mean sd median trimmed mad  min  max range skew kurtosis se\nX1    1 1 1.17 NA   1.17    1.17   0 1.17 1.17     0   NA       NA NA\n---------------------------------------------------- \ngroup: F\n   vars n mean   sd median trimmed  mad  min max range skew kurtosis\nX1    1 2 2.33 0.94   2.33    2.33 0.99 1.67   3  1.33    0    -2.75\n     se\nX1 0.67\n\nShow code\n\nboxplot(allNAfinal$meanOE ~ allNAfinal$LETTER, las=2,ylab=\"Average OE\")\n\n\n\n\nAnd we can also run an ANOVA among the different letter grades and we see significant difference between many letter groups. Check the column “p adj” for the p value\n\n\nShow code\n\nTukeyHSD( aov(allNAfinal$meanOE ~ allNAfinal$LETTER))\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = allNAfinal$meanOE ~ allNAfinal$LETTER)\n\n$`allNAfinal$LETTER`\n             diff         lwr          upr     p adj\nA--A  -0.42006033 -1.07217915  0.232058483 0.5773984\nB-A   -1.31227306 -1.95959992 -0.664946190 0.0000000\nB--A  -1.39869281 -2.03339132 -0.763994301 0.0000000\nB+-A  -0.81682147 -1.42687609 -0.206766842 0.0010753\nC-A   -1.73707665 -2.54610148 -0.928051821 0.0000000\nC--A  -1.53758170 -2.53039473 -0.544768667 0.0000589\nC+-A  -1.96078431 -2.67790818 -1.243660450 0.0000000\nD-A   -2.84313725 -4.00501071 -1.681263803 0.0000000\nD--A  -3.00980392 -5.16117551 -0.858432332 0.0004851\nF-A   -1.84313725 -3.40607248 -0.280202030 0.0076123\nB-A-  -0.89221273 -1.46669022 -0.317735236 0.0000552\nB--A- -0.97863248 -1.53884182 -0.418423133 0.0000028\nB+-A- -0.39676113 -0.92888796  0.135365691 0.3520873\nC-A-  -1.31701632 -2.06902262 -0.565010010 0.0000026\nC--A- -1.11752137 -2.06444799 -0.170594744 0.0075360\nC+-A- -1.54072398 -2.19284280 -0.888605167 0.0000000\nD-A-  -2.42307692 -3.54599376 -1.300160082 0.0000000\nD--A- -2.58974359 -4.72032849 -0.459158686 0.0049882\nF-A-  -1.42307692 -2.95727340  0.111119554 0.0957831\nB--B  -0.08641975 -0.64104362  0.468204116 0.9999892\nB+-B   0.49545159 -0.03079178  1.021694960 0.0849945\nC-B   -0.42480359 -1.17265826  0.323051080 0.7459180\nC--B  -0.22530864 -1.16894160  0.718324314 0.9994581\nC+-B  -0.64851126 -1.29583812 -0.001184389 0.0491424\nD-B   -1.53086420 -2.65100497 -0.410723423 0.0007474\nD--B  -1.69753086 -3.82665396  0.431592231 0.2562108\nF-B   -0.53086420 -2.06302997  1.001301575 0.9882838\nB+-B-  0.58187135  0.07124212  1.092500571 0.0119190\nC-B-  -0.33838384 -1.07533481  0.398567133 0.9185141\nC--B- -0.13888889 -1.07390401  0.796126234 0.9999931\nC+-B- -0.56209150 -1.19679001  0.072607006 0.1351248\nD-B-  -1.44444444 -2.55733504 -0.331553848 0.0018185\nD--B- -1.61111111 -3.73642880  0.514206578 0.3274411\nF-B-  -0.44444444 -1.97131775  1.082428858 0.9970598\nC-B+  -0.92025518 -1.63609118 -0.204419182 0.0021241\nC--B+ -0.72076023 -1.63922511  0.197704641 0.2778656\nC+-B+ -1.14396285 -1.75401747 -0.533908224 0.0000003\nD-B+  -2.02631579 -3.12533805 -0.927293530 0.0000006\nD--B+ -2.19298246 -4.31107115 -0.074893759 0.0355273\nF-B+  -1.02631579 -2.54311061  0.490479031 0.5023129\nC--C   0.19949495 -0.86160460  1.260594499 0.9999374\nC+-C  -0.22370766 -1.03273249  0.585317163 0.9980828\nD-C   -1.10606061 -2.32679991  0.114678701 0.1146152\nD--C  -1.27272727 -3.45645213  0.910997585 0.7151075\nF-C   -0.10606061 -1.71323858  1.501117373 1.0000000\nC+-C- -0.42320261 -1.41601565  0.569610418 0.9490516\nD-C-  -1.30555556 -2.65513364  0.044022527 0.0675802\nD--C- -1.47222222 -3.73049829  0.786053846 0.5593833\nF-C-  -0.30555556 -2.01265180  1.401540693 0.9999603\nD-C+  -0.88235294 -2.04422639  0.279520511 0.3247925\nD--C+ -1.04901961 -3.20039120  1.102351982 0.8839316\nF-C+   0.11764706 -1.44528817  1.680582284 1.0000000\nD--D  -0.16666667 -2.50420447  2.170871141 1.0000000\nF-D    1.00000000 -0.81064900  2.810649000 0.7771284\nF-D-   1.16666667 -1.39397771  3.727311039 0.9222908\n\nTwo different types of mistakes in OE\nThere are two main reasons why students would not get full marks for each statement\nStatement is not relevant or it is redundant\nStatement is not true\nWe can calculate the average of times that student made those two types of mistakes\n\n\nShow code\n\n#students with high instances of answers being not relevant\nnotRelevant <- all[,grepl(\"not.relevant\", colnames(all))]\na <- rowSums(notRelevant == \"TRUE\",na.rm = TRUE)\nb <- rowSums(notRelevant == \"FALSE\", na.rm = TRUE)\nall$notRelevant <- a/(a+b)\n#students with high instances of answers being not true\nnotTrue <- all[,grepl(\"not.true\", colnames(all))]\na <- rowSums(notTrue == \"TRUE\",na.rm = TRUE)\nb <- rowSums(notTrue == \"FALSE\", na.rm = TRUE)\nall$notTrue <- a/(a+b)\nallNAfinal = all[!is.na(all$MAX),]\n\n\n\nAnd we can see how making those two types of mistakes correlates with the other course performance. The table below is with “Not True” mistakes. Notice that the correlation in both types of mistakes is, as expected, negative, which means that the higher the number of mistakes the lower the performance in the course.\n\n\nShow code\n\nr2<-cor(allNAfinal$notTrue, allNAfinal$MAX )\nr2finalexam<-cor(allNAfinal$notTrue, allNAfinal$Final.Exam )\nr2written<-cor(allNAfinal$notTrue, allNAfinal$OpenEnded )\nr2finalLab<-cor(allNAfinal$notTrue, allNAfinal$LabFinal )\nr2HW<-cor(allNAfinal$notTrue, allNAfinal$HW )\nr2Report<-cor(allNAfinal$notTrue, allNAfinal$Report.. )\nr2Prelab <-cor(allNAfinal$notTrue, allNAfinal$Prelab.. )\nr2table <- data.frame(\"Final grade\" = c(r2), \"Final Exam\" = c(r2finalexam), \"Writen Exams\"= c(r2written), \"Lab final\" = c(r2finalLab), \"Homework\" = c(r2HW), \"Lab reports\" = c(r2Report), \"Prelab\" = c(r2Prelab)  )\nkable(r2table, caption=\"Correlation with Not True mistakes\")\n\n\nTable 1: Correlation with Not True mistakes\nFinal.grade\nFinal.Exam\nWriten.Exams\nLab.final\nHomework\nLab.reports\nPrelab\n-0.5063556\n-0.4116678\n-0.637651\n-0.4290181\n-0.2054091\n-0.1721591\n-0.0044984\n\nShow code\n\nplot(allNAfinal$notTrue,allNAfinal$MAX, main = \"Final course grade vs Not true mistakes\")\nabline(lm(allNAfinal$MAX~ allNAfinal$notTrue))\nlegend(x='bottomright',legend=paste(\"Cor=\",round(r2,4)))\n\n\n\n\nAnd this table is with “Not relevant” mistakes\n\n\nShow code\n\nr2<-cor(allNAfinal$notRelevant, allNAfinal$MAX )\nr2finalexam<-cor(allNAfinal$notRelevant , allNAfinal$Final.Exam )\nr2written<-cor(allNAfinal$notRelevant, allNAfinal$OpenEnded )\nr2finalLab<-cor(allNAfinal$notRelevant, allNAfinal$LabFinal )\nr2HW<-cor(allNAfinal$notRelevant, allNAfinal$HW )\nr2Report<-cor(allNAfinal$notRelevant, allNAfinal$Report.. )\nr2Prelab <-cor(allNAfinal$notRelevant, allNAfinal$Prelab.. )\nr2table <- data.frame(\"Final grade\" = c(r2), \"Final Exam\" = c(r2finalexam), \"Writen Exams\"= c(r2written), \"Lab final\" = c(r2finalLab), \"Homework\" = c(r2HW), \"Lab reports\" = c(r2Report), \"Prelab\" = c(r2Prelab)  )\nkable(r2table, caption=\"Correlation with Not True mistakes\")\n\n\nTable 2: Correlation with Not True mistakes\nFinal.grade\nFinal.Exam\nWriten.Exams\nLab.final\nHomework\nLab.reports\nPrelab\n-0.2034072\n-0.2341019\n-0.1863573\n-0.1541022\n-0.0451623\n-0.007477\n0.0414387\n\nShow code\n\nplot(allNAfinal$notRelevant,allNAfinal$MAX, main = \"Final course grade vs Not Relevant mistakes\")\nabline(lm(allNAfinal$MAX~ allNAfinal$notRelevant))\nlegend(x='bottomright',legend=paste(\"Cor=\",round(r2,4)))\n\n\n\n\nConclusions\nIt does make sense that performance in Open Ended assessments correlates positively with all the grade categories in the course. As for the types of mistakes, making “not true” mistakes in these type of questions correlates better with performance in the course.\n\n\n\n",
    "preview": "posts/openended_5_items/openended_5_items_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2022-08-15T14:56:24-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/triangles_of_curriculum_design/",
    "title": "Chemistry curriculum: triangles everywhere",
    "description": "Many aspects of Chemistry learning, from delivery to learning objectives come in triads of linked concepts",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2019-08-01",
    "categories": [
      "Curriculum"
    ],
    "contents": "\n\nContents\nThree ideas for curriculum design\nThree ideas for learning objectives\nThree concepts to improve summative assessments\n\nThree representations in chemistry\nFour languages of chemistry (not three)\n\nI will use the triangle shape to represent the idea that three linked and intertwined concepts cannot be separated from one another. In this case, in my daily life as a teacher, there are many instances where my job requires considering a triad of linked concepts. I’m listing here those different triads, from the most general aspects of curriculum design to the most specific ones of chemistry learning.\nThree ideas for curriculum design\nYou want to design a new course, reshape an existing one. There are so many things to be taken care. One can spend a lot of time choosing a textbook that may be barely used, designing with careful detail an activity in class or a lecture that students will barely be paying attention to. Should you use a homework online system provided by the publisher? How can you make sure students learn what they are supposed to? How can you tell them? To me it’s the class activities. I make sure that the class activities align with the preclass content and with the assessment. They do not only define your pedagogy during class but also the content. You may have made a video explaining something, or a great lecture or demonstration, but if it is not assessed in form of activity, if students do not see “what’s in it for them”, it will fall flat.\nThree ideas for learning objectives\nAlong the same lines to what I said above, the key word is alignment. The same way that is\nThree concepts to improve summative assessments\nThree representations in chemistry\nThis is an old idea I believe first expressed by Johnstone, A.H. J. Comp. Assist. Learn. 1991, 7, 701-703\nThe very symbol H2O, depending on the context, may represent the molecule or the macroscopic state of matter or both. Chemists will often switch from one representation to another and the instructor must always be aware of how hard it is for some students to make this switch.\nA good exercise to practice this switch is asking students to associate a list of properties to a macroscopic, submicroscopic or symbolic. For example: density, bond length, two hydrogen per oxygen atom, boiling point, temperature, vibrational frequency, ionization energy…\nFour languages of chemistry (not three)\nToo often undergraduate chemistry pays a lot of focus to the least intuitive (for most students) language of chemistry, the calculations.\nKnowing chemistry implies that you can express the same idea in its four languages, but that doesn’t mean that you should expect students to use the four of them at the same time. My experience tells me that from the most intuitive to the least (for most students) is drawing, representing, explaining, and calculating.\nStarting with an intuitive language such as drawing and representing helps students make the necessary connections and lower their cognitive load once you move to a more abstract and less intuitive concepts and languages.\nHere’s two examples of how atoms may be understood using the four different languages. The first model represents an essentially coulombic or classical atom and the second one comes from the quantum solution of the hydrogen atom.\n\n\n\n",
    "preview": "posts/triangles_of_curriculum_design/triangle1.png",
    "last_modified": "2022-08-15T14:56:16-05:00",
    "input_file": {},
    "preview_width": 1077,
    "preview_height": 754
  },
  {
    "path": "posts/web_lotteries_bingos_for_student_games/",
    "title": "Web lotteries and Bingos for Student Engaging Games",
    "description": "Simple web interfaces in Javascript for light-weight games in the chemistry class",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2019-08-01",
    "categories": [
      "Curriculum",
      "Javascript"
    ],
    "contents": "\nIn my chemistry class there’s always a day that I need to engage students in a different way. Perhaps is the day\nbefore the exam and don’t want to introduce any new or serious content, perhaps is that weird day right before\nThanksgiving when you are not canceling class but half of students are not attending.\nIn any case, through the years I have been using different forms of games that can carry a tangential pedagogical value\nand that can engage students in a time when their focus is probably at their best.\nFor example, for a first semester chemistry (CHEM1331 - so a little different) we run a bingo game.\nThe game takes place after students have learned their functional\ngroups, skeletal structure, NMR peaks and IR signals.\nTo generate the bingo cards you need\na list of compound names or properties: http://chem.r.umn.edu/chem1331/bingo/list_compounds\na python script that generates the cards by building a table with randomized names: http://chem.r.umn.edu/chem1331/bingo/parse.py\n\nA site that randomly chooses compounds. A developed different ones with different types of animations\nBingo 1 site: Uses jQuery to spin through a wheel of compounds http://chem.r.umn.edu/chem1331/bingo/\nBingo 2 site: The next one does not spin, it just fills out a table so that no compound is ever repeated (which was a problem with the first site) http://chem.r.umn.edu/chem1331/bingo/bingo.html\n\nAs you can see, the site requires having generated the 2d structures. Nowadays you may be better off using RDKit to convert names to 2d images (https://www.rdkit.org/docs/GettingStartedInPython.html). I think I generated that group of images by querying the NIC server. For example, the URL https://cactus.nci.nih.gov/chemical/structure/methylamine/image returns an image of methylamine. So, with the terminal you can build a script that downloads all those files such as:\ncat list|while read mol\ndo\n    curl -o $mol.gif https://cactus.nci.nih.gov/chemical/structure/$mol/image\ndone\nThat’s all. The class will take a solid 30 minutes to even have completed a line (let alone a full bingo card).\n\n\n\n\n\n\n",
    "preview": {},
    "last_modified": "2024-07-02T14:50:14-05:00",
    "input_file": {}
  },
  {
    "path": "posts/analysis_seven_years_genchem/",
    "title": "Analysis of seven years of General Chemistry student data",
    "description": "Trying to make sense of student performance and identifying possible predictors of academic success.",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2018-08-10",
    "categories": [
      "Curriculum",
      "R"
    ],
    "contents": "\n\nContents\nAbstract\nOverview Final Course grades\nComparing means by semester\nGraphically by semester\nStatiscal analysis by semester\nOther grades besides final grade\nLetter grades\nSemester exams and previous exams\n\n\nPredictors of performance in Chemistry\nMath ACT is a good predictor\nWas math ACT different through the years?\nCorrelation models: ACT vs GenChem1\n\nPrevious GPA is a better predictor\nWas Incoming GPA different through the years?\nCorrelation models: Prev. GPA vs GenChem grades\n\nIs Highschool performance relevant?\nWas Highschool performance different through the years?\nCorrelation models: HS rank vs GenChem grades\n\n\nDemographics\nGender\nComparing the two genders each year\nPerformance by each gender through the years\n\nEthnicity\nStatistical analysis of performance in GenChem1 by ethnicity\nDifferent ethnicities\n\nFirst generation students\nStatistical analysis of performance 1st generation in GenChem1\n\n\n\nAbstract\nBetween Fall 2010 and Spring 2018 I taught the two semester sequence of General Chemistry (GC). The way our curriculum was structured, these two semesters were usually taken during the sophomore year for students majoring in a Bachelor of Sciences in Health Sciences.\nDuring all this time, while the chemistry content has not changed significantly, the forms of delivery and assessment have been evolving towards, hopefully, better pedagogies of engagment and towards a clearer assessment of learning objectives. Probably the most remarkable change was flipping the class with videos in the fall of 2014.\nOverview Final Course grades\nLet’s just look at how students have performed in the two GC semesters by looking at their final grade in different semesters.\nIMPORTANT: We will see statistical significance between years and other demographics when analyzing the final percent grade. However, when we analyze the letter grade, those significances disappear. This is important because when a student is disengaged their score may be 60% or 5%, and while the means and medians may be affected, the letter grade analysis will not. Also, during the semester of Fall 2011 - Spring 2012 the laboratory was still a different course, this means that the criteria for a passing grade was not 70%, but lower.\nComparing means by semester\n\n\nShow code\n\nsetwd(\"~/Gd/Research/StudentData/Discover\")\n\n#Load demographics for all years\nallGC1 <- read.csv(\"./genchem1_nosummer_11_16.csv\",header=TRUE)\nallGC2 <- read.csv(\"./genchem2_11_17.csv\",header=TRUE)\nallGC1_ <- read.csv(\"./genchem1_nosummer_11_16_mergedsex.csv\",header = TRUE)\nallGC2_ <- read.csv(\"./genchem2_11_17_mergedsex.csv\",header = TRUE)\n\nlibrary(psych)\nmata<-describeBy(allGC1$TG_Total.Grade....,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1\")\n\n\nTable 1: GenChem1\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n68\n78.43\n10.11\n79.50\n78.98\n9.79\n42.50\n95.90\n53.40\nX12\nFall 2012\n84\n73.95\n12.41\n73.00\n73.78\n14.53\n43.00\n96.20\n53.20\nX13\nFall 2013\n69\n83.81\n8.17\n84.70\n84.49\n6.82\n41.60\n96.50\n54.90\nX14\nFall 2014\n105\n80.95\n9.73\n81.52\n81.78\n8.61\n40.52\n98.78\n58.27\nX15\nFall 2015\n60\n81.44\n9.74\n82.68\n82.39\n7.19\n43.47\n96.33\n52.87\nX16\nFall 2016\n35\n84.17\n7.24\n84.95\n84.60\n7.82\n66.98\n97.14\n30.16\n\nShow code\n\nmata<-describeBy(allGC2$TG_Total.Grade....,allGC2$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2\")\n\n\nTable 1: GenChem2\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nSpring 2011\n16\n87.38\n8.34\n89.84\n88.07\n6.55\n67.70\n97.38\n29.68\nX12\nSpring 2012\n45\n69.01\n12.95\n70.70\n69.90\n10.53\n35.60\n94.00\n58.40\nX13\nSpring 2013\n51\n81.51\n9.42\n82.00\n82.00\n9.93\n53.70\n97.50\n43.80\nX14\nSpring 2014\n55\n80.31\n8.51\n79.30\n80.55\n8.75\n60.10\n96.70\n36.60\nX15\nSpring 2015\n61\n78.51\n10.00\n77.78\n78.55\n11.92\n57.26\n96.14\n38.88\nX16\nSpring 2016\n44\n77.15\n9.66\n78.73\n77.77\n9.49\n54.39\n94.10\n39.72\nX17\nSpring 2017\n37\n80.01\n9.80\n80.78\n80.64\n8.01\n48.11\n97.22\n49.11\n\nGraphically by semester\n\n\nShow code\n\nlibrary(ggplot2)\nggplot(allGC1, aes(x=TG_Total.Grade...., fill=Semester))+geom_histogram()+ggtitle(\"GenChem1 by semester\")\n\n\n\nShow code\n\nggplot(allGC2, aes(x=TG_Total.Grade...., fill=Semester))+geom_histogram()+ggtitle(\"GenChem2\")\n\n\n\n\nStatiscal analysis by semester\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$TG_Total.Grade.... ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. GenChem1 Grade among semesters\")\n\n\nTable 2: Anova. GenChem1 Grade among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-4.4779412\n-9.1423644\n0.186482\n0.0681540\nFall 2013-Fall 2011\n5.3836530\n0.4976747\n10.269631\n0.0211987\nFall 2014-Fall 2011\n2.5216131\n-1.9292496\n6.972476\n0.5843701\nFall 2015-Fall 2011\n3.0089288\n-2.0556712\n8.073529\n0.5318824\nFall 2016-Fall 2011\n5.7435885\n-0.2048146\n11.691992\n0.0653478\nFall 2013-Fall 2012\n9.8615942\n5.2158876\n14.507301\n0.0000000\nFall 2014-Fall 2012\n6.9995543\n2.8138662\n11.185242\n0.0000345\nFall 2015-Fall 2012\n7.4868700\n2.6536537\n12.320086\n0.0001708\nFall 2016-Fall 2012\n10.2215296\n4.4688516\n15.974208\n0.0000082\nFall 2014-Fall 2013\n-2.8620399\n-7.2932841\n1.569204\n0.4353187\nFall 2015-Fall 2013\n-2.3747242\n-7.4220918\n2.672643\n0.7584349\nFall 2016-Fall 2013\n0.3599354\n-5.5738024\n6.293673\n0.9999781\nFall 2015-Fall 2014\n0.4873157\n-4.1401366\n5.114768\n0.9996654\nFall 2016-Fall 2014\n3.2219753\n-2.3589421\n8.802893\n0.5638475\nFall 2016-Fall 2015\n2.7346596\n-3.3470041\n8.816323\n0.7918982\n\nShow code\n\na<- TukeyHSD( aov(allGC2$TG_Total.Grade.... ~ allGC2$Semester)) \nb<-as.data.frame(a$`allGC2$Semester`)\nknitr::kable(b, caption = \"Anova. GenChem2 Grade among semesters\")\n\n\nTable 2: Anova. GenChem2 Grade among semesters\n\ndiff\nlwr\nupr\np adj\nSpring 2012-Spring 2011\n-18.3719243\n-27.016530\n-9.7273185\n0.0000000\nSpring 2013-Spring 2011\n-5.8749308\n-14.385113\n2.6352511\n0.3860347\nSpring 2014-Spring 2011\n-7.0680859\n-15.504043\n1.3678712\n0.1676882\nSpring 2015-Spring 2011\n-8.8701902\n-17.212128\n-0.5282519\n0.0288746\nSpring 2016-Spring 2011\n-10.2332149\n-18.903549\n-1.5628812\n0.0094423\nSpring 2017-Spring 2011\n-7.3733537\n-16.259708\n1.5130002\n0.1767662\nSpring 2013-Spring 2012\n12.4969935\n6.422770\n18.5712173\n0.0000001\nSpring 2014-Spring 2012\n11.3038384\n5.334050\n17.2736265\n0.0000009\nSpring 2015-Spring 2012\n9.5017341\n3.665559\n15.3379087\n0.0000439\nSpring 2016-Spring 2012\n8.1387093\n1.842068\n14.4353503\n0.0028667\nSpring 2017-Spring 2012\n10.9985706\n4.407646\n17.5894949\n0.0000250\nSpring 2014-Spring 2013\n-1.1931551\n-6.966573\n4.5802632\n0.9963637\nSpring 2015-Spring 2013\n-2.9952594\n-8.630410\n2.6398912\n0.6966880\nSpring 2016-Spring 2013\n-4.3582841\n-10.469068\n1.7524994\n0.3452669\nSpring 2017-Spring 2013\n-1.4984229\n-7.912023\n4.9151776\n0.9928975\nSpring 2015-Spring 2014\n-1.8021043\n-7.324522\n3.7203134\n0.9603189\nSpring 2016-Spring 2014\n-3.1651291\n-9.172113\n2.8418544\n0.7053683\nSpring 2017-Spring 2014\n-0.3052678\n-6.620048\n6.0095122\n0.9999993\nSpring 2016-Spring 2015\n-1.3630247\n-7.237241\n4.5111913\n0.9931556\nSpring 2017-Spring 2015\n1.4968365\n-4.691783\n7.6854560\n0.9914449\nSpring 2017-Spring 2016\n2.8598613\n-3.764772\n9.4844944\n0.8600771\n\n\n\nShow code\n\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\nggboxplot(allGC1, x = \"Semester\", y = \"TG_Total.Grade....\",  title = \"Final grade in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(allGC2, x = \"Semester\", y = \"TG_Total.Grade....\",  title = \"Final grade in GC2\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC2$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nOther grades besides final grade\n.\nLetter grades\nI converted the letter grades into the 4-scale. The plot should only show 4, 3.66, 3.33, 3… but it seems to add more variability…\n\n\nShow code\n\n#need to load this other file, as it contains the letter grades\nallGC1_bosco <- read.csv(\"~/Research/StudentData/XavierData/Clean/allGC1.csv\",header = TRUE)\n\na<- allGC1_bosco$Final.letter\na <- gsub(\"A\\\\-\", 3.667,a)\na <- gsub(\"A\", 4.000,a)\na <- gsub(\"B\\\\+\", 3.333,a)\na <- gsub(\"B\\\\-\", 2.667,a)\na <- gsub(\"B\", 3.000,a)\na <- gsub(\"C\\\\+\", 2.333,a)\na <- gsub(\"C\\\\-\", 1.667,a)\na <- gsub(\"C\", 2.000,a)\na <- gsub(\"D\\\\+\", 1.333,a)\na <- gsub(\"D\", 1.000,a)\na <- gsub(\"F\", 0.000,a)\na <- gsub(\"I\", 0.000,a)\nallGC1_bosco$Final.letter.number <- as.numeric(as.character(a))\nggboxplot(allGC1_bosco, x = \"Semester\", y = \"Final.letter.number\",  title = \"Final letter grade in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1_bosco$Final.letter.number), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\n#ggplot(data=allGC1_bosco,aes(x=Semester,y=Final.letter)) + geom_bar(stat=\"identity\") + geom_bar(aes(fill = Final.letter))\n\n\n\nSemester exams and previous exams\n\n\nShow code\n\nsetwd(\"~/Gd/Research/StudentData/Discover\")\n#lets write the prepost file into the discover folder\nprePost <- read.csv(\"/Users/xavier/Gd/Research/StudentData/ExamPrePost.csv\",header=TRUE,sep = \"\\t\")\nsource(\"~/Gd/Research/R/deid.R\")\nprePost <- deIdThis(prePost)\nwrite.csv(prePost,file=\"prePost.csv\")\nprePost <- read.csv(\"./prePost.csv\", header = TRUE)\nprePost$inc1<-prePost$Grade1-prePost$Mid1\nprePost$inc2<-prePost$Grade2-prePost$Mid2\nprePost$inc3<-prePost$Grade3-prePost$Mid3\nprePost$meanInc <- rowMeans( prePost[c('inc1','inc2','inc3')])\nprePost$meanExam <- rowMeans( prePost[c('Grade1','Grade2','Grade3')])\n\n\n\nThe final exam is a second opportunity for students to improve their semester exams. Let’s measure how exams score and improvement evolved through the years.\n\n\nShow code\n\nggboxplot(prePost, x = \"Semester\", y = \"meanExam\",  title = \"Average grade in final exams\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(prePost$meanExam), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 105 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nThis is plots the increment\n\n\nShow code\n\nggboxplot(prePost, x = \"Semester\", y = \"meanInc\",  title = \"Average increment from semester exams to final\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(prePost$meanInc), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 40 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nThere’s something funky about some of these numbers. Fall 2014 doesn’t seem to apply the >40% rule, which I actually implemented.\nSo let’s check that I obtain the same result if I plot grade exams from BoSCO data\n\n\nShow code\n\nallGC1_bosco$meanExam <-  rowMeans( allGC1_bosco[c('Exam1','Exam2','Exam3')], na.rm=TRUE)\n\nggboxplot(allGC1_bosco, x = \"Semester\", y = \"meanExam\",  title = \"Average grade in final exams (Bosco source)\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1_bosco$meanExam), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 105 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nPredictors of performance in Chemistry\nThere are different variables that we want to look at. Performance factors such as ACT scores or GPA or High School rank , as well as demographic factors such as ethnicity and first-year generation.\nMath ACT is a good predictor\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_ACT.MATH,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"ACT Math - Fall sophomore\")\n\n\nTable 3: ACT Math - Fall sophomore\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n64\n24.75\n3.50\n24.5\n24.67\n3.71\n18\n33\n15\nX12\nFall 2012\n70\n24.41\n4.01\n24.0\n24.29\n4.45\n17\n34\n17\nX13\nFall 2013\n66\n25.14\n2.82\n25.0\n25.26\n2.97\n18\n31\n13\nX14\nFall 2014\n101\n25.47\n3.21\n26.0\n25.40\n2.97\n17\n34\n17\nX15\nFall 2015\n57\n24.72\n3.19\n24.0\n24.70\n2.97\n18\n32\n14\nX16\nFall 2016\n32\n24.94\n2.64\n25.0\n24.96\n2.97\n19\n30\n11\n\nShow code\n\nmata<-describeBy(allGC2$DEM_ACT.MATH,allGC2$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"ACT Math - Spring sophomore\")\n\n\nTable 3: ACT Math - Spring sophomore\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nSpring 2011\n16\n25.88\n4.18\n25.5\n25.79\n3.71\n19\n34\n15\nX12\nSpring 2012\n42\n25.57\n3.47\n25.0\n25.56\n2.97\n18\n33\n15\nX13\nSpring 2013\n44\n26.23\n4.15\n26.0\n26.17\n4.45\n18\n34\n16\nX14\nSpring 2014\n52\n24.90\n2.76\n25.0\n25.17\n2.97\n18\n29\n11\nX15\nSpring 2015\n58\n26.10\n3.36\n26.0\n26.04\n2.97\n19\n34\n15\nX16\nSpring 2016\n42\n25.10\n3.27\n25.0\n25.00\n2.97\n18\n32\n14\nX17\nSpring 2017\n33\n25.27\n2.47\n26.0\n25.30\n2.97\n21\n30\n9\n\nWe see that the second semester is a subselection of the first semester with a higher ACT math score. Therefore, we can just use GenChem1 for the analysis.\nWas math ACT different through the years?\nAs we can see below. There is no significant difference in ACT throughout the years\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$DEM_ACT.MATH ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. ACTMath among semesters\")\n\n\nTable 4: Anova. ACTMath among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-0.3357143\n-1.9769059\n1.3054774\n0.9919345\nFall 2013-Fall 2011\n0.3863636\n-1.2784117\n2.0511390\n0.9856326\nFall 2014-Fall 2011\n0.7153465\n-0.8007861\n2.2314792\n0.7559117\nFall 2015-Fall 2011\n-0.0307018\n-1.7589701\n1.6975666\n1.0000000\nFall 2016-Fall 2011\n0.1875000\n-1.8670492\n2.2420492\n0.9998340\nFall 2013-Fall 2012\n0.7220779\n-0.9060719\n2.3502278\n0.8010990\nFall 2014-Fall 2012\n1.0510608\n-0.4247621\n2.5268838\n0.3217601\nFall 2015-Fall 2012\n0.3050125\n-1.3880045\n1.9980295\n0.9955408\nFall 2016-Fall 2012\n0.5232143\n-1.5017715\n2.5482001\n0.9767972\nFall 2014-Fall 2013\n0.3289829\n-1.1730225\n1.8309883\n0.9889559\nFall 2015-Fall 2013\n-0.4170654\n-2.1329539\n1.2988231\n0.9823133\nFall 2016-Fall 2013\n-0.1988636\n-2.2430100\n1.8452827\n0.9997726\nFall 2015-Fall 2014\n-0.7460483\n-2.3181344\n0.8260379\n0.7513444\nFall 2016-Fall 2014\n-0.5278465\n-2.4528701\n1.3971770\n0.9699093\nFall 2016-Fall 2015\n0.2182018\n-1.8779779\n2.3143814\n0.9996831\n\nShow code\n\nggboxplot(allGC1, x = \"Semester\", y = \"DEM_ACT.MATH\",  title = \"ACT Math in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$DEM_ACT.MATH, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 40) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nCorrelation models: ACT vs GenChem1\n\n\nShow code\n\n#par(mfrow = c(1, 2))\nplot(allGC1$TG_Total.Grade....,allGC1$DEM_ACT.MATH,main=\"GenChem1\")\na<- lm(allGC1$DEM_ACT.MATH~allGC1$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2a<-summary(a)$r.squared\n\nplot(allGC2$TG_Total.Grade....,allGC2$DEM_ACT.MATH,main=\"GenChem2\")\na<-lm(allGC2$DEM_ACT.MATH~allGC2$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2b<-summary(a)$r.squared\n\n\n\nWe obtain a r-squared for both 0.2042773 and 0.1569021, respectively. We need to find a better predictor. Let’s see cumulative GPA before enrolling\nPrevious GPA is a better predictor\nWhile ACT.Math historically seems to correlate well, since we’re teaching sophomores, previous GPA is even a better predictor\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1\")\n\n\nTable 5: GenChem1\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n65\n3.11\n0.46\n3.14\n3.13\n0.52\n1.88\n3.93\n2.05\nX12\nFall 2012\n79\n2.94\n0.53\n2.94\n2.95\n0.59\n1.44\n3.98\n2.54\nX13\nFall 2013\n69\n3.18\n0.44\n3.21\n3.19\n0.53\n1.84\n3.98\n2.14\nX14\nFall 2014\n105\n3.00\n0.48\n2.98\n3.00\n0.47\n1.33\n4.00\n2.67\nX15\nFall 2015\n60\n3.00\n0.39\n3.00\n2.98\n0.33\n2.18\n3.97\n1.79\nX16\nFall 2016\n35\n3.12\n0.48\n3.26\n3.14\n0.42\n2.06\n4.00\n1.94\n\nShow code\n\nmata<-describeBy(allGC2$DEM_Cumulative.GPA,allGC2$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2\")\n\n\nTable 5: GenChem2\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nSpring 2011\n16\n3.46\n0.43\n3.54\n3.47\n0.46\n2.73\n4.00\n1.27\nX12\nSpring 2012\n43\n3.18\n0.39\n3.19\n3.19\n0.40\n2.33\n3.95\n1.62\nX13\nSpring 2013\n50\n3.20\n0.48\n3.24\n3.24\n0.39\n2.05\n3.97\n1.92\nX14\nSpring 2014\n53\n3.25\n0.44\n3.27\n3.26\n0.50\n2.18\n3.98\n1.80\nX15\nSpring 2015\n61\n3.18\n0.46\n3.19\n3.18\n0.49\n2.19\n4.00\n1.81\nX16\nSpring 2016\n44\n3.07\n0.41\n3.05\n3.06\n0.36\n2.13\n3.96\n1.83\nX17\nSpring 2017\n36\n3.23\n0.40\n3.26\n3.25\n0.34\n2.13\n4.00\n1.87\n\nWas Incoming GPA different through the years?\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$DEM_Cumulative.GPA ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. Entering GPA among semesters\")\n\n\nTable 6: Anova. Entering GPA among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-0.1679124\n-0.3930252\n0.0572004\n0.2709961\nFall 2013-Fall 2011\n0.0728361\n-0.1595233\n0.3051956\n0.9469705\nFall 2014-Fall 2011\n-0.1069817\n-0.3191411\n0.1051777\n0.7000993\nFall 2015-Fall 2011\n-0.1160769\n-0.3567413\n0.1245875\n0.7384580\nFall 2016-Fall 2011\n0.0137802\n-0.2680571\n0.2956175\n0.9999925\nFall 2013-Fall 2012\n0.2407485\n0.0192443\n0.4622527\n0.0242003\nFall 2014-Fall 2012\n0.0609307\n-0.1392812\n0.2611426\n0.9531219\nFall 2015-Fall 2012\n0.0518354\n-0.1783657\n0.2820366\n0.9874925\nFall 2016-Fall 2012\n0.1816926\n-0.0912643\n0.4546495\n0.3999188\nFall 2014-Fall 2013\n-0.1798178\n-0.3881444\n0.0285087\n0.1350707\nFall 2015-Fall 2013\n-0.1889130\n-0.4262055\n0.0483794\n0.2048113\nFall 2016-Fall 2013\n-0.0590559\n-0.3380193\n0.2199075\n0.9905679\nFall 2015-Fall 2014\n-0.0090952\n-0.2266461\n0.2084557\n0.9999966\nFall 2016-Fall 2014\n0.1207619\n-0.1416144\n0.3831382\n0.7750540\nFall 2016-Fall 2015\n0.1298571\n-0.1560608\n0.4157750\n0.7847500\n\nShow code\n\nggboxplot(allGC1, x = \"Semester\", y = \"DEM_Cumulative.GPA\",  title = \"Entering GPA in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$DEM_Cumulative.GPA, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nCorrelation models: Prev. GPA vs GenChem grades\nWhen we plot previous GPA (typically first year GPA) against final grade\n\n\nShow code\n\n#par(mfrow = c(1, 2))\nplot(allGC1$TG_Total.Grade....,allGC1$DEM_Cumulative.GPA,main=\"GenChem1\")\na<-lm(allGC1$DEM_Cumulative.GPA~allGC1$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2a<-summary(a)$r.squared\nplot(allGC2$TG_Total.Grade....,allGC2$DEM_Cumulative.GPA,main=\"GenChem2\")\na<-lm(allGC2$DEM_Cumulative.GPA~allGC2$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2b<-summary(a)$r.squared\n\n\n\nIn this case we obtain better r-squared for both 0.656591 and 0.5840838, respectively\nIs Highschool performance relevant?\nFor large schools, highschool(HS) ranking can be used as a better measurement than HS GPA. Also, HS-GPA is currently unavailable :). The units are given in percentile, so the higher the better\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_HS.Rank,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1\")\n\n\nTable 7: GenChem1\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n60\n79.18\n14.08\n80.5\n80.48\n15.57\n46\n97\n51\nX12\nFall 2012\n65\n73.57\n17.01\n76.0\n74.98\n16.31\n20\n99\n79\nX13\nFall 2013\n57\n81.21\n12.33\n81.0\n82.04\n13.34\n47\n99\n52\nX14\nFall 2014\n86\n79.43\n16.62\n84.0\n81.56\n13.34\n26\n99\n73\nX15\nFall 2015\n51\n81.27\n13.83\n86.0\n82.93\n8.90\n37\n99\n62\nX16\nFall 2016\n25\n82.28\n11.75\n85.0\n82.95\n10.38\n60\n98\n38\n\nWas Highschool performance different through the years?\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$DEM_HS.Rank ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. HS ranking among semesters\")\n\n\nTable 8: Anova. HS ranking among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-5.6141026\n-13.2615244\n2.033319\n0.2876124\nFall 2013-Fall 2011\n2.0271930\n-5.8736280\n9.928014\n0.9774141\nFall 2014-Fall 2011\n0.2468992\n-6.9383845\n7.432183\n0.9999987\nFall 2015-Fall 2011\n2.0911765\n-6.0444897\n10.226843\n0.9772356\nFall 2016-Fall 2011\n3.0966667\n-7.0718166\n13.265150\n0.9527517\nFall 2013-Fall 2012\n7.6412955\n-0.1100688\n15.392660\n0.0558988\nFall 2014-Fall 2012\n5.8610018\n-1.1596093\n12.881613\n0.1616772\nFall 2015-Fall 2012\n7.7052790\n-0.2853243\n15.695882\n0.0659401\nFall 2016-Fall 2012\n8.7107692\n-1.3420278\n18.763566\n0.1318975\nFall 2014-Fall 2013\n-1.7802938\n-9.0761070\n5.515519\n0.9819290\nFall 2015-Fall 2013\n0.0639835\n-8.1694637\n8.297431\n1.0000000\nFall 2016-Fall 2013\n1.0694737\n-9.1774107\n11.316358\n0.9996775\nFall 2015-Fall 2014\n1.8442772\n-5.7052249\n9.393779\n0.9818377\nFall 2016-Fall 2014\n2.8497674\n-6.8561055\n12.555640\n0.9594958\nFall 2016-Fall 2015\n1.0054902\n-9.4235429\n11.434523\n0.9997815\n\nShow code\n\nggboxplot(allGC1, x = \"Semester\", y = \"DEM_HS.Rank\",  title = \"Highschool Rank in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$DEM_HS.Rank, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nFall2012 seems to stand out again.\nCorrelation models: HS rank vs GenChem grades\n\n\nShow code\n\n#par(mfrow = c(1, 2))\nplot(allGC1$TG_Total.Grade....,allGC1$DEM_HS.Rank,main=\"GenChem1\")\na<-lm(allGC1$DEM_HS.Rank~allGC1$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2a<-summary(a)$r.squared\nplot(allGC2$TG_Total.Grade....,allGC2$DEM_HS.Rank,main=\"GenChem2\")\na<-lm(allGC2$DEM_HS.Rank~allGC2$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2b<-summary(a)$r.squared\n\n\n\nFairly poor r-squared for both 0.1667476 and 0.0552476, respectively\nDemographics\nGiven the good correlation given above between previous GPA and final grade, let’s then analyze how students of different demographics perform in chemistry when compared to their incoming GPA. In other words, instead of comparing how first-generation vs non-first-generation do, it is more interesting to see how considering their college readiness (as desribed by GPA) how they did in GenChem\n\n\nGender\nLook at how previous GPA and GenChem grades is among selfidentified genders. There was no data besides male and female.\n\n\nShow code\n\n#there are some underfined that mess up the graphs\nonlyMF_gc1<- allGC1_[complete.cases(allGC1_$Sex),]\nonlyMF_gc2<- allGC2_[complete.cases(allGC2_$Sex),]\nmata<-describeBy(onlyMF_gc1$DEM_Cumulative.GPA,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA and Sex\")\n\n\nTable 9: 1st year GPA and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n286\n3.08\n0.45\n3.06\n3.08\n0.48\n1.88\n4\n2.12\nX12\nM\n117\n3.00\n0.53\n3.04\n3.01\n0.49\n1.33\n4\n2.67\n\nShow code\n\nmata<-describeBy(onlyMF_gc1$DEM_ACT.MATH,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"ACT math and Sex\")\n\n\nTable 9: ACT math and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n270\n24.61\n3.09\n25\n24.61\n2.97\n17\n34\n17\nX12\nM\n111\n25.82\n3.71\n26\n25.83\n2.97\n17\n34\n17\n\nShow code\n\nmata<-describeBy(onlyMF_gc1$DEM_HS.Rank,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"HS rank and Sex\")\n\n\nTable 9: HS rank and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n239\n81.02\n14.21\n84.0\n82.64\n11.86\n25\n99\n74\nX12\nM\n96\n74.40\n16.42\n76.5\n75.46\n17.05\n20\n99\n79\n\nFrom the above, we can see that females come to GenChem with very slightly higher GPA and remarkably better HS ranking, but with a lower ACT-math score. Also, males have a broader range of values and higher standard deviation, this tell us that male performance may not be treated as a single group, and it may require a further finer classification. In any case, How will these three factors affect their performance in GenChem? The number of students may not be exactly the same because not all students have ACT or HS data.\n\n\nShow code\n\nmata<-describeBy(onlyMF_gc1$TG_Total.Grade....,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1 grade and Sex\")\n\n\nTable 10: GenChem1 grade and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n289\n79.74\n9.84\n81.0\n80.49\n9.49\n42.50\n98.78\n56.28\nX12\nM\n120\n80.88\n12.12\n82.1\n82.36\n11.15\n40.52\n97.14\n56.62\n\nShow code\n\nmata<-describeBy(onlyMF_gc2$TG_Total.Grade....,onlyMF_gc2$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2 grade and Sex\")\n\n\nTable 10: GenChem2 grade and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n196\n78.18\n10.51\n79.28\n78.88\n9.62\n38.20\n97.50\n59.30\nX12\nM\n102\n80.31\n10.19\n80.57\n80.76\n13.00\n56.39\n97.22\n40.83\n\nComparing the two genders each year\nWhile it may look like males do better than females, even though females came with better GPA and HS ranking, there is actually no significant difference when compared the two groups in general.\n\n\nShow code\n\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\np <- ggboxplot(onlyMF_gc1, x = \"Sex\", y = \"TG_Total.Grade....\", color = \"Sex\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\n\nHowever, when the two groups are compared each semester we notice that Fall 2011 is the only semester with a significant difference between genders.\n\n\nShow code\n\np <- ggboxplot(onlyMF_gc1, x = \"Semester.x\", y = \"TG_Total.Grade....\", color = \"Sex\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=Sex),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nPerformance by each gender through the years\nBefore we jump into conclusions, however, we may need to look into how the females in Fall 2011 performed compared to other semester’s females.\n\n\nShow code\n\n#selecting females\nonlyF_gc1 <- onlyMF_gc1[onlyMF_gc1$Sex==\"F\",]\nonlyM_gc1 <- onlyMF_gc1[onlyMF_gc1$Sex==\"M\",]\n\nggboxplot(onlyF_gc1, x = \"Semester.x\", y = \"TG_Total.Grade....\",  title = \"Females in GC1\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyF_gc1$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyM_gc1, x = \"Semester.x\", y = \"TG_Total.Grade....\",  title = \"Males in GC1\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyM_gc1$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyF_gc1, x = \"Semester.x\", y = \"DEM_Cumulative.GPA\",  title = \"Incoming GPA for females\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyF_gc1$DEM_Cumulative.GPA, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyM_gc1, x = \"Semester.x\", y = \"DEM_Cumulative.GPA\",  title = \"Incoming GPA for males\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyM_gc1$DEM_Cumulative.GPA, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyF_gc1, x = \"Semester.x\", y = \"DEM_HS.Rank\",  title = \"HS Ranking for females\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyF_gc1$DEM_HS.Rank, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyM_gc1, x = \"Semester.x\", y = \"DEM_HS.Rank\",  title = \"HS Ranking for males\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyM_gc1$DEM_HS.Rank, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nWe saw that females had performed significantly lower in Fall2011, and almost significantly higher in Fall2013 than males. However, we see that these differences may also be explained by the differences with the incoming GPAs, but not by HS ranking. Also, many students lack HS Ranking so the statistics may be lacking.\nEthnicity\nLet’s compare the GPA before enrolling in GenChem for students selfidentified ethnicity.\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$DEM_Student.of.Color,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA and Student of Color: Y/N\")\n\n\nTable 11: 1st year GPA and Student of Color: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nN\n340\n3.09\n0.47\n3.11\n3.10\n0.47\n1.33\n4.00\n2.67\nX12\nY\n73\n2.84\n0.46\n2.78\n2.82\n0.43\n1.90\n3.97\n2.07\n\nShow code\n\nmata<-describeBy(allGC1$TG_Total.Grade....,allGC1$DEM_Student.of.Color,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1 grade and Student of Color: Y/N\")\n\n\nTable 11: GenChem1 grade and Student of Color: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nN\n348\n81.06\n10.20\n82.34\n82.13\n8.65\n40.52\n98.78\n58.27\nX12\nY\n73\n74.67\n10.48\n75.30\n74.58\n12.04\n43.47\n96.02\n52.56\n\nShow code\n\nmata<-describeBy(allGC2$TG_Total.Grade....,allGC2$DEM_Student.of.Color,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2 grade and Student of Color: Y/N\")\n\n\nTable 11: GenChem2 grade and Student of Color: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nN\n260\n79.13\n10.33\n79.74\n79.75\n10.53\n43.7\n97.5\n53.8\nX12\nY\n49\n74.44\n12.76\n72.95\n75.31\n9.87\n35.6\n96.7\n61.1\n\n\n\nShow code\n\nrequire(gridExtra)\nplotA <-ggplot(allGC1, aes(x=TG_Total.Grade...., fill=DEM_Student.of.Color)) + geom_histogram() + ggtitle(\"GenChem1 by Ethnicity\")\nplotB <-ggplot(allGC1, aes(x=DEM_Cumulative.GPA, fill=DEM_Student.of.Color)) + geom_histogram() + ggtitle(\"Prev GPA by Ethnicity\")\ngrid.arrange(plotA,plotB)\n\n\n\n\nStatistical analysis of performance in GenChem1 by ethnicity\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"DEM_Student.of.Color\", y = \"TG_Total.Grade....\", color = \"DEM_Student.of.Color\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"Semester\", y = \"TG_Total.Grade....\", color = \"DEM_Student.of.Color\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_Student.of.Color),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nDifferent ethnicities\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$DEM_Ethnicity,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA for different ethnicities\")\n\n\nTable 12: 1st year GPA for different ethnicities\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n0\nNaN\nNA\nNA\nNaN\nNA\nInf\n-Inf\n-Inf\nX12\nAm. Indian\n5\n2.74\n0.36\n2.62\n2.74\n0.43\n2.33\n3.23\n0.90\nX13\nAsian\n38\n2.88\n0.47\n2.81\n2.86\n0.36\n1.90\n3.97\n2.07\nX14\nBlack\n29\n2.96\n0.50\n2.89\n2.95\n0.56\n2.12\n3.95\n1.83\nX15\nHawaiian\n1\n3.11\nNA\n3.11\n3.11\n0.00\n3.11\n3.11\n0.00\nX16\nHispanic\n13\n2.78\n0.38\n2.70\n2.76\n0.36\n2.27\n3.47\n1.20\nX17\nNS\n3\n2.72\n0.66\n2.37\n2.72\n0.07\n2.32\n3.48\n1.16\nX18\nWhite\n324\n3.10\n0.47\n3.11\n3.11\n0.47\n1.33\n4.00\n2.67\n\nWe can also run an anova among different ethnicities, but in any case it’s hard to do statistics on such small numbers maybe only black and asian are large enough to be compared with whites.\n\n\nShow code\n\nTukeyHSD( aov(allGC1$TG_Total.Grade.... ~ allGC1$DEM_Ethnicity))\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = allGC1$TG_Total.Grade.... ~ allGC1$DEM_Ethnicity)\n\n$`allGC1$DEM_Ethnicity`\n                           diff         lwr       upr     p adj\nAm. Indian-          -1.2520311 -24.3127791 21.808717 0.9999998\nAsian-                1.8944333 -17.0079964 20.796863 0.9999879\nBlack-                0.2271630 -18.9237458 19.378072 1.0000000\nHawaiian-             6.1165333 -30.3457108 42.578777 0.9996050\nHispanic-            -0.4325654 -20.6581794 19.793049 1.0000000\nNS-                  -4.9680000 -30.7507001 20.814700 0.9990190\nWhite-                5.7148568 -12.5997033 24.029417 0.9807262\nAsian-Am. Indian      3.1464644 -11.8319308 18.124860 0.9982867\nBlack-Am. Indian      1.4791941 -13.8115804 16.769969 0.9999905\nHawaiian-Am. Indian   7.3685644 -27.2225576 41.959686 0.9981266\nHispanic-Am. Indian   0.8194657 -15.7975718 17.436503 0.9999999\nNS-Am. Indian        -3.7159689 -26.7767169 19.344779 0.9996975\nWhite-Am. Indian      6.9668879  -7.2624335 21.196209 0.8117453\nBlack-Asian          -1.6672703  -9.3686684  6.034128 0.9979230\nHawaiian-Asian        4.2221000 -27.7474084 36.191609 0.9999204\nHispanic-Asian       -2.3269987 -12.4081536  7.754156 0.9968822\nNS-Asian             -6.8624333 -25.7648630 12.039996 0.9553407\nWhite-Asian           3.8204235  -1.4689372  9.109784 0.3535646\nHawaiian-Black        5.8893703 -26.2276802 38.006421 0.9992900\nHispanic-Black       -0.6597284 -11.1994223  9.879965 0.9999995\nNS-Black             -5.1951630 -24.3460719 13.955746 0.9915567\nWhite-Black           5.4876938  -0.6305412 11.605929 0.1158453\nHispanic-Hawaiian    -6.5490987 -39.3183386 26.220141 0.9987569\nNS-Hawaiian         -11.0845333 -47.5467775 25.377711 0.9834226\nWhite-Hawaiian       -0.4016765 -32.0271526 31.223800 1.0000000\nNS-Hispanic          -4.5354346 -24.7610486 15.690179 0.9974027\nWhite-Hispanic        6.1474222  -2.7829165 15.077761 0.4184543\nWhite-NS             10.6828568  -7.6317033 28.997417 0.6360504\n\nFirst generation students\nLet’s compare the GPA before enrolling in GenChem for 1st generation vs the rest. Notice for how many people we have information (a total of 421 students in Genchem1 and 309 in GenChem2)\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$DEM_First.Generation,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA and 1st generation: Y/N\")\n\n\nTable 13: 1st year GPA and 1st generation: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n20\n2.85\n0.58\n2.70\n2.79\n0.45\n1.88\n4.00\n2.12\nX12\nN\n248\n3.05\n0.48\n3.09\n3.07\n0.50\n1.33\n3.98\n2.65\nX13\nY\n145\n3.07\n0.44\n3.02\n3.06\n0.43\n1.90\n4.00\n2.10\n\nShow code\n\nmata<-describeBy(allGC1$TG_Total.Grade....,allGC1$DEM_First.Generation,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1 grade and 1st generation: Y/N\")\n\n\nTable 13: GenChem1 grade and 1st generation: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n24\n72.63\n14.01\n73.62\n72.63\n15.20\n42.50\n97.14\n54.64\nX12\nN\n252\n79.81\n10.51\n81.07\n80.73\n10.29\n40.52\n97.07\n56.55\nX13\nY\n145\n81.41\n9.36\n82.18\n82.20\n8.03\n53.00\n98.78\n45.78\n\nShow code\n\nmata<-describeBy(allGC2$TG_Total.Grade....,allGC2$DEM_First.Generation,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2 grade and 1st generation: Y/N\")\n\n\nTable 13: GenChem2 grade and 1st generation: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n16\n78.97\n11.79\n79.30\n79.09\n14.52\n60.1\n96.14\n36.04\nX12\nN\n187\n78.26\n10.96\n79.10\n78.94\n11.12\n38.2\n97.50\n59.30\nX13\nY\n106\n78.52\n10.64\n79.42\n79.24\n10.49\n35.6\n96.70\n61.10\n\n\n\nShow code\n\nggplot(allGC1, aes(x=TG_Total.Grade...., fill=DEM_First.Generation )) + geom_histogram() + ggtitle(\"GenChem1 by First Generation\")\n\n\n\nShow code\n\nggplot(allGC2, aes(x=TG_Total.Grade...., fill=DEM_First.Generation))+geom_histogram()+ggtitle(\"GenChem2 by First Generation\")\n\n\n\n\nStatistical analysis of performance 1st generation in GenChem1\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"DEM_First.Generation\", y = \"TG_Total.Grade....\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"Semester\", y = \"TG_Total.Grade....\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_First.Generation),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC2, x = \"Semester\", y = \"TG_Total.Grade....\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")  + rotate_x_text(angle = 45)\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_First.Generation),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nFirst generation students seem to do slightly better or the same than the rest. Are they coming in with equal preparation? We can look at HS rank to try to answer that.\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"DEM_First.Generation\", y = \"DEM_HS.Rank\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"Semester\", y = \"DEM_HS.Rank\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_HS.Rank),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nIt seems that the first generation students are already better prepared than the non-first generation.\n\n\n\n",
    "preview": "posts/analysis_seven_years_genchem/distill-preview.png",
    "last_modified": "2022-08-15T14:56:24-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  }
]
