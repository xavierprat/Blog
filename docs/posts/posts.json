[
  {
    "path": "posts/calculating_pmf/",
    "title": "The Simplest Potential of Mean Force using Molecular Dynamics ",
    "description": "Using the rotation of ethane to understand some tools from statistical mechanics.",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2021-07-15",
    "categories": [
      "Modeling",
      "R"
    ],
    "contents": "\n\nContents\n1 What is this?\n2 Some theory: reaction paths, free energy, and molecular dynamics\n3 Studying the rotation of ethane: Free energy\n3.1 Analysis of a 1ns trajectory\n3.2 Option 1: Effect of increasing the temperature\n3.3 Option 2: Longer dynamics\n\n4 Studying the different options for the PMF\n\n\n\nShow code\n\ncalcBarrier <- function(dens,init,endit){\n  #this function calculates the barrier in a dens$log array\n  #the dens array comes from the hist function, the log is calculated separately\n  #init is the first angle and endit is the last angle\n  firstindex = which.min( abs(dens$mids - init ) )\n  lastindex = which.min( abs(dens$mids - endit ) )\n  DE= max(dens$log[firstindex:lastindex]) - min(dens$log[firstindex:lastindex])\n  return(DE)\n}\nop = function(x, d=2) sprintf(paste0(\"%1.\",d,\"f\"), x)\n\n\n\n1 What is this?\nThe goal of this post is exclusively pedagogical: learning to calculate PMFs in the simplest system for chemists. Some context here, you have a background in physical chemistry and computational chemistry but never really quite understood the meaning of free energy of reaction, aka Potential of Mean Force (PMF) and how to use molecular simulations to calculate it.\nI want to give a very quick overview of tools from statistical thermodynamics to study reactions. There are lots of textbooks covering this material and there’s online tutorials with complex biological systems. But I wonder if I can cover the simplest of the systems (conformational rotation of ethane) with the simplest of the approaches (unbiased molecular dynamics).\nFor higher barriers and higher number of atoms you will have to use more sophisticated approaches to calculate the free energy paths (check this review). And that may be the main roadblock for students who want to understand it. You are told to use tools such as metadynamics or string-based replica exchange dynamics without really knowing why. You may feel like you are not grasping the fundamentals… or at least that’s how I felt back when I was a grad student.\n2 Some theory: reaction paths, free energy, and molecular dynamics\nIn computational chemistry we are interested in identifying the molecular structures that participate in a chemical reaction, what we call a reaction mechanism or reaction path that connects reactants and products. Invoking the second law of thermodynamics (principle of minimum energy) we identify the lowest energy structures as the most stable and therefore the most likely path. Sometimes a good estimation is to calculate the potential energy of the system. Or at most you may just need to approximate your surface with a parabola (build a Hessian, diagonalize it, get the frequencies and calculate the harmonic vibrational partition function). You probably have run a gaussian freq calculation before?\nBut that won’t be the case if your system is wiggly and floppy (read here that your reactants is a collection of conformers or a rugged surface, not a single deep minimum) or your reaction requires a large rearrangement (high entropy change between reactants and transition state). That’s when you need to sample the phase space. The phase space is what we call all the positions and velocities available at a given temperature.\nI’m going to throw academic rigor out of the window and I will simplify the equations saying that the free energy along a coordinate “r” can be obtained by calculating the density (\\(\\rho(r)\\) : think of a histogram of probability) along that coordinate “r”\n\\[ G(r) = -RT ln(\\rho(r)) \\] In other words values of “r” that are more probable will show higher density and therefore lower free energy. In other words, this density is just a histogram of the values of your reaction coordinate during a molecular dynamics simulation.\nI remember being confused about this result. How could it be that just a histogram of a coordinate could contain the precious free energy information. To answer that question we need to unpack this density function.\nThe density \\(\\rho(r)\\) has in it all the thermal and stability information about the path:\n\\[\n\\rho(r) = \\frac{\\int{ \\delta(q-r) e^{(-V(q)/RT)} dq} }{\\int{ e^{(-V(q)/RT)} dq}}\n\\] “V(q)” is the potential energy of the system, “q” is all the coordinates of our system and the symbol \\(\\delta(q-r)\\) just Dirac’s delta function, which is just a way to say that we’re just counting states of a given value “r”.\nLet me translate that into English. Because we are going to be running a molecular dynamics simulation at a given temperature, the states that the simulation visits are already following the Boltzmann factor \\(e^{(-V(q)/RT)}\\). This means that high energy states are less visited than low energy states, which is exactly what the integral is doing.\nNotice that the integral is overall all positions. We hope we’ll visit all positions after some longish simulation. This is, simplified, the ergodic hypothesis, we will converge the integral (summation of states) that goes over all cases if we run the simulation over a long enough time. It is not exactly true that all of the states that we visit in a molecular dynamics run they all belong to the statistical ensemble. Two consecutive steps are too correlated and they probably overrepresenting that area of the phase space. This is why we will usually take measurements every “x” steps of a dynamics run. The longer “x” is the less correlated two measurements will be, but the longer the dynamics will need to be run. The objective of this strategy is to collect a true Markov chain.\nFor a useful discussion on ergodic hypothesis using molecular dynamics check this article.\n\n3 Studying the rotation of ethane: Free energy\nDisclaimer: The very concept of Temperature and some of the assumptions from Statistical Mechanics are not valid for a single molecule simulation. That being said, it is still conceptually useful to study a single molecule of ethane for how fast calculations are and how conceptually simple the system is.\nWe are a running a simple molecular dynamics simulations with the NAMD code. You may download the input and outputs used in this post here We will use VMD to load the trajectory file (dcd) and save the dihedral data. Assuming we’ve done all that, let the analysis begin!\n3.1 Analysis of a 1ns trajectory\n\n\nShow code\n\nsetwd(\"~/Teaching/Pchem/S18/Pchem_SharedFolder/10_Modeling/ethane\")\ndihed600 = read.table(\"./dihedral.txt\")\ndihed300 = read.table(\"./dihedral_300.txt\")\n\n\n\nLet’s plot how the dihedral evolves during a 1 ns simulation (1fs/step = 1E6 steps). We have saved the structure every 5 steps, which means we have 1E6/5 = 2E5 structures.\n\n\nShow code\n\nplot( dihed300$V2,type=\"l\",main=\"Ethane dihedral @300K/1ns\",ylab=\"Dihedral\",xlab=\"Structure/5steps\")\n\n\n\nShow code\n\n#legend(\"bottomleft\",legend=c(\"Ethane Dihedral 300K\"))\n\n\n\nSo, not much is happening, during most of the 1ns simulation, the dihedral remains at -60 and by structure 150000 it switches to +60 with some swing back and forth.\nThis same information can be obtained by calculating a histogram\n\n\nShow code\n\nhist(dihed300$V2,breaks=120,main=\"Ethane dihedral @300K/1ns\",xlab=\"dihedral\")\n\n\n\nShow code\n\n#legend(\"topright\",legend=c(\"Ethane Dihedral 300K\"))\n\n\n\nThis is a somewhat unexpected result. We would liked to see three peaks corresponding to three minima where the staggered conformation of ethane is most stable. The first conclusion that one may extract is that the dynamics has not explored all the available phase space given the 1ns simulation and therefore given that we have not converged the phase space integral we cannot calculate the free energy. So, we have two options if we want to explore it further:\nOption 1 is to increase the temperature\nOption 2 is to run the simulation longer than 1ns\nOption 3 is to add some additional potential or bias that makes the simulations explore other regions.\nIn this post we will explore option 1 and 2.\n3.2 Option 1: Effect of increasing the temperature\nIf we are interested in measuring the free energy barrier of the dihedral rotation at 300K. Increasing the temperature will improve our exploration of the phase space, but it won’t be the same ensemble as in 300K and therefore the free energy won’t be the same. But for the sake of argument, let’s just compare how much more the dihedral changes when increasing the temperature Remember, temperature for 8 atoms is ill-defined and it does not make much sense to exactly specifiy it. But the atoms will have higher velocity and therefore higher chance to overcome the energy barrier.\n\n\nShow code\n\nplot( dihed600$V2,type=\"l\",col=c(\"red\"),ylim=c(-200,200),main=\"Ethane dihedral MD@300K and @600K/1ns\",ylab=\"Dihedral\",xlab=\"Structure/5steps\")\nlines( dihed300$V2,col=c(\"black\"))\nlegend(\"bottomleft\",legend=c(\"Dihedral@300K\",\"Dihedral@600K\"),text.col=c(\"black\",\"red\"))\n\n\n\n\nSo there is clearly more exploration at a higher temperature. Let’s compare the histograms. I randomly decide to make a bin every three degrees giving a total of 360/3 = 120 bins.\n\nLet’s display the histogram\n\n\nShow code\n\ndenshist600 =hist(dihed600$V2,breaks=120,main=\"Ethane dihedral MD@600K/1ns\",xlab=\"dihedral\",xlim = c(-180,180))\n\n\n\nShow code\n\n#plot(denshist600$mids, denshist600$density,type=\"l\",col=c(\"red\"),xlim=c(-180,180))\n#legend(\"topright\",legend=c(\"Dihedral@600K\"),text.col=c(\"red\"))\n\n\n\nNow let’s calculate the free energy \\(\\Delta G = -RT ln(\\rho)\\) in kcal/mol (using R = 1.985E-3). As we said in the first section, the density \\(\\rho(r)\\) is just the histogram of the dihedral shown above. We’ll calculate the logarithm of the density and multiply it by (-RT).\n\n\nShow code\n\nR = 1.985E-3\nTemp = 600.0\ndenshist600$log = -R*Temp*log(denshist600$density)\nplot(denshist600$mids,denshist600$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(5,10),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\nlegend(\"topright\",legend=c(\"Free Energy@600K 1ns \"),text.col=c(\"red\"))\n\n\n\nShow code\n\n#record barriers\nbarr_all_f5_1ns_1 = toString(op(calcBarrier(denshist600,-179.9,-110)))\nbarr_all_f5_1ns_2 = toString(op(calcBarrier(denshist600,-70,10)))\nbarr_all_f5_1ns_3 = toString(op(calcBarrier(denshist600,50,130)))\n\n\n\nQualitatively, the barrier is between 1 and 1.5 kcal/mol at 600K, remember that free energy barriers will depend on the temperature. Also, notice that the dihedral barriers should be the same for all the staggered-eclipsed transitions. It looks like that the dihedral at -60 degrees is more stable than at 60 degrees, which makes no physical sense. We’ll have to discard some data, read below for more info about this.\nLet’s calculate the barrier between the -60 and 0 degrees as precisely as we can: 3.02 which is different than the one between +60 and +120 which is 2.33. They should not be different as all hydrogens are identical. This discrepancy means that we explored all phase space but not equally or at least we are oversampling the minimum at -60 which makes it too stable. Let’s remember this result for later.\n3.3 Option 2: Longer dynamics\nIf we run a longer molecular dynamics, say five times longer, 5 ns (5E6 steps) at 300 K we can see that we cover all of the dihedral’s phase space. The details are the same, 300K, saving a structure every 5 steps.\n\n\nShow code\n\nsetwd(\"~/Teaching/Pchem/S18/Pchem_SharedFolder/10_Modeling/ethane\")\ndihed300_5ns = read.table(\"./dihed_300_5ns.txt\")\ndenshist300_5ns =hist(dihed300_5ns$V2,breaks=120,main=\"Ethane diehdral MD@300K/5ns\",xlab = \"dihedral\",xlim = c(-180,180),plot=TRUE)\n\n\n\nShow code\n\n#plot(denshist300_5ns$mids, denshist300_5ns$density,type=\"l\",col=c(\"red\"),xlim=c(-180,180))\n#legend(\"topright\",legend=c(\"Dihedral@300K - 5ns\"),text.col=c(\"red\"))\n\n\n\nCompared to the 300K@1ns the histograms above are now showing that at 5ns at 300K we are exploring all the dihedral range. However, the 180 degree seems to be more visited than the -60 and +60, which still does not make physical sense. Let’s calculate the free energy using the above histogram to see its impact on the value of free energy of rotation.\n\n\nShow code\n\ninit = 10020 # discard the first 50000 steps (50 ps)\nfinal = 1000020\nfreq = 1 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300.0\nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\nplot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\nlegend(\"topright\",legend=c(\"Free Energy@300K-5ns \"),text.col=c(\"red\"))\n\n\n\nShow code\n\n#record barriers\nbarr_1s_f5_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_1s_f5_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_1s_f5_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nIf you look at the R code above you’ll see that I discarded the first 50ps of simulations. This is a common procedure to eliminate the first equilibration steps. The barrier is now 2.58 kcal/mol between -60 and 0 degrees and 2.65 kcal/mol between +60 and +120 degrees.\n4 Studying the different options for the PMF\nFor the above calculation of rotation barrier we have made some random decisions. We have collected a dihedral measurement every 5 steps of the simulation, we have discarded the first 50 ps but used the rest. So we have some valid questions ahead:\nShould we measure the dihedral less frequently? How often?\nShould we discard the first part of the simulation? How much?\nWhat tells us when we have converged results?\nBelow is the code for: 5ns - Every 10 steps - Discard the first 50ps - Use the rest\n\n\nShow code\n\ninit = 10020 # discard the first 50000 steps (50 ps)\nfinal = 1000020\nfreq = 2 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300.0\nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\nplot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\nlegend(\"topright\",legend=c(\"300K - Every 10 steps - from 50ps to 5ns \"),text.col=c(\"red\"))\n\n\n\nShow code\n\n#record barriers\nbarr_1s_f10_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_1s_f10_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_1s_f10_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 50 steps - Discard the first 50ps - Use the rest. We will not plot the rest of free energy profiles, but you may do it by uncommenting the plot lines in the R code.\n\n\nShow code\n\ninit = 10020 # discard the first 50000 steps (50 ps)\nfinal = 1000020\nfreq = 10 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 50 steps - from 50ps to 5ns \"),text.col=c(\"red\"))\n\n#record barriers\nbarr_1s_f50_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_1s_f50_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_1s_f50_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 5 steps - Discard the first 3ns - Use the rest\n\n\nShow code\n\ninit = 600020 # discard the first 3ns (5x6E5)\nfinal = 1000020\nfreq = 1 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 50 steps - from 50ps to 5ns \"),text.col=c(\"red\"))\n\n#record barriers\nbarr_2s_f5_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_2s_f5_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_2s_f5_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 10 steps - Discard the first 3ns - Use the rest\n\n\nShow code\n\ninit = 600020 # discard the first 3ns (5x6E5)\nfinal = 1000020\nfreq = 2 # every 10 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 10 steps - from 3ns to 5ns \"),text.col=c(\"red\"))\n\n#record barriers\nbarr_2s_f10_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_2s_f10_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_2s_f10_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nBelow is the code for: 5ns - Every 50 steps - Discard the first 3ns - Use the rest\n\n\nShow code\n\ninit = 600020 # discard the first 3ns (5x6E5)\nfinal = 1000020\nfreq = 10 # every 50 steps (5x10)\nR = 1.985E-3\nTemp = 300 \nthisDihed = dihed300_5ns$V2[seq(init,final,freq)]\n\ndens = hist(thisDihed,breaks=120,xlim = c(-180,180),plot = FALSE)\ndens$log = -R*Temp*log(dens$density)\n#plot(dens$mids, dens$log,type=\"l\",col=c(\"red\"),xlim=c(-180,180),ylim = c(2,7),main = \"Free Energy - Ethane Rotation\",xlab = \"Dihedral\",ylab = \"G(kcal/mol)\")\n#legend(\"topright\",legend=c(\"300K - Every 50 steps - 50-5ns \"),text.col=c(\"red\"))\n#record barriers\nbarr_2s_f50_5ns_1 = toString(op(calcBarrier(dens,-179.99,-110)))\nbarr_2s_f50_5ns_2 = toString(op(calcBarrier(dens,-70,10)))\nbarr_2s_f50_5ns_3 = toString(op(calcBarrier(dens,50,130)))\n\n\n\nThe table below compares the barrier of rotation for all these different options. At this point we will avoid using statistical tools to assess how converged or how different these results are. For a longer discussion and a more systematic approach on assessing the convergence of simulations, check the paper by Grossfield and Zuckerman entitled Quantifying uncertainty and sampling quality in biomolecular simulations\nSimulation  Simulation / Freq / sample\n-180°→ -120°\n-60°→0°\n+60°→+120°\n600K 1ns/5 steps/0 - 1ns\n2.68\n3.02\n2.33\n300K 5ns/5 steps/50ps- 5ns\n2.82\n2.58\n2.65\n300K 5ns/10 steps/50ps- 5ns\n2.85\n2.60\n2.66\n300K 5ns/50 steps/50ps- 5ns\n2.79\n2.63\n2.65\n300K 5ns/5 steps/3ns- 5ns\n2.66\n2.91\n2.86\n300K 5ns/10 steps/3ns- 5ns\n2.71\n2.93\n2.84\n300K 5ns/50 steps/3ns- 5ns\n2.70\n3.04\n2.85\nNotice that for a simple system such as ethane and for a relatively long simulation such as 5ns one can observe differences of the order of 0.5 kcal/mol. In the paper cited above by Grossfield and Zuckerman, the authors list in section 4 a series of recommendations. The take home message is that there is still a fair amount of heuristic analysis to settle on the calculation of thermodynamic properties using molecular simulation.\n\n\n\n",
    "preview": "posts/calculating_pmf/distill-preview.png",
    "last_modified": "2021-07-28T17:45:08+02:00",
    "input_file": "calculating_pmf.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/the_four_horsemen/",
    "title": "The four horsemen of curriculum innovation apocalypse",
    "description": "What everyone should take into account before investing in curriculum change",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2021-06-01",
    "categories": [
      "Curriculum"
    ],
    "contents": "\nFour Horsemen of the Apocalypse, an 1887 painting by Viktor Vasnetsov\nSo you want to change your curriculum, you may even want to be innovative and change everything, assessment, course sequence, type of delivery, calendar, degree requirements. Everything.\nWell, before you start dreaming, brainstorming with post-its, and drawing on a whiteboard with multiple colors, there are four gates, four checkpoints, four horsemen that will send your little innovative ideas to the apocalypse unless you keep them in mind. You may decide to fight each of them or you may just let them limit your possibilities, but you cannot ignore them.\nThey work as control checkpoints or valves. They slow down the process, sometimes for a good reason, because any significant change in the curriculum has a lasting impact to the institution and to the future of our students.\nSo here they are, Death, Famine, War, and Conquest\nHorseman 1. Death: Admission requirements for graduate and professional schools.\nWill your courses be accepted elsewhere? Will they transfer-in and transfer-out? The moment your courses do not have the name or the exact content or the prescribed number of credits, they may not be accepted elsewhere. It is not uncommon to not be transfer friendly among undergraduate degrees, specially for courses in the major. But it is a completely different set of problems if professional and graduate schools do not recognize our courses. This is specially problematic when many of our students pursue a post-graduate career. Not paying attention to this problem may undermine students career forever.\nAdvice\nMake a decision early and be aware of it. Unless your institution has a prestigious and recognized brand (Stanford courses will be accepted everywhere) there will be schools who will not recognize your courses.\nUsually not being transfer friendly lowers the number of applying students. Bring the “Admissions department” to the conversation, they may not be too happy. Give them information and materials to help them “sell” your innovative curriculum.\nConsult with student counselors and academic coaches. What do they think? Are we really undermining our students long-term careers?\nTalk to professional and graduate schools ahead of time. Possibly making minor changes to the curriculum (changing course names, or adding a pre-req) may make your courses more transfer friendly.\nHorseman 2. Famine: Calendar and scheduling challenges\nYou may have thought about “block scheduling”, or limiting sections to 10 people, or perhaps starting your semester earlier or perhaps having all students take the same chemistry lab during the same week. While there may be a pedagogically justified reason for your innovation it may be hindered by the following realities 1. Financial Aid and calendar stiffness. 2. Number of staff skilled, available and willing to teach it. 3. Number of special rooms or labs available at a given time.\nAdvice\nThese problems revolve around money. Just be aware of how much money you have for your innovations.\nHorseman 3. War: Faculty buy in. Whip your votes.\nThe same way that it is not bad that there are systems in place to slow down any curriculum change, it is not bad either that faculty may be skeptical or may require convincing. That being said, it is also important to recognize that not everyone will be on board. Be always careful with members who will keep vocalizing opposition, if not by votes, by volume and repetition. The tyranny of the loudest usually undermines the morale of the many.\nAdvice\nCount your votes before putting a motion on the floor. Designate a whip. Listen and be ready to compromise\nBe transparent: Perhaps create a site or a FAQ answering the questions to all the worse case scenarios. Always control your message and do not let unanswered questions linger.\nIt doesn’t matter how right you think you are, unless you are going to create a parallel department, do not think you can carry on without a significant buy-in of the faculty.\nHorseman 4. Conquest: Know your students. How many are you willing to leave behind?\nOften, curriculum innovations are motivated by pedagogy. You want your students to learn better, learn more, transfer, apply, do research, hands-on experience. In other words, you want your students to practice what is called “high-order cognitive skills” as early as possible. Problem solving is a high-order cognitive skill. Having taught first-years I learned that there’s a significant portion of our cohort who are not ready to engage in high-order activities, the cause may be many, as many as they may be described in the books about student success: mental health, socioeconomic pressures, studying skills, work ethics, academic preparation…\nAdvice\nUnless you “play safe” and become a fairly exclusive institution by not admitting students under certain academic performance there are two possibilities ahead:\nMaximizing the number of students on board which will minimize true problem-solving or any other high-order cognitive skill\nReal experience, hands-on, true problem-solving which will leave some students behind.\nThis last option is specially delicate with internships in the community and private sector. Will they be carefully planned? Any placement process? Otherwise, putting an unprepared student in a “real” environment, will not only be detrimental to the student education, but to the relationship with the community partner.\n\n\n\n",
    "preview": "posts/the_four_horsemen/Apocalypse_vasnetsov.jpg",
    "last_modified": "2021-06-05T16:34:29-05:00",
    "input_file": {}
  },
  {
    "path": "posts/history4kids/",
    "title": "World History for kids in times of plague",
    "description": "A drag and drop silly interface to teach kids fundamental events in human civilization",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2020-05-22",
    "categories": [
      "Javascript",
      "Kids"
    ],
    "contents": "\nDuring the first weeks of the pandemic I decided to teach some basic historical events in human (western) civilization.\nI know nothing about how to teach History, but it seems to me that while the analysis of historical events will always be more important than remembering facts, there has to be a minimum of memorization. Every functioning citizen should be able to place the important events in a timeline.\nSo I wrote a silly drag and drop interactive page using the jQuery library to give my kids a sense of accomplishment while they memorize this stuff.\nHere’s the link to the silly html page with Javascript\n\n\n\n\n\n",
    "preview": "posts/history4kids/histokids.png",
    "last_modified": "2021-06-08T07:22:14-05:00",
    "input_file": "history4kids.knit.md",
    "preview_width": 2290,
    "preview_height": 1582
  },
  {
    "path": "posts/learning_python_during_covid/",
    "title": "Learning Python during COVID0-19",
    "description": "Using simple examples of covid data manipulation and data representation for my independent study learning Python.",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2020-05-01",
    "categories": [
      "Python"
    ],
    "contents": "\nComparing current COVID pandemic with other causes of disease and death\nFocusing on deaths and the same country\nIn order to get a better picture of the magnitude of the pandemics, counting number of deaths by covid instead of number of cases may better. It has been reported that many COVID cases go undetected either because they are asymptomatic or the country has no resources or strategy to diagnose them. While this is also true when applied to number of deaths, one can expect a higher number of tests on the individuals who pass away.\nAlso, different countries have different strategies or resources for testing for COVID so our comparison will only make sense if we compare within the same country.\nSources\nInfluenza and pneumonia in the US: https://healthdata.gov/dataset/deaths-pneumonia-and-influenza-pi-and-all-deaths-state-and-region-national-center-health-0\nCOVID updated by the NYtimes https://github.com/nytimes/covid-19-data\n\n\nShow code\nimport pandas as pd\nimport numpy as np\n# pandas cheat sheet https://pandas.pydata.org/Pandas_Cheat_Sheet.pdf\n\nflu = pd.read_csv(\"/Users/xavier/IndepStudy20/CodingSpring20_students/COVID19/flu_deaths_cdc.csv\")\ncovid = pd.read_csv(\"https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-states.csv\")\n\nstates = covid['state'].unique()\n\nHow many people die on a regular basis? Using New York state as example.\nIn order to make sense of the current news it may be useful to first get a sense of how current deaths compare with previous, non-pandemic, years.\n\n\nShow code\nflu19ny = flu.loc[ (flu[\"season\"] == \"2018-19\") & (flu['geoid'] == \"State\") & (flu[\"State\"] == \"New York\")]\nflu19ny = flu19ny.sort_values(by=\"MMWR Year/Week\")\nfluXweek = np.mean(flu19ny[\"Deaths from pneumonia and influenza\"])\ndeatXweek = np.mean(flu19ny[\"All Deaths\"])\nprint(fluXweek,deatXweek)\n167.6 1988.2\n\nOn average, in New York state, every week {{fluXweek}} people die of flu/pneumonia out of a total average of {{deatXweek}} all deaths per week.\n\n\nShow code\n#flu19 = flu.loc[ (flu[\"season\"] == \"2018-19\") & (flu['geoid'] == \"State\") ]\n\n#flu19ny.plot.barh(x=\"MMWR Year/Week\",y=[\"Deaths from pneumonia and influenza\",\"All Deaths\"],figsize=(15,15))\n#flu19ny.plot.bar(x=\"MMWR Year/Week\",y=\"All Deaths\")\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-07-15T09:30:02-05:00",
    "input_file": "learning_python_during_covid.knit.md"
  },
  {
    "path": "posts/openended_5_items/",
    "title": "Open ended questions: Name 5 relevant items",
    "description": "Analysis of asking students to name 5 relevant statement pertinent to a topic",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2020-01-15",
    "categories": [
      "R",
      "Assessment"
    ],
    "contents": "\n\nContents\nIntroduction\nAnalysis\nOverall O.E. grade distribution\nCorrelating performance in O.E. and course grades\nLetter grades groups and performance in OE\nTwo different types of mistakes in OE\n\nConclusions\n\nIntroduction\nThe following data was collected during the fall semester of 2019 from a cohort of about 200 first-year students taking a first-year chemistry semester. We are analyzing here the results of implementing the open ended assessments as they had been described in “Establishing open-ended assessments: investigating the validity of creative exercises” SE Lewis, JL Shaw, KA Freeman - Chem. Educ. Res. Pract., 2011, 12, 158-166 (https://doi.org/10.1039/C1RP90020J)\nDuring these “open Ended questions” (OE) students are asked to name 5 relevant statements about a topic. Students had practiced similar questions at the beginning of each class session. During the three semester exams they were asked to name five relevant things about certain topics. For example:\nExam 1: Given a table of “Mass atom” and “Natural abundance” for two unknown elements they were asked “Below are shown the isotopic breakdown of 2 different elements. Make 5 relevant statements about these two samples”\n\nExam 2: Make 5 relevant and true statements about the organic compound shown below. CH3CH(CH3)CH2CHO\nExam 3: Given three skeletal structures of biomolecules “Make 5 relevant and true statements about the three molecules below”\n\nAnalysis\nLet’s load the data and clean it up a little bit. All the information is in the “all” dataframe\n\n\nShow code\n\nsetwd(\"~/Gd/Research/OpenEnded5relevantThings/\")\n \n#Load grades\ngrades <- read.csv(\"./chem1331grades_f19_new.csv\",header=TRUE)\n#load gradescope results for Exam1, 2, and 3\nopend1 <- read.csv(\"./Ex1_f19_1_5_things.csv\",header=TRUE)\nopend2 <- read.csv(\"./Ex2_f19_1_5_Things.csv\",header=TRUE)\nopend3 <- read.csv(\"./Ex3_f19_1_5_Things.csv\",header=TRUE)\n#Merge the whole thing into on DF\nall = merge(opend1,opend2, by=\"Email\",all=TRUE)\nall = merge(all,opend3, by=\"Email\",all=TRUE)\nall = merge(all,grades, by=\"Email\",all=TRUE)\nrownames(all) <- all$Email\nall$Email <- NULL\n#calcualate mean score in OE and create a new column\nall$meanOE = rowMeans(subset(all,select = c(\"Score.x\",\"Score.y\",\"Score\")),na.rm = TRUE)\n\n\n\nOverall O.E. grade distribution\nOn a total grade of 5 points, if we average the score of each student over exam 1, 2, and 3 we can see that most students obtain an average between 2.5 and 4.\n\n\nShow code\n\n#plot the average vs final grade\nhist(all$meanOE, main=\"Average grade of Op End in Exam 1,2,3\")\n\n\n\n\nCorrelating performance in O.E. and course grades\nIf we plot performance in O.E. vs final grades we obtain a significant positive correlation\n\n\nShow code\n\n#remove students who dont have a final grade\nallNAfinal = all[!is.na(all$MAX),]\nplot(allNAfinal$meanOE,allNAfinal$MAX, main = \"Final course grade vs OE avg score\")\nr2<-cor(allNAfinal$meanOE, allNAfinal$MAX)\nabline(lm(allNAfinal$MAX~ allNAfinal$meanOE))\nlegend(x='bottomright',legend=paste(\"Cor=\",round(r2,4)))\n\n\n\n\nWe can also find for other grade categories that correlate better with O.E. than final grades. For obvious reasons, the highest correlation is with the open ended written exams as the O.E. questions are part of it.\n\n\nShow code\n\nr2finalexam<-cor(allNAfinal$meanOE, allNAfinal$Final.Exam )\nr2written<-cor(allNAfinal$meanOE, allNAfinal$OpenEnded )\nr2finalLab<-cor(allNAfinal$meanOE, allNAfinal$LabFinal )\nr2HW<-cor(allNAfinal$meanOE, allNAfinal$HW )\nr2Report<-cor(allNAfinal$meanOE, allNAfinal$Report.. )\nr2Prelab <-cor(allNAfinal$meanOE, allNAfinal$Prelab.. )\nr2table <- data.frame(\"Final grade\" = c(r2), \"Final Exam\" = c(r2finalexam), \"Writen Exams\"= c(r2written), \"Lab final\" = c(r2finalLab), \"Homework\" = c(r2HW), \"Lab reports\" = c(r2Report), \"Prelab\" = c(r2Prelab)  )\nkable(r2table)\n\n\nFinal.grade\nFinal.Exam\nWriten.Exams\nLab.final\nHomework\nLab.reports\nPrelab\n0.618099\n0.5305522\n0.747087\n0.4978805\n0.2553626\n0.1902491\n0.0019672\n\nLetter grades groups and performance in OE\nWe can analyze the performance in OE for different letter grades. The number of students obtaining C and D are too low and the noise is too high.\n\n\nShow code\n\ndescribeBy( allNAfinal$meanOE , allNAfinal$LETTER )\n\n\n\n Descriptive statistics by group \ngroup: A\n   vars  n mean   sd median trimmed  mad  min max range skew kurtosis\nX1    1 17 4.18 0.49      4    4.18 0.49 3.33   5  1.67    0    -1.41\n     se\nX1 0.12\n---------------------------------------------------- \ngroup: A-\n   vars  n mean   sd median trimmed  mad  min  max range skew\nX1    1 26 3.76 0.49   3.67    3.76 0.49 2.83 4.67  1.83 0.05\n   kurtosis  se\nX1    -1.02 0.1\n---------------------------------------------------- \ngroup: B\n   vars  n mean   sd median trimmed  mad  min  max range skew\nX1    1 27 2.86 0.68   2.67    2.83 0.49 1.67 4.33  2.67 0.54\n   kurtosis   se\nX1    -0.28 0.13\n---------------------------------------------------- \ngroup: B-\n   vars  n mean  sd median trimmed  mad min max range  skew kurtosis\nX1    1 30 2.78 0.7   2.75    2.79 0.86 1.5   4   2.5 -0.19    -1.05\n     se\nX1 0.13\n---------------------------------------------------- \ngroup: B+\n   vars  n mean   sd median trimmed  mad min  max range  skew\nX1    1 38 3.36 0.71   3.33    3.39 0.74 1.5 4.67  3.17 -0.52\n   kurtosis   se\nX1    -0.21 0.11\n---------------------------------------------------- \ngroup: C\n   vars  n mean   sd median trimmed  mad min  max range  skew\nX1    1 11 2.44 0.69   2.67     2.5 0.49   1 3.33  2.33 -0.63\n   kurtosis   se\nX1    -0.77 0.21\n---------------------------------------------------- \ngroup: C-\n   vars n mean   sd median trimmed  mad  min  max range skew kurtosis\nX1    1 6 2.64 0.36    2.5    2.64 0.12 2.33 3.33     1  1.1    -0.49\n     se\nX1 0.15\n---------------------------------------------------- \ngroup: C+\n   vars  n mean   sd median trimmed  mad  min  max range  skew\nX1    1 17 2.22 0.67   2.33    2.24 0.49 0.67 3.33  2.67 -0.85\n   kurtosis   se\nX1     0.44 0.16\n---------------------------------------------------- \ngroup: D\n   vars n mean   sd median trimmed  mad min max range skew kurtosis\nX1    1 4 1.33 0.45   1.17    1.33 0.12   1   2     1 0.68    -1.73\n     se\nX1 0.23\n---------------------------------------------------- \ngroup: D-\n   vars n mean sd median trimmed mad  min  max range skew kurtosis se\nX1    1 1 1.17 NA   1.17    1.17   0 1.17 1.17     0   NA       NA NA\n---------------------------------------------------- \ngroup: F\n   vars n mean   sd median trimmed  mad  min max range skew kurtosis\nX1    1 2 2.33 0.94   2.33    2.33 0.99 1.67   3  1.33    0    -2.75\n     se\nX1 0.67\n\nShow code\n\nboxplot(allNAfinal$meanOE ~ allNAfinal$LETTER, las=2,ylab=\"Average OE\")\n\n\n\n\nAnd we can also run an ANOVA among the different letter grades and we see significant difference between many letter groups. Check the column “p adj” for the p value\n\n\nShow code\n\nTukeyHSD( aov(allNAfinal$meanOE ~ allNAfinal$LETTER))\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = allNAfinal$meanOE ~ allNAfinal$LETTER)\n\n$`allNAfinal$LETTER`\n             diff         lwr          upr     p adj\nA--A  -0.42006033 -1.07217915  0.232058483 0.5773984\nB-A   -1.31227306 -1.95959992 -0.664946190 0.0000000\nB--A  -1.39869281 -2.03339132 -0.763994301 0.0000000\nB+-A  -0.81682147 -1.42687609 -0.206766842 0.0010753\nC-A   -1.73707665 -2.54610148 -0.928051821 0.0000000\nC--A  -1.53758170 -2.53039473 -0.544768667 0.0000589\nC+-A  -1.96078431 -2.67790818 -1.243660450 0.0000000\nD-A   -2.84313725 -4.00501071 -1.681263803 0.0000000\nD--A  -3.00980392 -5.16117551 -0.858432332 0.0004851\nF-A   -1.84313725 -3.40607248 -0.280202030 0.0076123\nB-A-  -0.89221273 -1.46669022 -0.317735236 0.0000552\nB--A- -0.97863248 -1.53884182 -0.418423133 0.0000028\nB+-A- -0.39676113 -0.92888796  0.135365691 0.3520873\nC-A-  -1.31701632 -2.06902262 -0.565010010 0.0000026\nC--A- -1.11752137 -2.06444799 -0.170594744 0.0075360\nC+-A- -1.54072398 -2.19284280 -0.888605167 0.0000000\nD-A-  -2.42307692 -3.54599376 -1.300160082 0.0000000\nD--A- -2.58974359 -4.72032849 -0.459158686 0.0049882\nF-A-  -1.42307692 -2.95727340  0.111119554 0.0957831\nB--B  -0.08641975 -0.64104362  0.468204116 0.9999892\nB+-B   0.49545159 -0.03079178  1.021694960 0.0849945\nC-B   -0.42480359 -1.17265826  0.323051080 0.7459180\nC--B  -0.22530864 -1.16894160  0.718324314 0.9994581\nC+-B  -0.64851126 -1.29583812 -0.001184389 0.0491424\nD-B   -1.53086420 -2.65100497 -0.410723423 0.0007474\nD--B  -1.69753086 -3.82665396  0.431592231 0.2562108\nF-B   -0.53086420 -2.06302997  1.001301575 0.9882838\nB+-B-  0.58187135  0.07124212  1.092500571 0.0119190\nC-B-  -0.33838384 -1.07533481  0.398567133 0.9185141\nC--B- -0.13888889 -1.07390401  0.796126234 0.9999931\nC+-B- -0.56209150 -1.19679001  0.072607006 0.1351248\nD-B-  -1.44444444 -2.55733504 -0.331553848 0.0018185\nD--B- -1.61111111 -3.73642880  0.514206578 0.3274411\nF-B-  -0.44444444 -1.97131775  1.082428858 0.9970598\nC-B+  -0.92025518 -1.63609118 -0.204419182 0.0021241\nC--B+ -0.72076023 -1.63922511  0.197704641 0.2778656\nC+-B+ -1.14396285 -1.75401747 -0.533908224 0.0000003\nD-B+  -2.02631579 -3.12533805 -0.927293530 0.0000006\nD--B+ -2.19298246 -4.31107115 -0.074893759 0.0355273\nF-B+  -1.02631579 -2.54311061  0.490479031 0.5023129\nC--C   0.19949495 -0.86160460  1.260594499 0.9999374\nC+-C  -0.22370766 -1.03273249  0.585317163 0.9980828\nD-C   -1.10606061 -2.32679991  0.114678701 0.1146152\nD--C  -1.27272727 -3.45645213  0.910997585 0.7151075\nF-C   -0.10606061 -1.71323858  1.501117373 1.0000000\nC+-C- -0.42320261 -1.41601565  0.569610418 0.9490516\nD-C-  -1.30555556 -2.65513364  0.044022527 0.0675802\nD--C- -1.47222222 -3.73049829  0.786053846 0.5593833\nF-C-  -0.30555556 -2.01265180  1.401540693 0.9999603\nD-C+  -0.88235294 -2.04422639  0.279520511 0.3247925\nD--C+ -1.04901961 -3.20039120  1.102351982 0.8839316\nF-C+   0.11764706 -1.44528817  1.680582284 1.0000000\nD--D  -0.16666667 -2.50420447  2.170871141 1.0000000\nF-D    1.00000000 -0.81064900  2.810649000 0.7771284\nF-D-   1.16666667 -1.39397771  3.727311039 0.9222908\n\nTwo different types of mistakes in OE\nThere are two main reasons why students would not get full marks for each statement\nStatement is not relevant or it is redundant\nStatement is not true\nWe can calculate the average of times that student made those two types of mistakes\n\n\nShow code\n\n#students with high instances of answers being not relevant\nnotRelevant <- all[,grepl(\"not.relevant\", colnames(all))]\na <- rowSums(notRelevant == \"TRUE\",na.rm = TRUE)\nb <- rowSums(notRelevant == \"FALSE\", na.rm = TRUE)\nall$notRelevant <- a/(a+b)\n#students with high instances of answers being not true\nnotTrue <- all[,grepl(\"not.true\", colnames(all))]\na <- rowSums(notTrue == \"TRUE\",na.rm = TRUE)\nb <- rowSums(notTrue == \"FALSE\", na.rm = TRUE)\nall$notTrue <- a/(a+b)\nallNAfinal = all[!is.na(all$MAX),]\n\n\n\nAnd we can see how making those two types of mistakes correlates with the other course performance. The table below is with “Not True” mistakes. Notice that the correlation in both types of mistakes is, as expected, negative, which means that the higher the number of mistakes the lower the performance in the course.\n\n\nShow code\n\nr2<-cor(allNAfinal$notTrue, allNAfinal$MAX )\nr2finalexam<-cor(allNAfinal$notTrue, allNAfinal$Final.Exam )\nr2written<-cor(allNAfinal$notTrue, allNAfinal$OpenEnded )\nr2finalLab<-cor(allNAfinal$notTrue, allNAfinal$LabFinal )\nr2HW<-cor(allNAfinal$notTrue, allNAfinal$HW )\nr2Report<-cor(allNAfinal$notTrue, allNAfinal$Report.. )\nr2Prelab <-cor(allNAfinal$notTrue, allNAfinal$Prelab.. )\nr2table <- data.frame(\"Final grade\" = c(r2), \"Final Exam\" = c(r2finalexam), \"Writen Exams\"= c(r2written), \"Lab final\" = c(r2finalLab), \"Homework\" = c(r2HW), \"Lab reports\" = c(r2Report), \"Prelab\" = c(r2Prelab)  )\nkable(r2table, caption=\"Correlation with Not True mistakes\")\n\n\nTable 1: Correlation with Not True mistakes\nFinal.grade\nFinal.Exam\nWriten.Exams\nLab.final\nHomework\nLab.reports\nPrelab\n-0.5063556\n-0.4116678\n-0.637651\n-0.4290181\n-0.2054091\n-0.1721591\n-0.0044984\n\nShow code\n\nplot(allNAfinal$notTrue,allNAfinal$MAX, main = \"Final course grade vs Not true mistakes\")\nabline(lm(allNAfinal$MAX~ allNAfinal$notTrue))\nlegend(x='bottomright',legend=paste(\"Cor=\",round(r2,4)))\n\n\n\n\nAnd this table is with “Not relevant” mistakes\n\n\nShow code\n\nr2<-cor(allNAfinal$notRelevant, allNAfinal$MAX )\nr2finalexam<-cor(allNAfinal$notRelevant , allNAfinal$Final.Exam )\nr2written<-cor(allNAfinal$notRelevant, allNAfinal$OpenEnded )\nr2finalLab<-cor(allNAfinal$notRelevant, allNAfinal$LabFinal )\nr2HW<-cor(allNAfinal$notRelevant, allNAfinal$HW )\nr2Report<-cor(allNAfinal$notRelevant, allNAfinal$Report.. )\nr2Prelab <-cor(allNAfinal$notRelevant, allNAfinal$Prelab.. )\nr2table <- data.frame(\"Final grade\" = c(r2), \"Final Exam\" = c(r2finalexam), \"Writen Exams\"= c(r2written), \"Lab final\" = c(r2finalLab), \"Homework\" = c(r2HW), \"Lab reports\" = c(r2Report), \"Prelab\" = c(r2Prelab)  )\nkable(r2table, caption=\"Correlation with Not True mistakes\")\n\n\nTable 2: Correlation with Not True mistakes\nFinal.grade\nFinal.Exam\nWriten.Exams\nLab.final\nHomework\nLab.reports\nPrelab\n-0.2034072\n-0.2341019\n-0.1863573\n-0.1541022\n-0.0451623\n-0.007477\n0.0414387\n\nShow code\n\nplot(allNAfinal$notRelevant,allNAfinal$MAX, main = \"Final course grade vs Not Relevant mistakes\")\nabline(lm(allNAfinal$MAX~ allNAfinal$notRelevant))\nlegend(x='bottomright',legend=paste(\"Cor=\",round(r2,4)))\n\n\n\n\nConclusions\nIt does make sense that performance in Open Ended assessments correlates positively with all the grade categories in the course. As for the types of mistakes, making “not true” mistakes in these type of questions correlates better with performance in the course.\n\n\n\n",
    "preview": "posts/openended_5_items/openended_5_items_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-06-03T12:32:50-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/triangles_of_curriculum_design/",
    "title": "Chemistry curriculum: triangles everywhere",
    "description": "Many aspects of Chemistry learning, from delivery to learning objectives come in triads of linked concepts",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2019-08-01",
    "categories": [
      "Curriculum"
    ],
    "contents": "\n\nContents\nThree ideas for curriculum design\nThree ideas for learning objectives\nThree concepts to improve summative assessments\n\nThree representations in chemistry\nFour languages of chemistry (not three)\n\nI will use the triangle shape to represent the idea that three linked and intertwined concepts cannot be separated from one another. In this case, in my daily life as a teacher, there are many instances where my job requires considering a triad of linked concepts. I’m listing here those different triads, from the most general aspects of curriculum design to the most specific ones of chemistry learning.\nThree ideas for curriculum design\nYou want to design a new course, reshape an existing one. There are so many things to be taken care. One can spend a lot of time choosing a textbook that may be barely used, designing with careful detail an activity in class or a lecture that students will barely be paying attention to. Should you use a homework online system provided by the publisher? How can you make sure students learn what they are supposed to? How can you tell them? To me it’s the class activities. I make sure that the class activities align with the preclass content and with the assessment. They do not only define your pedagogy during class but also the content. You may have made a video explaining something, or a great lecture or demonstration, but if it is not assessed in form of activity, if students do not see “what’s in it for them”, it will fall flat.\nThree ideas for learning objectives\nAlong the same lines to what I said above, the key word is alignment. The same way that is\nThree concepts to improve summative assessments\nThree representations in chemistry\nThis is an old idea I believe first expressed by Johnstone, A.H. J. Comp. Assist. Learn. 1991, 7, 701-703\nThe very symbol H2O, depending on the context, may represent the molecule or the macroscopic state of matter or both. Chemists will often switch from one representation to another and the instructor must always be aware of how hard it is for some students to make this switch.\nA good exercise to practice this switch is asking students to associate a list of properties to a macroscopic, submicroscopic or symbolic. For example: density, bond length, two hydrogen per oxygen atom, boiling point, temperature, vibrational frequency, ionization energy…\nFour languages of chemistry (not three)\nToo often undergraduate chemistry pays a lot of focus to the least intuitive (for most students) language of chemistry, the calculations.\nKnowing chemistry implies that you can express the same idea in its four languages, but that doesn’t mean that you should expect students to use the four of them at the same time. My experience tells me that from the most intuitive to the least (for most students) is drawing, representing, explaining, and calculating.\nStarting with an intuitive language such as drawing and representing helps students make the necessary connections and lower their cognitive load once you move to a more abstract and less intuitive concepts and languages.\nHere’s two examples of how atoms may be understood using the four different languages. The first model represents an essentially coulombic or classical atom and the second one comes from the quantum solution of the hydrogen atom.\n\n\n\n",
    "preview": "posts/triangles_of_curriculum_design/triangle1.png",
    "last_modified": "2021-06-05T12:57:17-05:00",
    "input_file": {},
    "preview_width": 1077,
    "preview_height": 754
  },
  {
    "path": "posts/analysis_seven_years_genchem/",
    "title": "Analysis of seven years of General Chemistry student data",
    "description": "Trying to make sense of student performance and identifying possible predictors of academic success.",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2018-08-10",
    "categories": [
      "Curriculum",
      "R"
    ],
    "contents": "\n\nContents\nAbstract\nOverview Final Course grades\nComparing means by semester\nGraphically by semester\nStatiscal analysis by semester\nOther grades besides final grade\nLetter grades\nSemester exams and previous exams\n\n\nPredictors of performance in Chemistry\nMath ACT is a good predictor\nWas math ACT different through the years?\nCorrelation models: ACT vs GenChem1\n\nPrevious GPA is a better predictor\nWas Incoming GPA different through the years?\nCorrelation models: Prev. GPA vs GenChem grades\n\nIs Highschool performance relevant?\nWas Highschool performance different through the years?\nCorrelation models: HS rank vs GenChem grades\n\n\nDemographics\nGender\nComparing the two genders each year\nPerformance by each gender through the years\n\nEthnicity\nStatistical analysis of performance in GenChem1 by ethnicity\nDifferent ethnicities\n\nFirst generation students\nStatistical analysis of performance 1st generation in GenChem1\n\n\n\nAbstract\nBetween Fall 2010 and Spring 2018 I taught the two semester sequence of General Chemistry (GC). The way our curriculum was structured, these two semesters were usually taken during the sophomore year for students majoring in a Bachelor of Sciences in Health Sciences.\nDuring all this time, while the chemistry content has not changed significantly, the forms of delivery and assessment have been evolving towards, hopefully, better pedagogies of engagment and towards a clearer assessment of learning objectives. Probably the most remarkable change was flipping the class with videos in the fall of 2014.\nOverview Final Course grades\nLet’s just look at how students have performed in the two GC semesters by looking at their final grade in different semesters.\nIMPORTANT: We will see statistical significance between years and other demographics when analyzing the final percent grade. However, when we analyze the letter grade, those significances disappear. This is important because when a student is disengaged their score may be 60% or 5%, and while the means and medians may be affected, the letter grade analysis will not. Also, during the semester of Fall 2011 - Spring 2012 the laboratory was still a different course, this means that the criteria for a passing grade was not 70%, but lower.\nComparing means by semester\n\n\nShow code\n\nsetwd(\"~/Gd/Research/StudentData/Discover\")\n\n#Load demographics for all years\nallGC1 <- read.csv(\"./genchem1_nosummer_11_16.csv\",header=TRUE)\nallGC2 <- read.csv(\"./genchem2_11_17.csv\",header=TRUE)\nallGC1_ <- read.csv(\"./genchem1_nosummer_11_16_mergedsex.csv\",header = TRUE)\nallGC2_ <- read.csv(\"./genchem2_11_17_mergedsex.csv\",header = TRUE)\n\nlibrary(psych)\nmata<-describeBy(allGC1$TG_Total.Grade....,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1\")\n\n\nTable 1: GenChem1\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n68\n78.43\n10.11\n79.50\n78.98\n9.79\n42.50\n95.90\n53.40\nX12\nFall 2012\n84\n73.95\n12.41\n73.00\n73.78\n14.53\n43.00\n96.20\n53.20\nX13\nFall 2013\n69\n83.81\n8.17\n84.70\n84.49\n6.82\n41.60\n96.50\n54.90\nX14\nFall 2014\n105\n80.95\n9.73\n81.52\n81.78\n8.61\n40.52\n98.78\n58.27\nX15\nFall 2015\n60\n81.44\n9.74\n82.68\n82.39\n7.19\n43.47\n96.33\n52.87\nX16\nFall 2016\n35\n84.17\n7.24\n84.95\n84.60\n7.82\n66.98\n97.14\n30.16\n\nShow code\n\nmata<-describeBy(allGC2$TG_Total.Grade....,allGC2$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2\")\n\n\nTable 1: GenChem2\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nSpring 2011\n16\n87.38\n8.34\n89.84\n88.07\n6.55\n67.70\n97.38\n29.68\nX12\nSpring 2012\n45\n69.01\n12.95\n70.70\n69.90\n10.53\n35.60\n94.00\n58.40\nX13\nSpring 2013\n51\n81.51\n9.42\n82.00\n82.00\n9.93\n53.70\n97.50\n43.80\nX14\nSpring 2014\n55\n80.31\n8.51\n79.30\n80.55\n8.75\n60.10\n96.70\n36.60\nX15\nSpring 2015\n61\n78.51\n10.00\n77.78\n78.55\n11.92\n57.26\n96.14\n38.88\nX16\nSpring 2016\n44\n77.15\n9.66\n78.73\n77.77\n9.49\n54.39\n94.10\n39.72\nX17\nSpring 2017\n37\n80.01\n9.80\n80.78\n80.64\n8.01\n48.11\n97.22\n49.11\n\nGraphically by semester\n\n\nShow code\n\nlibrary(ggplot2)\nggplot(allGC1, aes(x=TG_Total.Grade...., fill=Semester))+geom_histogram()+ggtitle(\"GenChem1 by semester\")\n\n\n\nShow code\n\nggplot(allGC2, aes(x=TG_Total.Grade...., fill=Semester))+geom_histogram()+ggtitle(\"GenChem2\")\n\n\n\n\nStatiscal analysis by semester\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$TG_Total.Grade.... ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. GenChem1 Grade among semesters\")\n\n\nTable 2: Anova. GenChem1 Grade among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-4.4779412\n-9.1423644\n0.186482\n0.0681540\nFall 2013-Fall 2011\n5.3836530\n0.4976747\n10.269631\n0.0211987\nFall 2014-Fall 2011\n2.5216131\n-1.9292496\n6.972476\n0.5843701\nFall 2015-Fall 2011\n3.0089288\n-2.0556712\n8.073529\n0.5318824\nFall 2016-Fall 2011\n5.7435885\n-0.2048146\n11.691992\n0.0653478\nFall 2013-Fall 2012\n9.8615942\n5.2158876\n14.507301\n0.0000000\nFall 2014-Fall 2012\n6.9995543\n2.8138662\n11.185242\n0.0000345\nFall 2015-Fall 2012\n7.4868700\n2.6536537\n12.320086\n0.0001708\nFall 2016-Fall 2012\n10.2215296\n4.4688516\n15.974208\n0.0000082\nFall 2014-Fall 2013\n-2.8620399\n-7.2932841\n1.569204\n0.4353187\nFall 2015-Fall 2013\n-2.3747242\n-7.4220918\n2.672643\n0.7584349\nFall 2016-Fall 2013\n0.3599354\n-5.5738024\n6.293673\n0.9999781\nFall 2015-Fall 2014\n0.4873157\n-4.1401366\n5.114768\n0.9996654\nFall 2016-Fall 2014\n3.2219753\n-2.3589421\n8.802893\n0.5638475\nFall 2016-Fall 2015\n2.7346596\n-3.3470041\n8.816323\n0.7918982\n\nShow code\n\na<- TukeyHSD( aov(allGC2$TG_Total.Grade.... ~ allGC2$Semester)) \nb<-as.data.frame(a$`allGC2$Semester`)\nknitr::kable(b, caption = \"Anova. GenChem2 Grade among semesters\")\n\n\nTable 2: Anova. GenChem2 Grade among semesters\n\ndiff\nlwr\nupr\np adj\nSpring 2012-Spring 2011\n-18.3719243\n-27.016530\n-9.7273185\n0.0000000\nSpring 2013-Spring 2011\n-5.8749308\n-14.385113\n2.6352511\n0.3860347\nSpring 2014-Spring 2011\n-7.0680859\n-15.504043\n1.3678712\n0.1676882\nSpring 2015-Spring 2011\n-8.8701902\n-17.212128\n-0.5282519\n0.0288746\nSpring 2016-Spring 2011\n-10.2332149\n-18.903549\n-1.5628812\n0.0094423\nSpring 2017-Spring 2011\n-7.3733537\n-16.259708\n1.5130002\n0.1767662\nSpring 2013-Spring 2012\n12.4969935\n6.422770\n18.5712173\n0.0000001\nSpring 2014-Spring 2012\n11.3038384\n5.334050\n17.2736265\n0.0000009\nSpring 2015-Spring 2012\n9.5017341\n3.665559\n15.3379087\n0.0000439\nSpring 2016-Spring 2012\n8.1387093\n1.842068\n14.4353503\n0.0028667\nSpring 2017-Spring 2012\n10.9985706\n4.407646\n17.5894949\n0.0000250\nSpring 2014-Spring 2013\n-1.1931551\n-6.966573\n4.5802632\n0.9963637\nSpring 2015-Spring 2013\n-2.9952594\n-8.630410\n2.6398912\n0.6966880\nSpring 2016-Spring 2013\n-4.3582841\n-10.469068\n1.7524994\n0.3452669\nSpring 2017-Spring 2013\n-1.4984229\n-7.912023\n4.9151776\n0.9928975\nSpring 2015-Spring 2014\n-1.8021043\n-7.324522\n3.7203134\n0.9603189\nSpring 2016-Spring 2014\n-3.1651291\n-9.172113\n2.8418544\n0.7053683\nSpring 2017-Spring 2014\n-0.3052678\n-6.620048\n6.0095122\n0.9999993\nSpring 2016-Spring 2015\n-1.3630247\n-7.237241\n4.5111913\n0.9931556\nSpring 2017-Spring 2015\n1.4968365\n-4.691783\n7.6854560\n0.9914449\nSpring 2017-Spring 2016\n2.8598613\n-3.764772\n9.4844944\n0.8600771\n\n\n\nShow code\n\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\nggboxplot(allGC1, x = \"Semester\", y = \"TG_Total.Grade....\",  title = \"Final grade in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(allGC2, x = \"Semester\", y = \"TG_Total.Grade....\",  title = \"Final grade in GC2\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC2$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nOther grades besides final grade\n.\nLetter grades\nI converted the letter grades into the 4-scale. The plot should only show 4, 3.66, 3.33, 3… but it seems to add more variability…\n\n\nShow code\n\n#need to load this other file, as it contains the letter grades\nallGC1_bosco <- read.csv(\"~/Research/StudentData/XavierData/Clean/allGC1.csv\",header = TRUE)\n\na<- allGC1_bosco$Final.letter\na <- gsub(\"A\\\\-\", 3.667,a)\na <- gsub(\"A\", 4.000,a)\na <- gsub(\"B\\\\+\", 3.333,a)\na <- gsub(\"B\\\\-\", 2.667,a)\na <- gsub(\"B\", 3.000,a)\na <- gsub(\"C\\\\+\", 2.333,a)\na <- gsub(\"C\\\\-\", 1.667,a)\na <- gsub(\"C\", 2.000,a)\na <- gsub(\"D\\\\+\", 1.333,a)\na <- gsub(\"D\", 1.000,a)\na <- gsub(\"F\", 0.000,a)\na <- gsub(\"I\", 0.000,a)\nallGC1_bosco$Final.letter.number <- as.numeric(as.character(a))\nggboxplot(allGC1_bosco, x = \"Semester\", y = \"Final.letter.number\",  title = \"Final letter grade in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1_bosco$Final.letter.number), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\n#ggplot(data=allGC1_bosco,aes(x=Semester,y=Final.letter)) + geom_bar(stat=\"identity\") + geom_bar(aes(fill = Final.letter))\n\n\n\nSemester exams and previous exams\n\n\nShow code\n\nsetwd(\"~/Gd/Research/StudentData/Discover\")\n#lets write the prepost file into the discover folder\nprePost <- read.csv(\"/Users/xavier/Gd/Research/StudentData/ExamPrePost.csv\",header=TRUE,sep = \"\\t\")\nsource(\"~/Gd/Research/R/deid.R\")\nprePost <- deIdThis(prePost)\nwrite.csv(prePost,file=\"prePost.csv\")\nprePost <- read.csv(\"./prePost.csv\", header = TRUE)\nprePost$inc1<-prePost$Grade1-prePost$Mid1\nprePost$inc2<-prePost$Grade2-prePost$Mid2\nprePost$inc3<-prePost$Grade3-prePost$Mid3\nprePost$meanInc <- rowMeans( prePost[c('inc1','inc2','inc3')])\nprePost$meanExam <- rowMeans( prePost[c('Grade1','Grade2','Grade3')])\n\n\n\nThe final exam is a second opportunity for students to improve their semester exams. Let’s measure how exams score and improvement evolved through the years.\n\n\nShow code\n\nggboxplot(prePost, x = \"Semester\", y = \"meanExam\",  title = \"Average grade in final exams\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(prePost$meanExam), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 105 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nThis is plots the increment\n\n\nShow code\n\nggboxplot(prePost, x = \"Semester\", y = \"meanInc\",  title = \"Average increment from semester exams to final\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(prePost$meanInc), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 40 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nThere’s something funky about some of these numbers. Fall 2014 doesn’t seem to apply the >40% rule, which I actually implemented.\nSo let’s check that I obtain the same result if I plot grade exams from BoSCO data\n\n\nShow code\n\nallGC1_bosco$meanExam <-  rowMeans( allGC1_bosco[c('Exam1','Exam2','Exam3')], na.rm=TRUE)\n\nggboxplot(allGC1_bosco, x = \"Semester\", y = \"meanExam\",  title = \"Average grade in final exams (Bosco source)\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1_bosco$meanExam), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 105 ) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nPredictors of performance in Chemistry\nThere are different variables that we want to look at. Performance factors such as ACT scores or GPA or High School rank , as well as demographic factors such as ethnicity and first-year generation.\nMath ACT is a good predictor\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_ACT.MATH,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"ACT Math - Fall sophomore\")\n\n\nTable 3: ACT Math - Fall sophomore\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n64\n24.75\n3.50\n24.5\n24.67\n3.71\n18\n33\n15\nX12\nFall 2012\n70\n24.41\n4.01\n24.0\n24.29\n4.45\n17\n34\n17\nX13\nFall 2013\n66\n25.14\n2.82\n25.0\n25.26\n2.97\n18\n31\n13\nX14\nFall 2014\n101\n25.47\n3.21\n26.0\n25.40\n2.97\n17\n34\n17\nX15\nFall 2015\n57\n24.72\n3.19\n24.0\n24.70\n2.97\n18\n32\n14\nX16\nFall 2016\n32\n24.94\n2.64\n25.0\n24.96\n2.97\n19\n30\n11\n\nShow code\n\nmata<-describeBy(allGC2$DEM_ACT.MATH,allGC2$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"ACT Math - Spring sophomore\")\n\n\nTable 3: ACT Math - Spring sophomore\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nSpring 2011\n16\n25.88\n4.18\n25.5\n25.79\n3.71\n19\n34\n15\nX12\nSpring 2012\n42\n25.57\n3.47\n25.0\n25.56\n2.97\n18\n33\n15\nX13\nSpring 2013\n44\n26.23\n4.15\n26.0\n26.17\n4.45\n18\n34\n16\nX14\nSpring 2014\n52\n24.90\n2.76\n25.0\n25.17\n2.97\n18\n29\n11\nX15\nSpring 2015\n58\n26.10\n3.36\n26.0\n26.04\n2.97\n19\n34\n15\nX16\nSpring 2016\n42\n25.10\n3.27\n25.0\n25.00\n2.97\n18\n32\n14\nX17\nSpring 2017\n33\n25.27\n2.47\n26.0\n25.30\n2.97\n21\n30\n9\n\nWe see that the second semester is a subselection of the first semester with a higher ACT math score. Therefore, we can just use GenChem1 for the analysis.\nWas math ACT different through the years?\nAs we can see below. There is no significant difference in ACT throughout the years\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$DEM_ACT.MATH ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. ACTMath among semesters\")\n\n\nTable 4: Anova. ACTMath among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-0.3357143\n-1.9769059\n1.3054774\n0.9919345\nFall 2013-Fall 2011\n0.3863636\n-1.2784117\n2.0511390\n0.9856326\nFall 2014-Fall 2011\n0.7153465\n-0.8007861\n2.2314792\n0.7559117\nFall 2015-Fall 2011\n-0.0307018\n-1.7589701\n1.6975666\n1.0000000\nFall 2016-Fall 2011\n0.1875000\n-1.8670492\n2.2420492\n0.9998340\nFall 2013-Fall 2012\n0.7220779\n-0.9060719\n2.3502278\n0.8010990\nFall 2014-Fall 2012\n1.0510608\n-0.4247621\n2.5268838\n0.3217601\nFall 2015-Fall 2012\n0.3050125\n-1.3880045\n1.9980295\n0.9955408\nFall 2016-Fall 2012\n0.5232143\n-1.5017715\n2.5482001\n0.9767972\nFall 2014-Fall 2013\n0.3289829\n-1.1730225\n1.8309883\n0.9889559\nFall 2015-Fall 2013\n-0.4170654\n-2.1329539\n1.2988231\n0.9823133\nFall 2016-Fall 2013\n-0.1988636\n-2.2430100\n1.8452827\n0.9997726\nFall 2015-Fall 2014\n-0.7460483\n-2.3181344\n0.8260379\n0.7513444\nFall 2016-Fall 2014\n-0.5278465\n-2.4528701\n1.3971770\n0.9699093\nFall 2016-Fall 2015\n0.2182018\n-1.8779779\n2.3143814\n0.9996831\n\nShow code\n\nggboxplot(allGC1, x = \"Semester\", y = \"DEM_ACT.MATH\",  title = \"ACT Math in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$DEM_ACT.MATH, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 40) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nCorrelation models: ACT vs GenChem1\n\n\nShow code\n\n#par(mfrow = c(1, 2))\nplot(allGC1$TG_Total.Grade....,allGC1$DEM_ACT.MATH,main=\"GenChem1\")\na<- lm(allGC1$DEM_ACT.MATH~allGC1$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2a<-summary(a)$r.squared\n\nplot(allGC2$TG_Total.Grade....,allGC2$DEM_ACT.MATH,main=\"GenChem2\")\na<-lm(allGC2$DEM_ACT.MATH~allGC2$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2b<-summary(a)$r.squared\n\n\n\nWe obtain a r-squared for both 0.2042773 and 0.1569021, respectively. We need to find a better predictor. Let’s see cumulative GPA before enrolling\nPrevious GPA is a better predictor\nWhile ACT.Math historically seems to correlate well, since we’re teaching sophomores, previous GPA is even a better predictor\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1\")\n\n\nTable 5: GenChem1\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n65\n3.11\n0.46\n3.14\n3.13\n0.52\n1.88\n3.93\n2.05\nX12\nFall 2012\n79\n2.94\n0.53\n2.94\n2.95\n0.59\n1.44\n3.98\n2.54\nX13\nFall 2013\n69\n3.18\n0.44\n3.21\n3.19\n0.53\n1.84\n3.98\n2.14\nX14\nFall 2014\n105\n3.00\n0.48\n2.98\n3.00\n0.47\n1.33\n4.00\n2.67\nX15\nFall 2015\n60\n3.00\n0.39\n3.00\n2.98\n0.33\n2.18\n3.97\n1.79\nX16\nFall 2016\n35\n3.12\n0.48\n3.26\n3.14\n0.42\n2.06\n4.00\n1.94\n\nShow code\n\nmata<-describeBy(allGC2$DEM_Cumulative.GPA,allGC2$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2\")\n\n\nTable 5: GenChem2\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nSpring 2011\n16\n3.46\n0.43\n3.54\n3.47\n0.46\n2.73\n4.00\n1.27\nX12\nSpring 2012\n43\n3.18\n0.39\n3.19\n3.19\n0.40\n2.33\n3.95\n1.62\nX13\nSpring 2013\n50\n3.20\n0.48\n3.24\n3.24\n0.39\n2.05\n3.97\n1.92\nX14\nSpring 2014\n53\n3.25\n0.44\n3.27\n3.26\n0.50\n2.18\n3.98\n1.80\nX15\nSpring 2015\n61\n3.18\n0.46\n3.19\n3.18\n0.49\n2.19\n4.00\n1.81\nX16\nSpring 2016\n44\n3.07\n0.41\n3.05\n3.06\n0.36\n2.13\n3.96\n1.83\nX17\nSpring 2017\n36\n3.23\n0.40\n3.26\n3.25\n0.34\n2.13\n4.00\n1.87\n\nWas Incoming GPA different through the years?\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$DEM_Cumulative.GPA ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. Entering GPA among semesters\")\n\n\nTable 6: Anova. Entering GPA among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-0.1679124\n-0.3930252\n0.0572004\n0.2709961\nFall 2013-Fall 2011\n0.0728361\n-0.1595233\n0.3051956\n0.9469705\nFall 2014-Fall 2011\n-0.1069817\n-0.3191411\n0.1051777\n0.7000993\nFall 2015-Fall 2011\n-0.1160769\n-0.3567413\n0.1245875\n0.7384580\nFall 2016-Fall 2011\n0.0137802\n-0.2680571\n0.2956175\n0.9999925\nFall 2013-Fall 2012\n0.2407485\n0.0192443\n0.4622527\n0.0242003\nFall 2014-Fall 2012\n0.0609307\n-0.1392812\n0.2611426\n0.9531219\nFall 2015-Fall 2012\n0.0518354\n-0.1783657\n0.2820366\n0.9874925\nFall 2016-Fall 2012\n0.1816926\n-0.0912643\n0.4546495\n0.3999188\nFall 2014-Fall 2013\n-0.1798178\n-0.3881444\n0.0285087\n0.1350707\nFall 2015-Fall 2013\n-0.1889130\n-0.4262055\n0.0483794\n0.2048113\nFall 2016-Fall 2013\n-0.0590559\n-0.3380193\n0.2199075\n0.9905679\nFall 2015-Fall 2014\n-0.0090952\n-0.2266461\n0.2084557\n0.9999966\nFall 2016-Fall 2014\n0.1207619\n-0.1416144\n0.3831382\n0.7750540\nFall 2016-Fall 2015\n0.1298571\n-0.1560608\n0.4157750\n0.7847500\n\nShow code\n\nggboxplot(allGC1, x = \"Semester\", y = \"DEM_Cumulative.GPA\",  title = \"Entering GPA in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$DEM_Cumulative.GPA, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nCorrelation models: Prev. GPA vs GenChem grades\nWhen we plot previous GPA (typically first year GPA) against final grade\n\n\nShow code\n\n#par(mfrow = c(1, 2))\nplot(allGC1$TG_Total.Grade....,allGC1$DEM_Cumulative.GPA,main=\"GenChem1\")\na<-lm(allGC1$DEM_Cumulative.GPA~allGC1$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2a<-summary(a)$r.squared\nplot(allGC2$TG_Total.Grade....,allGC2$DEM_Cumulative.GPA,main=\"GenChem2\")\na<-lm(allGC2$DEM_Cumulative.GPA~allGC2$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2b<-summary(a)$r.squared\n\n\n\nIn this case we obtain better r-squared for both 0.656591 and 0.5840838, respectively\nIs Highschool performance relevant?\nFor large schools, highschool(HS) ranking can be used as a better measurement than HS GPA. Also, HS-GPA is currently unavailable :). The units are given in percentile, so the higher the better\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_HS.Rank,allGC1$Semester,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1\")\n\n\nTable 7: GenChem1\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nFall 2011\n60\n79.18\n14.08\n80.5\n80.48\n15.57\n46\n97\n51\nX12\nFall 2012\n65\n73.57\n17.01\n76.0\n74.98\n16.31\n20\n99\n79\nX13\nFall 2013\n57\n81.21\n12.33\n81.0\n82.04\n13.34\n47\n99\n52\nX14\nFall 2014\n86\n79.43\n16.62\n84.0\n81.56\n13.34\n26\n99\n73\nX15\nFall 2015\n51\n81.27\n13.83\n86.0\n82.93\n8.90\n37\n99\n62\nX16\nFall 2016\n25\n82.28\n11.75\n85.0\n82.95\n10.38\n60\n98\n38\n\nWas Highschool performance different through the years?\n\n\nShow code\n\na<- TukeyHSD( aov(allGC1$DEM_HS.Rank ~ allGC1$Semester)) \nb<-as.data.frame(a$`allGC1$Semester`)\nknitr::kable(b, caption = \"Anova. HS ranking among semesters\")\n\n\nTable 8: Anova. HS ranking among semesters\n\ndiff\nlwr\nupr\np adj\nFall 2012-Fall 2011\n-5.6141026\n-13.2615244\n2.033319\n0.2876124\nFall 2013-Fall 2011\n2.0271930\n-5.8736280\n9.928014\n0.9774141\nFall 2014-Fall 2011\n0.2468992\n-6.9383845\n7.432183\n0.9999987\nFall 2015-Fall 2011\n2.0911765\n-6.0444897\n10.226843\n0.9772356\nFall 2016-Fall 2011\n3.0966667\n-7.0718166\n13.265150\n0.9527517\nFall 2013-Fall 2012\n7.6412955\n-0.1100688\n15.392660\n0.0558988\nFall 2014-Fall 2012\n5.8610018\n-1.1596093\n12.881613\n0.1616772\nFall 2015-Fall 2012\n7.7052790\n-0.2853243\n15.695882\n0.0659401\nFall 2016-Fall 2012\n8.7107692\n-1.3420278\n18.763566\n0.1318975\nFall 2014-Fall 2013\n-1.7802938\n-9.0761070\n5.515519\n0.9819290\nFall 2015-Fall 2013\n0.0639835\n-8.1694637\n8.297431\n1.0000000\nFall 2016-Fall 2013\n1.0694737\n-9.1774107\n11.316358\n0.9996775\nFall 2015-Fall 2014\n1.8442772\n-5.7052249\n9.393779\n0.9818377\nFall 2016-Fall 2014\n2.8497674\n-6.8561055\n12.555640\n0.9594958\nFall 2016-Fall 2015\n1.0054902\n-9.4235429\n11.434523\n0.9997815\n\nShow code\n\nggboxplot(allGC1, x = \"Semester\", y = \"DEM_HS.Rank\",  title = \"Highschool Rank in GC1\",\n          color = \"Semester\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(allGC1$DEM_HS.Rank, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nFall2012 seems to stand out again.\nCorrelation models: HS rank vs GenChem grades\n\n\nShow code\n\n#par(mfrow = c(1, 2))\nplot(allGC1$TG_Total.Grade....,allGC1$DEM_HS.Rank,main=\"GenChem1\")\na<-lm(allGC1$DEM_HS.Rank~allGC1$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2a<-summary(a)$r.squared\nplot(allGC2$TG_Total.Grade....,allGC2$DEM_HS.Rank,main=\"GenChem2\")\na<-lm(allGC2$DEM_HS.Rank~allGC2$TG_Total.Grade.... )\nabline(a)\n\n\n\nShow code\n\nr2b<-summary(a)$r.squared\n\n\n\nFairly poor r-squared for both 0.1667476 and 0.0552476, respectively\nDemographics\nGiven the good correlation given above between previous GPA and final grade, let’s then analyze how students of different demographics perform in chemistry when compared to their incoming GPA. In other words, instead of comparing how first-generation vs non-first-generation do, it is more interesting to see how considering their college readiness (as desribed by GPA) how they did in GenChem\n\n\nGender\nLook at how previous GPA and GenChem grades is among selfidentified genders. There was no data besides male and female.\n\n\nShow code\n\n#there are some underfined that mess up the graphs\nonlyMF_gc1<- allGC1_[complete.cases(allGC1_$Sex),]\nonlyMF_gc2<- allGC2_[complete.cases(allGC2_$Sex),]\nmata<-describeBy(onlyMF_gc1$DEM_Cumulative.GPA,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA and Sex\")\n\n\nTable 9: 1st year GPA and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n286\n3.08\n0.45\n3.06\n3.08\n0.48\n1.88\n4\n2.12\nX12\nM\n117\n3.00\n0.53\n3.04\n3.01\n0.49\n1.33\n4\n2.67\n\nShow code\n\nmata<-describeBy(onlyMF_gc1$DEM_ACT.MATH,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"ACT math and Sex\")\n\n\nTable 9: ACT math and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n270\n24.61\n3.09\n25\n24.61\n2.97\n17\n34\n17\nX12\nM\n111\n25.82\n3.71\n26\n25.83\n2.97\n17\n34\n17\n\nShow code\n\nmata<-describeBy(onlyMF_gc1$DEM_HS.Rank,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"HS rank and Sex\")\n\n\nTable 9: HS rank and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n239\n81.02\n14.21\n84.0\n82.64\n11.86\n25\n99\n74\nX12\nM\n96\n74.40\n16.42\n76.5\n75.46\n17.05\n20\n99\n79\n\nFrom the above, we can see that females come to GenChem with very slightly higher GPA and remarkably better HS ranking, but with a lower ACT-math score. Also, males have a broader range of values and higher standard deviation, this tell us that male performance may not be treated as a single group, and it may require a further finer classification. In any case, How will these three factors affect their performance in GenChem? The number of students may not be exactly the same because not all students have ACT or HS data.\n\n\nShow code\n\nmata<-describeBy(onlyMF_gc1$TG_Total.Grade....,onlyMF_gc1$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1 grade and Sex\")\n\n\nTable 10: GenChem1 grade and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n289\n79.74\n9.84\n81.0\n80.49\n9.49\n42.50\n98.78\n56.28\nX12\nM\n120\n80.88\n12.12\n82.1\n82.36\n11.15\n40.52\n97.14\n56.62\n\nShow code\n\nmata<-describeBy(onlyMF_gc2$TG_Total.Grade....,onlyMF_gc2$Sex,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2 grade and Sex\")\n\n\nTable 10: GenChem2 grade and Sex\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nF\n196\n78.18\n10.51\n79.28\n78.88\n9.62\n38.20\n97.50\n59.30\nX12\nM\n102\n80.31\n10.19\n80.57\n80.76\n13.00\n56.39\n97.22\n40.83\n\nComparing the two genders each year\nWhile it may look like males do better than females, even though females came with better GPA and HS ranking, there is actually no significant difference when compared the two groups in general.\n\n\nShow code\n\n#install.packages(\"ggpubr\")\nlibrary(ggpubr)\np <- ggboxplot(onlyMF_gc1, x = \"Sex\", y = \"TG_Total.Grade....\", color = \"Sex\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\n\nHowever, when the two groups are compared each semester we notice that Fall 2011 is the only semester with a significant difference between genders.\n\n\nShow code\n\np <- ggboxplot(onlyMF_gc1, x = \"Semester.x\", y = \"TG_Total.Grade....\", color = \"Sex\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=Sex),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nPerformance by each gender through the years\nBefore we jump into conclusions, however, we may need to look into how the females in Fall 2011 performed compared to other semester’s females.\n\n\nShow code\n\n#selecting females\nonlyF_gc1 <- onlyMF_gc1[onlyMF_gc1$Sex==\"F\",]\nonlyM_gc1 <- onlyMF_gc1[onlyMF_gc1$Sex==\"M\",]\n\nggboxplot(onlyF_gc1, x = \"Semester.x\", y = \"TG_Total.Grade....\",  title = \"Females in GC1\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyF_gc1$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyM_gc1, x = \"Semester.x\", y = \"TG_Total.Grade....\",  title = \"Males in GC1\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyM_gc1$TG_Total.Grade....), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyF_gc1, x = \"Semester.x\", y = \"DEM_Cumulative.GPA\",  title = \"Incoming GPA for females\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyF_gc1$DEM_Cumulative.GPA, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyM_gc1, x = \"Semester.x\", y = \"DEM_Cumulative.GPA\",  title = \"Incoming GPA for males\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyM_gc1$DEM_Cumulative.GPA, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 5) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyF_gc1, x = \"Semester.x\", y = \"DEM_HS.Rank\",  title = \"HS Ranking for females\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyF_gc1$DEM_HS.Rank, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\nShow code\n\nggboxplot(onlyM_gc1, x = \"Semester.x\", y = \"DEM_HS.Rank\",  title = \"HS Ranking for males\",\n          color = \"Semester.x\", add = \"jitter\", legend=\"none\") + rotate_x_text(angle = 45) +  \n  geom_hline( yintercept = mean(onlyM_gc1$DEM_HS.Rank, na.rm = TRUE), linetype = 2) + \n  stat_compare_means(method = \"anova\", label.y = 110) +\n  stat_compare_means(label = \"p.format\", method = \"t.test\", ref.group = \".all.\")\n\n\n\n\nWe saw that females had performed significantly lower in Fall2011, and almost significantly higher in Fall2013 than males. However, we see that these differences may also be explained by the differences with the incoming GPAs, but not by HS ranking. Also, many students lack HS Ranking so the statistics may be lacking.\nEthnicity\nLet’s compare the GPA before enrolling in GenChem for students selfidentified ethnicity.\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$DEM_Student.of.Color,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA and Student of Color: Y/N\")\n\n\nTable 11: 1st year GPA and Student of Color: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nN\n340\n3.09\n0.47\n3.11\n3.10\n0.47\n1.33\n4.00\n2.67\nX12\nY\n73\n2.84\n0.46\n2.78\n2.82\n0.43\n1.90\n3.97\n2.07\n\nShow code\n\nmata<-describeBy(allGC1$TG_Total.Grade....,allGC1$DEM_Student.of.Color,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1 grade and Student of Color: Y/N\")\n\n\nTable 11: GenChem1 grade and Student of Color: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nN\n348\n81.06\n10.20\n82.34\n82.13\n8.65\n40.52\n98.78\n58.27\nX12\nY\n73\n74.67\n10.48\n75.30\n74.58\n12.04\n43.47\n96.02\n52.56\n\nShow code\n\nmata<-describeBy(allGC2$TG_Total.Grade....,allGC2$DEM_Student.of.Color,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2 grade and Student of Color: Y/N\")\n\n\nTable 11: GenChem2 grade and Student of Color: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\nN\n260\n79.13\n10.33\n79.74\n79.75\n10.53\n43.7\n97.5\n53.8\nX12\nY\n49\n74.44\n12.76\n72.95\n75.31\n9.87\n35.6\n96.7\n61.1\n\n\n\nShow code\n\nrequire(gridExtra)\nplotA <-ggplot(allGC1, aes(x=TG_Total.Grade...., fill=DEM_Student.of.Color)) + geom_histogram() + ggtitle(\"GenChem1 by Ethnicity\")\nplotB <-ggplot(allGC1, aes(x=DEM_Cumulative.GPA, fill=DEM_Student.of.Color)) + geom_histogram() + ggtitle(\"Prev GPA by Ethnicity\")\ngrid.arrange(plotA,plotB)\n\n\n\n\nStatistical analysis of performance in GenChem1 by ethnicity\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"DEM_Student.of.Color\", y = \"TG_Total.Grade....\", color = \"DEM_Student.of.Color\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"Semester\", y = \"TG_Total.Grade....\", color = \"DEM_Student.of.Color\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_Student.of.Color),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nDifferent ethnicities\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$DEM_Ethnicity,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA for different ethnicities\")\n\n\nTable 12: 1st year GPA for different ethnicities\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n0\nNaN\nNA\nNA\nNaN\nNA\nInf\n-Inf\n-Inf\nX12\nAm. Indian\n5\n2.74\n0.36\n2.62\n2.74\n0.43\n2.33\n3.23\n0.90\nX13\nAsian\n38\n2.88\n0.47\n2.81\n2.86\n0.36\n1.90\n3.97\n2.07\nX14\nBlack\n29\n2.96\n0.50\n2.89\n2.95\n0.56\n2.12\n3.95\n1.83\nX15\nHawaiian\n1\n3.11\nNA\n3.11\n3.11\n0.00\n3.11\n3.11\n0.00\nX16\nHispanic\n13\n2.78\n0.38\n2.70\n2.76\n0.36\n2.27\n3.47\n1.20\nX17\nNS\n3\n2.72\n0.66\n2.37\n2.72\n0.07\n2.32\n3.48\n1.16\nX18\nWhite\n324\n3.10\n0.47\n3.11\n3.11\n0.47\n1.33\n4.00\n2.67\n\nWe can also run an anova among different ethnicities, but in any case it’s hard to do statistics on such small numbers maybe only black and asian are large enough to be compared with whites.\n\n\nShow code\n\nTukeyHSD( aov(allGC1$TG_Total.Grade.... ~ allGC1$DEM_Ethnicity))\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = allGC1$TG_Total.Grade.... ~ allGC1$DEM_Ethnicity)\n\n$`allGC1$DEM_Ethnicity`\n                           diff         lwr       upr     p adj\nAm. Indian-          -1.2520311 -24.3127791 21.808717 0.9999998\nAsian-                1.8944333 -17.0079964 20.796863 0.9999879\nBlack-                0.2271630 -18.9237458 19.378072 1.0000000\nHawaiian-             6.1165333 -30.3457108 42.578777 0.9996050\nHispanic-            -0.4325654 -20.6581794 19.793049 1.0000000\nNS-                  -4.9680000 -30.7507001 20.814700 0.9990190\nWhite-                5.7148568 -12.5997033 24.029417 0.9807262\nAsian-Am. Indian      3.1464644 -11.8319308 18.124860 0.9982867\nBlack-Am. Indian      1.4791941 -13.8115804 16.769969 0.9999905\nHawaiian-Am. Indian   7.3685644 -27.2225576 41.959686 0.9981266\nHispanic-Am. Indian   0.8194657 -15.7975718 17.436503 0.9999999\nNS-Am. Indian        -3.7159689 -26.7767169 19.344779 0.9996975\nWhite-Am. Indian      6.9668879  -7.2624335 21.196209 0.8117453\nBlack-Asian          -1.6672703  -9.3686684  6.034128 0.9979230\nHawaiian-Asian        4.2221000 -27.7474084 36.191609 0.9999204\nHispanic-Asian       -2.3269987 -12.4081536  7.754156 0.9968822\nNS-Asian             -6.8624333 -25.7648630 12.039996 0.9553407\nWhite-Asian           3.8204235  -1.4689372  9.109784 0.3535646\nHawaiian-Black        5.8893703 -26.2276802 38.006421 0.9992900\nHispanic-Black       -0.6597284 -11.1994223  9.879965 0.9999995\nNS-Black             -5.1951630 -24.3460719 13.955746 0.9915567\nWhite-Black           5.4876938  -0.6305412 11.605929 0.1158453\nHispanic-Hawaiian    -6.5490987 -39.3183386 26.220141 0.9987569\nNS-Hawaiian         -11.0845333 -47.5467775 25.377711 0.9834226\nWhite-Hawaiian       -0.4016765 -32.0271526 31.223800 1.0000000\nNS-Hispanic          -4.5354346 -24.7610486 15.690179 0.9974027\nWhite-Hispanic        6.1474222  -2.7829165 15.077761 0.4184543\nWhite-NS             10.6828568  -7.6317033 28.997417 0.6360504\n\nFirst generation students\nLet’s compare the GPA before enrolling in GenChem for 1st generation vs the rest. Notice for how many people we have information (a total of 421 students in Genchem1 and 309 in GenChem2)\n\n\nShow code\n\nmata<-describeBy(allGC1$DEM_Cumulative.GPA,allGC1$DEM_First.Generation,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"1st year GPA and 1st generation: Y/N\")\n\n\nTable 13: 1st year GPA and 1st generation: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n20\n2.85\n0.58\n2.70\n2.79\n0.45\n1.88\n4.00\n2.12\nX12\nN\n248\n3.05\n0.48\n3.09\n3.07\n0.50\n1.33\n3.98\n2.65\nX13\nY\n145\n3.07\n0.44\n3.02\n3.06\n0.43\n1.90\n4.00\n2.10\n\nShow code\n\nmata<-describeBy(allGC1$TG_Total.Grade....,allGC1$DEM_First.Generation,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem1 grade and 1st generation: Y/N\")\n\n\nTable 13: GenChem1 grade and 1st generation: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n24\n72.63\n14.01\n73.62\n72.63\n15.20\n42.50\n97.14\n54.64\nX12\nN\n252\n79.81\n10.51\n81.07\n80.73\n10.29\n40.52\n97.07\n56.55\nX13\nY\n145\n81.41\n9.36\n82.18\n82.20\n8.03\n53.00\n98.78\n45.78\n\nShow code\n\nmata<-describeBy(allGC2$TG_Total.Grade....,allGC2$DEM_First.Generation,mat=TRUE,digits = 2)\nknitr::kable(mata[,c(2,4,5,6,7,8,9,10,11,12)] ,  caption = \"GenChem2 grade and 1st generation: Y/N\")\n\n\nTable 13: GenChem2 grade and 1st generation: Y/N\n\ngroup1\nn\nmean\nsd\nmedian\ntrimmed\nmad\nmin\nmax\nrange\nX11\n\n16\n78.97\n11.79\n79.30\n79.09\n14.52\n60.1\n96.14\n36.04\nX12\nN\n187\n78.26\n10.96\n79.10\n78.94\n11.12\n38.2\n97.50\n59.30\nX13\nY\n106\n78.52\n10.64\n79.42\n79.24\n10.49\n35.6\n96.70\n61.10\n\n\n\nShow code\n\nggplot(allGC1, aes(x=TG_Total.Grade...., fill=DEM_First.Generation )) + geom_histogram() + ggtitle(\"GenChem1 by First Generation\")\n\n\n\nShow code\n\nggplot(allGC2, aes(x=TG_Total.Grade...., fill=DEM_First.Generation))+geom_histogram()+ggtitle(\"GenChem2 by First Generation\")\n\n\n\n\nStatistical analysis of performance 1st generation in GenChem1\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"DEM_First.Generation\", y = \"TG_Total.Grade....\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"Semester\", y = \"TG_Total.Grade....\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_First.Generation),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC2, x = \"Semester\", y = \"TG_Total.Grade....\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")  + rotate_x_text(angle = 45)\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_First.Generation),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nFirst generation students seem to do slightly better or the same than the rest. Are they coming in with equal preparation? We can look at HS rank to try to answer that.\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"DEM_First.Generation\", y = \"DEM_HS.Rank\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means() #default is wilcox for comparing non-parametric two groups\n\n\n\nShow code\n\np <- ggboxplot(allGC1, x = \"Semester\", y = \"DEM_HS.Rank\", color = \"DEM_First.Generation\", palette = \"jco\", add = \"jitter\")\n#p + stat_compare_means(method = \"t.test\")\np + stat_compare_means(aes(group=DEM_HS.Rank),label=\"p.format\") #default is wilcox for comparing non-parametric two groups\n\n\n\n\nIt seems that the first generation students are already better prepared than the non-first generation.\n\n\n\n",
    "preview": "posts/analysis_seven_years_genchem/distill-preview.png",
    "last_modified": "2021-06-07T06:07:37-05:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/calculating_pmf_with_umbrella/",
    "title": "Calculate the Potential of Mean Force from MD of an enzymatic reaction",
    "description": "Parsing files, plotting numbers, calculating statistics",
    "author": [
      {
        "name": "Xavier Prat-Resina",
        "url": "https://pratresina.umn.edu"
      }
    ],
    "date": "2018-08-10",
    "categories": [
      "Modeling",
      "R"
    ],
    "contents": "\n\nContents\nAbstract\n\nAbstract\nLet’s first load a couple of files from the different umbrella sampling windows\n\n\nShow code\n\nsetwd(\"/Users/xavier/Gd/Research/Chile/10ns\")\n#myfiles contains the array of files\nmyfiles = list.files(path = \"./\", pattern=\"eqq+.*rc\")\n#reading the windows for R = -0.1 and -0.2\nn01 = read.table(myfiles[1])\nn02 = read.table(myfiles[2]) \n\n\n\nPlot their value through time\n\n\nShow code\n\nplot(n01$V2,type=\"l\")\nlines(n02$V2,col=c(\"red\"))\n\n\n\n\nPlot the histograms\n\n\nShow code\n\n# 100 bins gives a good continuum\nd_n01 = density(n01$V2,n=100)\nd_n02 = density(n02$V2,n=100)\nplot(d_n01$x,d_n01$y,type=\"l\")\nlines(d_n02$x,d_n02$y,col=c(\"red\"))\n\n\n\n\n\n\n\n",
    "preview": "posts/calculating_pmf_with_umbrella/distill-preview.png",
    "last_modified": "2021-07-15T12:01:14-05:00",
    "input_file": "calculating_pmf_with_umbrella.knit.md",
    "preview_width": 1248,
    "preview_height": 768
  }
]
